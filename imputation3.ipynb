{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 18:29:31.222331: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-07 18:29:31.393921: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-07 18:29:31.454683: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-07 18:29:31.905079: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-07 18:29:35.006566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import scipy.stats as stats\n",
    "# import statsmodels.api as sm\n",
    "import miceforest as mf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Lambda, Dropout, Concatenate\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error as sk_mse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>operation_date</th>\n",
       "      <th>redcap_event_name</th>\n",
       "      <th>ComplicationDate</th>\n",
       "      <th>dob</th>\n",
       "      <th>qol_date</th>\n",
       "      <th>age_diagnosis</th>\n",
       "      <th>gender</th>\n",
       "      <th>overall_primary_tumour</th>\n",
       "      <th>overall_regional_ln</th>\n",
       "      <th>...</th>\n",
       "      <th>a_e5</th>\n",
       "      <th>a_e6</th>\n",
       "      <th>a_e7</th>\n",
       "      <th>a_c6</th>\n",
       "      <th>a_c2</th>\n",
       "      <th>a_act11</th>\n",
       "      <th>readmission_30d</th>\n",
       "      <th>postop_comp</th>\n",
       "      <th>los</th>\n",
       "      <th>DischargeDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2049-08-04</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-08-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-08-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2010-03-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-02-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2009-03-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-02-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2008-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18182</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>preoperative_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18183</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>preoperative_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18184</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>preoperative_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-02-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18185</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>baseline_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1982-10-12</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18186</th>\n",
       "      <td>1770</td>\n",
       "      <td>2025-03-14</td>\n",
       "      <td>surgery_arm_1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18187 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id operation_date   redcap_event_name ComplicationDate        dob  \\\n",
       "0         1            NaT      baseline_arm_1              NaT 2049-08-04   \n",
       "1         1     2011-08-26                 NaN              NaT        NaT   \n",
       "2         1     2010-02-20                 NaN              NaT        NaT   \n",
       "3         1     2009-02-25                 NaN              NaT        NaT   \n",
       "4         1     2008-02-22                 NaN              NaT        NaT   \n",
       "...     ...            ...                 ...              ...        ...   \n",
       "18182  1770            NaT  preoperative_arm_1              NaT        NaT   \n",
       "18183  1770            NaT  preoperative_arm_1              NaT        NaT   \n",
       "18184  1770            NaT  preoperative_arm_1              NaT        NaT   \n",
       "18185  1770            NaT      baseline_arm_1              NaT 1982-10-12   \n",
       "18186  1770     2025-03-14       surgery_arm_1              NaT        NaT   \n",
       "\n",
       "        qol_date  age_diagnosis  gender overall_primary_tumour  \\\n",
       "0            NaT            NaN     1.0                    NaN   \n",
       "1            NaT            NaN     NaN                    NaN   \n",
       "2            NaT            NaN     NaN                    NaN   \n",
       "3            NaT            NaN     NaN                    NaN   \n",
       "4            NaT            NaN     NaN                    NaN   \n",
       "...          ...            ...     ...                    ...   \n",
       "18182        NaT            NaN     NaN                    NaN   \n",
       "18183        NaT            NaN     NaN                    NaN   \n",
       "18184 2025-02-24            NaN     NaN                    NaN   \n",
       "18185        NaT            NaN     1.0                    NaN   \n",
       "18186        NaT            NaN     NaN                    NaN   \n",
       "\n",
       "      overall_regional_ln  ...  a_e5  a_e6  a_e7  a_c6  a_c2  a_act11  \\\n",
       "0                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "1                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "2                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "3                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "4                     NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "...                   ...  ...   ...   ...   ...   ...   ...      ...   \n",
       "18182                 NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18183                 NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18184                 NaN  ...   0.0   0.0   0.0   1.0   3.0      3.0   \n",
       "18185                 NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "18186                 NaN  ...   NaN   NaN   NaN   NaN   NaN      NaN   \n",
       "\n",
       "       readmission_30d  postop_comp   los DischargeDate  \n",
       "0                  NaN          0.0   NaN    2012-04-27  \n",
       "1                  NaN          NaN   NaN    2011-08-26  \n",
       "2                  NaN          NaN  16.0    2010-03-08  \n",
       "3                  NaN          NaN   8.0    2009-03-05  \n",
       "4                  NaN          NaN  13.0    2008-03-06  \n",
       "...                ...          ...   ...           ...  \n",
       "18182              NaN          NaN   NaN           NaT  \n",
       "18183              NaN          NaN   NaN           NaT  \n",
       "18184              NaN          NaN   NaN           NaT  \n",
       "18185              NaN          NaN   NaN           NaT  \n",
       "18186              NaN          NaN   NaN           NaT  \n",
       "\n",
       "[18187 rows x 70 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file - linked file \n",
    "file_path = \"Merged_TSQIC_REDCap_ACCESS.xlsx\" \n",
    "df = pd.read_excel(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'operation_date', 'redcap_event_name', 'ComplicationDate', 'dob',\n",
       "       'qol_date', 'age_diagnosis', 'gender', 'overall_primary_tumour',\n",
       "       'overall_regional_ln', 'overall_distant_metastasis', 'neotx___notx',\n",
       "       'neotx___chemo', 'neotx___rads', 'neotx___chemorads', 'neotx___immuno',\n",
       "       'neotx___other', 'procedure123456', 'expectation_treatment',\n",
       "       'path_esoph_primtumour', 'path_esoph_regionalln',\n",
       "       'path_esoph_distantmetast', 'gp1', 'gp2', 'gp3', 'gp4', 'gp5', 'gp6',\n",
       "       'gp7', 'gs1', 'gs2', 'gs3', 'gs4', 'gs5', 'gs6', 'gs7', 'ge1', 'ge2',\n",
       "       'ge3', 'ge4', 'ge5', 'ge6', 'gf1', 'gf2', 'gf3', 'gf4', 'gf5', 'gf6',\n",
       "       'gf7', 'a_hn1', 'a_hn2', 'a_hn3', 'a_hn4', 'a_hn5', 'a_hn7', 'a_hn10',\n",
       "       'a_e1', 'a_e2', 'a_e3', 'a_e4', 'a_e5', 'a_e6', 'a_e7', 'a_c6', 'a_c2',\n",
       "       'a_act11', 'readmission_30d', 'postop_comp', 'los', 'DischargeDate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'operation_date', 'redcap_event_name', 'age_diagnosis', 'gender',\n",
       "       'overall_primary_tumour', 'overall_regional_ln',\n",
       "       'overall_distant_metastasis', 'neotx___notx', 'neotx___chemo',\n",
       "       'neotx___rads', 'neotx___chemorads', 'neotx___immuno', 'neotx___other',\n",
       "       'procedure123456', 'expectation_treatment', 'path_esoph_primtumour',\n",
       "       'path_esoph_regionalln', 'path_esoph_distantmetast', 'gp1', 'gp2',\n",
       "       'gp3', 'gp4', 'gp5', 'gp6', 'gp7', 'gs1', 'gs2', 'gs3', 'gs4', 'gs5',\n",
       "       'gs6', 'gs7', 'ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6', 'gf1', 'gf2',\n",
       "       'gf3', 'gf4', 'gf5', 'gf6', 'gf7', 'a_hn1', 'a_hn2', 'a_hn3', 'a_hn4',\n",
       "       'a_hn5', 'a_hn7', 'a_hn10', 'a_e1', 'a_e2', 'a_e3', 'a_e4', 'a_e5',\n",
       "       'a_e6', 'a_e7', 'a_c6', 'a_c2', 'a_act11', 'readmission_30d',\n",
       "       'postop_comp', 'los'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop dates\n",
    "df = df.drop(columns=[\"ComplicationDate\", \"dob\", \"qol_date\", \"DischargeDate\"]) #\"operation_date\"\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>operation_date</th>\n",
       "      <th>redcap_event_name</th>\n",
       "      <th>age_diagnosis</th>\n",
       "      <th>gender</th>\n",
       "      <th>overall_primary_tumour</th>\n",
       "      <th>overall_regional_ln</th>\n",
       "      <th>overall_distant_metastasis</th>\n",
       "      <th>neotx___notx</th>\n",
       "      <th>neotx___chemo</th>\n",
       "      <th>...</th>\n",
       "      <th>a_e4</th>\n",
       "      <th>a_e5</th>\n",
       "      <th>a_e6</th>\n",
       "      <th>a_e7</th>\n",
       "      <th>a_c6</th>\n",
       "      <th>a_c2</th>\n",
       "      <th>a_act11</th>\n",
       "      <th>readmission_30d</th>\n",
       "      <th>postop_comp</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-08-26</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-20</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-02-25</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-02-22</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18182</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18183</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18184</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18185</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18186</th>\n",
       "      <td>1770</td>\n",
       "      <td>2025-03-14</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18187 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id operation_date redcap_event_name  age_diagnosis gender  \\\n",
       "0         1            NaT                 8            NaN    1.0   \n",
       "1         1     2011-08-26                 9            NaN    NaN   \n",
       "2         1     2010-02-20                 9            NaN    NaN   \n",
       "3         1     2009-02-25                 9            NaN    NaN   \n",
       "4         1     2008-02-22                 9            NaN    NaN   \n",
       "...     ...            ...               ...            ...    ...   \n",
       "18182  1770            NaT                10            NaN    NaN   \n",
       "18183  1770            NaT                10            NaN    NaN   \n",
       "18184  1770            NaT                10            NaN    NaN   \n",
       "18185  1770            NaT                 8            NaN    1.0   \n",
       "18186  1770     2025-03-14                11            NaN    NaN   \n",
       "\n",
       "      overall_primary_tumour overall_regional_ln  overall_distant_metastasis  \\\n",
       "0                        NaN                 NaN                         NaN   \n",
       "1                        NaN                 NaN                         NaN   \n",
       "2                        NaN                 NaN                         NaN   \n",
       "3                        NaN                 NaN                         NaN   \n",
       "4                        NaN                 NaN                         NaN   \n",
       "...                      ...                 ...                         ...   \n",
       "18182                    NaN                 NaN                         NaN   \n",
       "18183                    NaN                 NaN                         NaN   \n",
       "18184                    NaN                 NaN                         NaN   \n",
       "18185                    NaN                 NaN                         NaN   \n",
       "18186                    NaN                 NaN                         NaN   \n",
       "\n",
       "      neotx___notx neotx___chemo  ... a_e4 a_e5 a_e6 a_e7 a_c6 a_c2 a_act11  \\\n",
       "0              NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "1              NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "2              NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "3              NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "4              NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "...            ...           ...  ...  ...  ...  ...  ...  ...  ...     ...   \n",
       "18182          0.0           0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18183          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18184          NaN           NaN  ...  0.0  0.0  0.0  0.0  1.0  3.0     3.0   \n",
       "18185          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18186          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "\n",
       "      readmission_30d  postop_comp   los  \n",
       "0                 NaN          0.0   NaN  \n",
       "1                 NaN          NaN   NaN  \n",
       "2                 NaN          NaN  16.0  \n",
       "3                 NaN          NaN   8.0  \n",
       "4                 NaN          NaN  13.0  \n",
       "...               ...          ...   ...  \n",
       "18182             NaN          NaN   NaN  \n",
       "18183             NaN          NaN   NaN  \n",
       "18184             NaN          NaN   NaN  \n",
       "18185             NaN          NaN   NaN  \n",
       "18186             NaN          NaN   NaN  \n",
       "\n",
       "[18187 rows x 66 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Preprocess categorical variables\n",
    "# Low-cardinality variables: Convert to category dtype\n",
    "low_cardinality_cols = [\n",
    "    'postop_comp', 'readmission_30d', 'gender', 'neotx___notx', 'neotx___chemo',\n",
    "    'neotx___rads', 'neotx___chemorads', 'neotx___immuno', 'neotx___other',\n",
    "    'expectation_treatment'\n",
    "]\n",
    "for col in low_cardinality_cols:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# High-cardinality variables: Label encode and convert to category\n",
    "le_redcap = LabelEncoder()\n",
    "df['redcap_event_name'] = df['redcap_event_name'].astype(str)  # Convert to string to handle NaN\n",
    "df['redcap_event_name'] = le_redcap.fit_transform(df['redcap_event_name'])\n",
    "df['redcap_event_name'] = df['redcap_event_name'].astype('category')\n",
    "\n",
    "# procedure123456 is already numerical but should be treated as categorical\n",
    "df['procedure123456'] = df['procedure123456'].astype('category')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>operation_date</th>\n",
       "      <th>redcap_event_name</th>\n",
       "      <th>age_diagnosis</th>\n",
       "      <th>gender</th>\n",
       "      <th>overall_primary_tumour</th>\n",
       "      <th>overall_regional_ln</th>\n",
       "      <th>overall_distant_metastasis</th>\n",
       "      <th>neotx___notx</th>\n",
       "      <th>neotx___chemo</th>\n",
       "      <th>...</th>\n",
       "      <th>a_e4</th>\n",
       "      <th>a_e5</th>\n",
       "      <th>a_e6</th>\n",
       "      <th>a_e7</th>\n",
       "      <th>a_c6</th>\n",
       "      <th>a_c2</th>\n",
       "      <th>a_act11</th>\n",
       "      <th>readmission_30d</th>\n",
       "      <th>postop_comp</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-08-26</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-20</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-02-25</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-02-22</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18182</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18183</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18184</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18185</th>\n",
       "      <td>1770</td>\n",
       "      <td>NaT</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18186</th>\n",
       "      <td>1770</td>\n",
       "      <td>2025-03-14</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18187 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id operation_date redcap_event_name  age_diagnosis gender  \\\n",
       "0         1            NaT                 8            NaN    1.0   \n",
       "1         1     2011-08-26                 9            NaN    NaN   \n",
       "2         1     2010-02-20                 9            NaN    NaN   \n",
       "3         1     2009-02-25                 9            NaN    NaN   \n",
       "4         1     2008-02-22                 9            NaN    NaN   \n",
       "...     ...            ...               ...            ...    ...   \n",
       "18182  1770            NaT                10            NaN    NaN   \n",
       "18183  1770            NaT                10            NaN    NaN   \n",
       "18184  1770            NaT                10            NaN    NaN   \n",
       "18185  1770            NaT                 8            NaN    1.0   \n",
       "18186  1770     2025-03-14                11            NaN    NaN   \n",
       "\n",
       "      overall_primary_tumour overall_regional_ln overall_distant_metastasis  \\\n",
       "0                        nan                 nan                        nan   \n",
       "1                        nan                 nan                        nan   \n",
       "2                        nan                 nan                        nan   \n",
       "3                        nan                 nan                        nan   \n",
       "4                        nan                 nan                        nan   \n",
       "...                      ...                 ...                        ...   \n",
       "18182                    nan                 nan                        nan   \n",
       "18183                    nan                 nan                        nan   \n",
       "18184                    nan                 nan                        nan   \n",
       "18185                    nan                 nan                        nan   \n",
       "18186                    nan                 nan                        nan   \n",
       "\n",
       "      neotx___notx neotx___chemo  ... a_e4 a_e5 a_e6 a_e7 a_c6 a_c2 a_act11  \\\n",
       "0              NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "1              NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "2              NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "3              NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "4              NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "...            ...           ...  ...  ...  ...  ...  ...  ...  ...     ...   \n",
       "18182          0.0           0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18183          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18184          NaN           NaN  ...  0.0  0.0  0.0  0.0  1.0  3.0     3.0   \n",
       "18185          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "18186          NaN           NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       "\n",
       "      readmission_30d postop_comp   los  \n",
       "0                 NaN         0.0   NaN  \n",
       "1                 NaN         NaN   NaN  \n",
       "2                 NaN         NaN  16.0  \n",
       "3                 NaN         NaN   8.0  \n",
       "4                 NaN         NaN  13.0  \n",
       "...               ...         ...   ...  \n",
       "18182             NaN         NaN   NaN  \n",
       "18183             NaN         NaN   NaN  \n",
       "18184             NaN         NaN   NaN  \n",
       "18185             NaN         NaN   NaN  \n",
       "18186             NaN         NaN   NaN  \n",
       "\n",
       "[18187 rows x 66 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Preprocess numerical and ordinal columns\n",
    "# True numerical columns: Ensure float/int dtype\n",
    "numerical_cols = ['los', 'age_diagnosis']\n",
    "for col in numerical_cols:\n",
    "    df[col] = df[col].astype(float)\n",
    "\n",
    "#Step 3\n",
    "# Ordinal columns: Treat as numerical (already float)\n",
    "ordinal_cols = (\n",
    "    [f\"gp{i}\" for i in range(1, 8)] + [f\"gs{i}\" for i in range(1, 8)] +\n",
    "    [f\"ge{i}\" for i in range(1, 7)] + [f\"gf{i}\" for i in range(1, 8)] +\n",
    "    [f\"a_hn{i}\" for i in range(1, 6)] + [\"a_hn7\", \"a_hn10\"] +\n",
    "    [f\"a_e{i}\" for i in range(1, 8)] + [\"a_c6\", \"a_c2\", \"a_act11\"]\n",
    ")\n",
    "# Subset for this example\n",
    "ordinal_cols = [col for col in ordinal_cols if col in df.columns]\n",
    "for col in ordinal_cols:\n",
    "    df[col] = df[col].astype(float)\n",
    "\n",
    "#Step 4\n",
    "# Categorical-like columns: Treat as categorical\n",
    "categorical_like_cols = [\n",
    "    'overall_primary_tumour', 'overall_regional_ln', 'overall_distant_metastasis', \n",
    "    'path_esoph_primtumour', 'path_esoph_regionalln', 'path_esoph_distantmetast'\n",
    "]\n",
    "for col in categorical_like_cols:\n",
    "    df[col] = df[col].astype(str)  # Convert to string to handle mixed types\n",
    "    df[col] = df[col].astype('category')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mice_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None):\n",
    "    \"\"\"\n",
    "    Apply MICE imputation using miceforest package\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation dataset with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation data\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "\n",
    "    # Set threads for LightGBM\n",
    "    os.environ['OMP_NUM_THREADS'] = '10'\n",
    "    \n",
    "    # Initialize the imputation kernel\n",
    "    kernel = mf.ImputationKernel(\n",
    "        df,\n",
    "        datasets=1,\n",
    "        variable_schema={\n",
    "            col: [c for c in df.columns if c != col] for col in columns_to_impute\n",
    "        },\n",
    "        random_state=42  # Using fixed seed for reproducibility, can be parameterized\n",
    "    )\n",
    "    \n",
    "    # Run imputation\n",
    "    for _ in tqdm(range(5), desc=\"MICE Imputation\"):\n",
    "        kernel.mice(\n",
    "            iterations=1,\n",
    "            verbose=False,\n",
    "            num_boost_round=80,\n",
    "            max_depth=10,\n",
    "            num_threads=10\n",
    "        )\n",
    "    \n",
    "    # Get imputed data\n",
    "    imputed_df = kernel.complete_data(0)\n",
    "    \n",
    "    # Check if there's a label encoder for redcap_event_name that needs inverse transformation\n",
    "    if 'redcap_event_name' in imputed_df.columns:\n",
    "        try:\n",
    "            # This is optional - only execute if le_redcap exists in the global scope\n",
    "            if 'le_redcap' in globals():\n",
    "                # Check if we're dealing with numeric values (could be int or float)\n",
    "                if pd.api.types.is_numeric_dtype(imputed_df['redcap_event_name']) or \\\n",
    "                   (hasattr(imputed_df['redcap_event_name'], 'cat') and pd.api.types.is_numeric_dtype(imputed_df['redcap_event_name'].cat.categories)):\n",
    "                    imputed_df['redcap_event_name'] = globals()['le_redcap'].inverse_transform(imputed_df['redcap_event_name'].astype(int))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not inverse transform redcap_event_name: {e}\")\n",
    "    \n",
    "    # Validate if validation data provided\n",
    "    validation_results = None\n",
    "    if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "        validation_results = {}\n",
    "        \n",
    "        # Compare imputed values to real values\n",
    "        for col in columns_to_impute:\n",
    "            # Get indices where values were artificially set to NaN\n",
    "            mask = validation_masks[col] & validation_df[col].isna()\n",
    "            \n",
    "            if mask.sum() == 0:\n",
    "                validation_results[col] = {\n",
    "                    'error': \"No artificially missing values\"\n",
    "                }\n",
    "                continue\n",
    "                \n",
    "            real_vals = original_values[col][mask]\n",
    "            imputed_vals = imputed_df[col][mask]\n",
    "            \n",
    "            # Calculate MAE and RMSE\n",
    "            mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "            rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "            \n",
    "            validation_results[col] = {\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'real_distribution': real_vals.describe(),\n",
    "                'imputed_distribution': imputed_vals.describe()\n",
    "            }\n",
    "    \n",
    "    return imputed_df, validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MICE Imputation: 100%|██████████| 5/5 [00:44<00:00,  8.87s/it]\n"
     ]
    }
   ],
   "source": [
    "columns_to_impute = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "imputed_df_mice, validation_results_mice = apply_mice_imputation(df, columns_to_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CUDA Diagnostics ===\n",
      "PyTorch version: 2.6.0\n",
      "CUDA version: 12.2\n",
      "\n",
      "CUDA availability check:\n",
      "torch.cuda.is_available(): False\n",
      "Could not run nvidia-smi\n",
      "\n",
      "CUDA Environment Variables:\n",
      "CUDA_VISIBLE_DEVICES: Not set\n",
      "CUDA_DEVICE_ORDER: Not set\n",
      "CUDA_HOME: Not set\n",
      "LD_LIBRARY_PATH: Not set\n",
      "\n",
      "No GPUs detected by PyTorch.\n",
      "If you see GPUs in nvidia-smi but not here, check CUDA version compatibility.\n",
      "\n",
      "=== End of Diagnostics ===\n",
      "\n",
      "\n",
      "--- Compute Canada / Cluster Environment Detection ---\n",
      "SLURM_JOB_ID: Not found\n",
      "SLURM_JOB_GPUS: Not found\n",
      "SLURM_GPUS: Not found\n",
      "CUDA_VISIBLE_DEVICES: Not found\n",
      "SLURM_JOB_PARTITION: Not found\n",
      "\n",
      "CUDA not available, using CPU\n",
      "\n",
      "--- End of Environment Detection ---\n",
      "\n",
      "Training VAE imputation model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training VAE:   0%|          | 0/100 [01:36<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 775\u001b[0m\n\u001b[1;32m    772\u001b[0m columns_to_impute \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge6\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    774\u001b[0m \u001b[38;5;66;03m# Apply VAE imputation\u001b[39;00m\n\u001b[0;32m--> 775\u001b[0m imputed_df_vae, validation_results_vae  \u001b[38;5;241m=\u001b[39m \u001b[43mapply_vae_imputation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 649\u001b[0m, in \u001b[0;36mapply_vae_imputation\u001b[0;34m(df, columns_to_impute, validation_df, validation_masks, original_values, hidden_dims, latent_dim, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# Fit and transform\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining VAE imputation model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 649\u001b[0m X_imputed \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;66;03m# Create imputed dataframe\u001b[39;00m\n\u001b[1;32m    652\u001b[0m imputed_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[0;32mIn[7], line 516\u001b[0m, in \u001b[0;36mVAEImputer.fit_transform\u001b[0;34m(self, X, columns_to_impute)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, columns_to_impute\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;124;03m    Fit model and impute missing values\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m        Data with imputed values\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Cell \u001b[0;32mIn[7], line 420\u001b[0m, in \u001b[0;36mVAEImputer.fit\u001b[0;34m(self, X, columns_to_impute)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m    419\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 420\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# Accumulate losses\u001b[39;00m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational VAE optimized for CPU usage\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=(64, 32, 16), latent_dim=8, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize VAE model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Dimension of input feature space\n",
    "        hidden_dims : tuple\n",
    "            Sizes of hidden layers in the encoder and decoder\n",
    "        latent_dim : int\n",
    "            Dimension of the latent space\n",
    "        dropout_rate : float\n",
    "            Dropout rate for regularization\n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Determine if we should use batch normalization (can be slow on CPU)\n",
    "        self.use_cpu_efficient = True  # Flag for CPU optimization\n",
    "        \n",
    "        # Encoder layers\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for h_dim in hidden_dims:\n",
    "            layer_components = [nn.Linear(prev_dim, h_dim)]\n",
    "            \n",
    "            # Use layer norm instead of batch norm if optimizing for CPU\n",
    "            if self.use_cpu_efficient:\n",
    "                layer_components.append(nn.LayerNorm(h_dim))\n",
    "            else:\n",
    "                layer_components.append(nn.BatchNorm1d(h_dim))\n",
    "                \n",
    "            layer_components.extend([\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            \n",
    "            encoder_layers.append(nn.Sequential(*layer_components))\n",
    "            prev_dim = h_dim\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList(encoder_layers)\n",
    "        \n",
    "        # Latent space mapping\n",
    "        self.mu_layer = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.logvar_layer = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        \n",
    "        # Decoder layers\n",
    "        decoder_layers = []\n",
    "        decoder_dims = list(reversed(hidden_dims))\n",
    "        \n",
    "        # First decoder layer from latent space\n",
    "        first_layer_components = [nn.Linear(latent_dim, decoder_dims[0])]\n",
    "        \n",
    "        # Use layer norm instead of batch norm if optimizing for CPU\n",
    "        if self.use_cpu_efficient:\n",
    "            first_layer_components.append(nn.LayerNorm(decoder_dims[0]))\n",
    "        else:\n",
    "            first_layer_components.append(nn.BatchNorm1d(decoder_dims[0]))\n",
    "            \n",
    "        first_layer_components.extend([\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        ])\n",
    "        \n",
    "        decoder_layers.append(nn.Sequential(*first_layer_components))\n",
    "        \n",
    "        # Remaining decoder layers\n",
    "        for i in range(len(decoder_dims) - 1):\n",
    "            layer_components = [nn.Linear(decoder_dims[i], decoder_dims[i+1])]\n",
    "            \n",
    "            # Use layer norm instead of batch norm if optimizing for CPU\n",
    "            if self.use_cpu_efficient:\n",
    "                layer_components.append(nn.LayerNorm(decoder_dims[i+1]))\n",
    "            else:\n",
    "                layer_components.append(nn.BatchNorm1d(decoder_dims[i+1]))\n",
    "                \n",
    "            layer_components.extend([\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            \n",
    "            decoder_layers.append(nn.Sequential(*layer_components))\n",
    "        \n",
    "        # Output layer\n",
    "        decoder_layers.append(nn.Linear(decoder_dims[-1], input_dim))\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList(decoder_layers)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode input to latent parameters\n",
    "        \"\"\"\n",
    "        h = x\n",
    "        for layer in self.encoder_layers:\n",
    "            h = layer(h)\n",
    "            \n",
    "        mu = self.mu_layer(h)\n",
    "        logvar = self.logvar_layer(h)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decode latent vector to reconstructed input\n",
    "        \"\"\"\n",
    "        h = z\n",
    "        for layer in self.decoder_layers:\n",
    "            h = layer(h)\n",
    "            \n",
    "        return h\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the VAE\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_reconstructed = self.decode(z)\n",
    "        \n",
    "        return x_reconstructed, mu, logvar\n",
    "    \n",
    "    \n",
    "class MaskedMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    MSE loss that only considers non-missing values\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "        \n",
    "    def forward(self, pred, target, mask):\n",
    "        \"\"\"\n",
    "        Calculate MSE loss ignoring missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pred : torch.Tensor\n",
    "            Predicted values\n",
    "        target : torch.Tensor\n",
    "            Target values\n",
    "        mask : torch.Tensor\n",
    "            Binary mask (1 for observed, 0 for missing)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        loss : torch.Tensor\n",
    "            Masked MSE loss\n",
    "        \"\"\"\n",
    "        # Only calculate loss for observed values\n",
    "        masked_pred = pred * mask\n",
    "        masked_target = target * mask\n",
    "        \n",
    "        # Calculate squared error\n",
    "        se = (masked_pred - masked_target) ** 2\n",
    "        \n",
    "        # Sum squared error and count observed values\n",
    "        sse = torch.sum(se)\n",
    "        count = torch.sum(mask)\n",
    "        \n",
    "        # Return MSE\n",
    "        return sse / (count + 1e-8)\n",
    "\n",
    "\n",
    "def kl_divergence_loss(mu, logvar):\n",
    "    \"\"\"\n",
    "    Calculate KL divergence loss\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mu : torch.Tensor\n",
    "        Mean of the latent space\n",
    "    logvar : torch.Tensor\n",
    "        Log variance of the latent space\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    kl_loss : torch.Tensor\n",
    "        KL divergence loss\n",
    "    \"\"\"\n",
    "    # KL divergence between q(z|x) and p(z)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return kl_loss\n",
    "\n",
    "\n",
    "class VAEImputer:\n",
    "    \"\"\"\n",
    "    Variational VAE Imputation model using PyTorch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hidden_dims=(64, 32, 16),\n",
    "                 latent_dim=8,\n",
    "                 batch_size=64,\n",
    "                 learning_rate=0.001,\n",
    "                 epochs=100,\n",
    "                 beta=1.0,\n",
    "                 device=None,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize VAE imputer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        hidden_dims : tuple\n",
    "            Sizes of hidden layers in the encoder and decoder\n",
    "        latent_dim : int\n",
    "            Dimension of the latent space\n",
    "        batch_size : int\n",
    "            Batch size for training\n",
    "        learning_rate : float\n",
    "            Learning rate for optimizer\n",
    "        epochs : int\n",
    "            Number of training epochs\n",
    "        beta : float\n",
    "            Weight of the KL divergence loss (beta-VAE)\n",
    "        device : torch.device\n",
    "            Device to use for training (CPU or GPU)\n",
    "        verbose : bool\n",
    "            Whether to print progress\n",
    "        \"\"\"\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.beta = beta\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Set device - Compute Canada specific approach\n",
    "        if device is None:\n",
    "            # Check for SLURM environment variables that indicate we're on a cluster\n",
    "            slurm_job_id = os.environ.get('SLURM_JOB_ID')\n",
    "            \n",
    "            if slurm_job_id:\n",
    "                print(f\"Running on Compute Canada cluster (Job ID: {slurm_job_id})\")\n",
    "                \n",
    "                # Check if GPUs were allocated\n",
    "                slurm_gpus = os.environ.get('SLURM_GPUS')\n",
    "                if slurm_gpus:\n",
    "                    print(f\"GPUs allocated: {slurm_gpus}\")\n",
    "                    \n",
    "                # On Compute Canada, we need to look at the environment variables\n",
    "                visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES')\n",
    "                print(f\"CUDA_VISIBLE_DEVICES: {visible_devices}\")\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    self.device = torch.device('cuda')\n",
    "                    n_gpus = torch.cuda.device_count()\n",
    "                    print(f\"PyTorch sees {n_gpus} GPUs\")\n",
    "                    if n_gpus > 0:\n",
    "                        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "                        # Show GPU memory\n",
    "                        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "                    else:\n",
    "                        print(\"Warning: torch.cuda.is_available() is True but device_count() is 0\")\n",
    "                        self.device = torch.device('cpu')\n",
    "                else:\n",
    "                    print(\"CUDA not available according to PyTorch\")\n",
    "                    # Try to provide more diagnostics\n",
    "                    try:\n",
    "                        if os.path.exists('/usr/bin/nvidia-smi'):\n",
    "                            import subprocess\n",
    "                            result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE)\n",
    "                            print(\"nvidia-smi output:\")\n",
    "                            print(result.stdout.decode('utf-8'))\n",
    "                        else:\n",
    "                            print(\"nvidia-smi not found in /usr/bin\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error checking for nvidia-smi: {e}\")\n",
    "                    \n",
    "                    self.device = torch.device('cpu')\n",
    "            else:\n",
    "                # Not on a cluster, use standard detection\n",
    "                if torch.cuda.is_available():\n",
    "                    self.device = torch.device('cuda')\n",
    "                    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "                else:\n",
    "                    self.device = torch.device('cpu')\n",
    "                    print(\"Using CPU\")\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        # Initialize model, scalers and masks\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.columns = None\n",
    "        \n",
    "    def fit(self, X, columns_to_impute=None):\n",
    "        \"\"\"\n",
    "        Fit VAE model for imputation\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "        columns_to_impute : list, optional\n",
    "            List of column names to impute. If None, all columns with missing values.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        # Identify columns to impute if not specified\n",
    "        if columns_to_impute is None:\n",
    "            columns_to_impute = [col for col in X.columns if X[col].isna().any()]\n",
    "        \n",
    "        # Store list of columns to impute and all columns\n",
    "        self.columns_to_impute = columns_to_impute\n",
    "        self.columns = list(X.columns)\n",
    "        \n",
    "        # Create missing value mask (1 for observed, 0 for missing)\n",
    "        missing_mask = ~X.isna()\n",
    "        \n",
    "        # Initialize scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Simple imputer for initial values (will be refined by VAE)\n",
    "        simple_imputer = SimpleImputer(strategy='mean')\n",
    "        X_simple_imputed = pd.DataFrame(\n",
    "            simple_imputer.fit_transform(X),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Scale data\n",
    "        X_scaled = self.scaler.fit_transform(X_simple_imputed)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_tensor = torch.FloatTensor(X_scaled)\n",
    "        mask_tensor = torch.FloatTensor(missing_mask.values)\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = TensorDataset(X_tensor, mask_tensor)\n",
    "        # Use multiple workers if on CPU for better performance\n",
    "        num_workers = 0\n",
    "        if self.device.type == 'cpu':\n",
    "            import multiprocessing\n",
    "            num_workers = min(2, multiprocessing.cpu_count() // 2)  # Use at most half the cores\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=(self.device.type == 'cuda')  # Only pin memory if using CUDA\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        input_dim = X.shape[1]\n",
    "        self.model = VAE(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dims=self.hidden_dims,\n",
    "            latent_dim=self.latent_dim\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # Initialize loss function\n",
    "        recon_loss_fn = MaskedMSELoss()\n",
    "        \n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        \n",
    "        # Create progress bar if verbose\n",
    "        pbar = range(self.epochs)\n",
    "        if self.verbose:\n",
    "            pbar = tqdm(pbar, desc=\"Training VAE\")\n",
    "            \n",
    "        for epoch in pbar:\n",
    "            epoch_loss = 0.0\n",
    "            epoch_recon_loss = 0.0\n",
    "            epoch_kl_loss = 0.0\n",
    "            \n",
    "            for x, mask in dataloader:\n",
    "                # Move tensors to device\n",
    "                x = x.to(self.device)\n",
    "                mask = mask.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                x_reconstructed, mu, logvar = self.model(x)\n",
    "                \n",
    "                # Calculate losses\n",
    "                recon_loss = recon_loss_fn(x_reconstructed, x, mask)\n",
    "                kl_loss = kl_divergence_loss(mu, logvar) / x.size(0)  # Normalize by batch size\n",
    "                \n",
    "                # Total loss (beta-VAE formulation)\n",
    "                loss = recon_loss + self.beta * kl_loss\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Accumulate losses\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_recon_loss += recon_loss.item()\n",
    "                epoch_kl_loss += kl_loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            if self.verbose:\n",
    "                pbar.set_postfix({\n",
    "                    \"loss\": epoch_loss / len(dataloader),\n",
    "                    \"recon\": epoch_recon_loss / len(dataloader),\n",
    "                    \"kl\": epoch_kl_loss / len(dataloader)\n",
    "                })\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Impute missing values using trained VAE model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_df : pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        # Create copy to avoid modifying original\n",
    "        imputed_df = X.copy()\n",
    "        \n",
    "        # Get missing value mask\n",
    "        missing_mask = X.isna()\n",
    "        \n",
    "        # Use simple imputation for initial values\n",
    "        simple_imputer = SimpleImputer(strategy='mean')\n",
    "        X_simple_imputed = pd.DataFrame(\n",
    "            simple_imputer.fit_transform(X),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Scale data\n",
    "        X_scaled = self.scaler.transform(X_simple_imputed)\n",
    "        \n",
    "        # Convert to PyTorch tensor\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass through VAE\n",
    "            X_reconstructed, _, _ = self.model(X_tensor)\n",
    "            \n",
    "            # Convert reconstructed values to numpy\n",
    "            X_reconstructed_np = X_reconstructed.cpu().numpy()\n",
    "            \n",
    "            # Inverse transform to original scale\n",
    "            X_reconstructed_orig = self.scaler.inverse_transform(X_reconstructed_np)\n",
    "            \n",
    "            # Create DataFrame with reconstructed values\n",
    "            X_reconstructed_df = pd.DataFrame(\n",
    "                X_reconstructed_orig,\n",
    "                columns=X.columns,\n",
    "                index=X.index\n",
    "            )\n",
    "            \n",
    "            # Replace missing values with imputed values for specified columns\n",
    "            for col in self.columns_to_impute:\n",
    "                # Only replace missing values\n",
    "                imputed_df.loc[missing_mask[col], col] = X_reconstructed_df.loc[missing_mask[col], col]\n",
    "        \n",
    "        return imputed_df\n",
    "    \n",
    "    def fit_transform(self, X, columns_to_impute=None):\n",
    "        \"\"\"\n",
    "        Fit model and impute missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "        columns_to_impute : list, optional\n",
    "            List of column names to impute. If None, all columns with missing values.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_df : pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        self.fit(X, columns_to_impute)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "def apply_vae_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None,\n",
    "                        hidden_dims=(64, 32, 16), latent_dim=8, batch_size=64, epochs=100, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Apply VAE imputation to data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation dataset with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation data\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "    hidden_dims : tuple\n",
    "        Sizes of hidden layers\n",
    "    latent_dim : int\n",
    "        Dimension of the latent space\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "    epochs : int\n",
    "        Number of training epochs\n",
    "    learning_rate : float\n",
    "        Learning rate for optimizer\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "    # Extract relevant columns including potential predictors\n",
    "    # Use all columns except those with excessive missing values\n",
    "    threshold = 0.5  # Columns with more than 50% missing values are excluded\n",
    "    columns_to_use = [col for col in df.columns \n",
    "                     if df[col].isna().mean() < threshold]\n",
    "    \n",
    "    # Ensure all columns_to_impute are included\n",
    "    for col in columns_to_impute:\n",
    "        if col not in columns_to_use:\n",
    "            columns_to_use.append(col)\n",
    "    \n",
    "    # Extract subset of data\n",
    "    X = df[columns_to_use].copy()\n",
    "    \n",
    "    # Ensure all columns are numeric\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    \n",
    "    # Compute Canada specific detection\n",
    "    print(\"\\n--- Compute Canada / Cluster Environment Detection ---\")\n",
    "    slurm_info = {\n",
    "        'SLURM_JOB_ID': os.environ.get('SLURM_JOB_ID', 'Not found'),\n",
    "        'SLURM_JOB_GPUS': os.environ.get('SLURM_JOB_GPUS', 'Not found'),\n",
    "        'SLURM_GPUS': os.environ.get('SLURM_GPUS', 'Not found'), \n",
    "        'CUDA_VISIBLE_DEVICES': os.environ.get('CUDA_VISIBLE_DEVICES', 'Not found'),\n",
    "        'SLURM_JOB_PARTITION': os.environ.get('SLURM_JOB_PARTITION', 'Not found'),\n",
    "    }\n",
    "    \n",
    "    for key, value in slurm_info.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Check CUDA availability with Compute Canada settings\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        n_gpus = torch.cuda.device_count()\n",
    "        print(f\"\\nPyTorch detects {n_gpus} GPUs\")\n",
    "        \n",
    "        # On Compute Canada clusters, sometimes device_count is incorrect\n",
    "        # but we can still use cuda:0 if CUDA_VISIBLE_DEVICES is set\n",
    "        if n_gpus == 0 and os.environ.get('CUDA_VISIBLE_DEVICES') is not None:\n",
    "            print(\"Detected potential Compute Canada environment mismatch\")\n",
    "            print(\"Attempting to force GPU usage...\")\n",
    "            try:\n",
    "                # Try to explicitly set the device\n",
    "                device = torch.device('cuda:0')\n",
    "                # Test if it works\n",
    "                test_tensor = torch.zeros(1).to(device)\n",
    "                print(\"Successfully forced GPU usage!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Force attempt failed: {e}\")\n",
    "                device = torch.device('cpu')\n",
    "                print(\"Falling back to CPU\")\n",
    "        else:\n",
    "            for i in range(n_gpus):\n",
    "                print(f\"GPU #{i}: {torch.cuda.get_device_name(i)}\")\n",
    "                props = torch.cuda.get_device_properties(i)\n",
    "                print(f\"  Memory: {props.total_memory / 1e9:.2f} GB\")\n",
    "                print(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"\\nCUDA not available, using CPU\")\n",
    "        \n",
    "        # Check if we're on a compute node that should have GPUs\n",
    "        if 'SLURM_JOB_GPUS' in os.environ or 'SLURM_GPUS' in os.environ:\n",
    "            print(\"Warning: GPUs were allocated in SLURM but PyTorch cannot detect them\")\n",
    "            print(\"This might be due to a configuration issue.\")\n",
    "            print(\"Try running 'module load cuda' before starting your script\")\n",
    "    \n",
    "    print(\"\\n--- End of Environment Detection ---\\n\")\n",
    "    \n",
    "    # Initialize VAE imputer\n",
    "    imputer = VAEImputer(\n",
    "        hidden_dims=hidden_dims,\n",
    "        latent_dim=latent_dim,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        epochs=epochs,\n",
    "        device=device,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Fit and transform\n",
    "    print(\"Training VAE imputation model...\")\n",
    "    X_imputed = imputer.fit_transform(X, columns_to_impute)\n",
    "    \n",
    "    # Create imputed dataframe\n",
    "    imputed_df = df.copy()\n",
    "    imputed_df[columns_to_impute] = X_imputed[columns_to_impute]\n",
    "    \n",
    "    # Validate if validation data provided\n",
    "    validation_results = None\n",
    "    if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "        validation_results = {}\n",
    "        \n",
    "        # Extract validation data\n",
    "        X_val = validation_df[columns_to_use].copy()\n",
    "        \n",
    "        # Ensure all validation columns are numeric\n",
    "        for col in X_val.columns:\n",
    "            X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "            \n",
    "        # Impute validation data\n",
    "        X_val_imputed = imputer.transform(X_val)\n",
    "        \n",
    "        # Compare imputed values to real values\n",
    "        with tqdm(columns_to_impute, desc=\"Validating results\") as pbar:\n",
    "            for col in pbar:\n",
    "                pbar.set_description(f\"Validating {col}\")\n",
    "                # Get indices where values were artificially set to NaN\n",
    "                mask = validation_masks[col] & validation_df[col].isna()\n",
    "                \n",
    "                if mask.sum() == 0:\n",
    "                    validation_results[col] = {\n",
    "                        'error': \"No artificially missing values\"\n",
    "                    }\n",
    "                    continue\n",
    "                    \n",
    "                real_vals = original_values[col][mask]\n",
    "                imputed_vals = X_val_imputed.loc[mask, col]\n",
    "                \n",
    "                # Calculate MAE and RMSE\n",
    "                mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "                rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "                \n",
    "                validation_results[col] = {\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'real_distribution': real_vals.describe(),\n",
    "                    'imputed_distribution': pd.Series(imputed_vals).describe()\n",
    "                }\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.set_postfix({\"MAE\": f\"{mae:.4f}\", \"RMSE\": f\"{rmse:.4f}\"})\n",
    "    \n",
    "    return imputed_df, validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # More detailed CUDA diagnostics\n",
    "    print(\"\\n=== CUDA Diagnostics ===\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    print(f\"\\nCUDA availability check:\")\n",
    "    print(f\"torch.cuda.is_available(): {torch.cuda.is_available()}\")\n",
    "    \n",
    "    # Show NVIDIA driver info if available\n",
    "    try:\n",
    "        import subprocess\n",
    "        nvidia_info = subprocess.check_output(\"nvidia-smi\", shell=True).decode()\n",
    "        print(\"\\nNVIDIA-SMI output:\")\n",
    "        print(nvidia_info)\n",
    "    except:\n",
    "        print(\"Could not run nvidia-smi\")\n",
    "    \n",
    "    # CUDA environment variables\n",
    "    print(\"\\nCUDA Environment Variables:\")\n",
    "    import os\n",
    "    cuda_vars = [\n",
    "        \"CUDA_VISIBLE_DEVICES\",\n",
    "        \"CUDA_DEVICE_ORDER\",\n",
    "        \"CUDA_HOME\",\n",
    "        \"LD_LIBRARY_PATH\"\n",
    "    ]\n",
    "    for var in cuda_vars:\n",
    "        print(f\"{var}: {os.environ.get(var, 'Not set')}\")\n",
    "    \n",
    "    # Try to force CUDA if available\n",
    "    if torch.cuda.device_count() > 0:\n",
    "        print(\"\\nGPUs detected by PyTorch:\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"  GPU #{i}: {torch.cuda.get_device_name(i)}\")\n",
    "            \n",
    "        # Try to force CUDA initialization\n",
    "        try:\n",
    "            print(\"\\nAttempting to force CUDA initialization...\")\n",
    "            x = torch.zeros(1).cuda()\n",
    "            print(f\"  Success! Test tensor on: {x.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to initialize CUDA: {e}\")\n",
    "            \n",
    "            # Suggest potential fixes\n",
    "            print(\"\\nPotential fixes:\")\n",
    "            print(\"1. Make sure your NVIDIA drivers match the CUDA version PyTorch was built with\")\n",
    "            print(\"2. Check if you have enough GPU memory available\")\n",
    "            print(\"3. Try setting: os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\")\n",
    "            print(\"4. Reinstall PyTorch with the correct CUDA version:\")\n",
    "            print(\"   pip install torch==2.0.0+cu118 -f https://download.pytorch.org/whl/cu118/torch_stable.html\")\n",
    "            \n",
    "            # Try enabling TF32 or reduced precision\n",
    "            try:\n",
    "                print(\"\\nTrying to enable TF32...\")\n",
    "                torch.backends.cuda.matmul.allow_tf32 = True\n",
    "                torch.backends.cudnn.allow_tf32 = True\n",
    "                x = torch.zeros(1).cuda()\n",
    "                print(f\"  Success with TF32! Test tensor on: {x.device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Failed with TF32: {e}\")\n",
    "    else:\n",
    "        print(\"\\nNo GPUs detected by PyTorch.\")\n",
    "        print(\"If you see GPUs in nvidia-smi but not here, check CUDA version compatibility.\")\n",
    "    \n",
    "    print(\"\\n=== End of Diagnostics ===\\n\")\n",
    "    \n",
    "    columns_to_impute = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "\n",
    "    # Apply VAE imputation\n",
    "    imputed_df_vae, validation_results_vae  = apply_vae_imputation(df, columns_to_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Denoising Autoencoder (DAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnDAE(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Denoising VAE implementation using scikit-learn's MLPRegressor\n",
    "    This avoids TensorFlow execution mode issues completely\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                    hidden_layer_sizes=(64, 32, 16, 32, 64),\n",
    "                    activation='relu',\n",
    "                    max_iter=200,\n",
    "                    learning_rate='adaptive',\n",
    "                    learning_rate_init=0.001,\n",
    "                    noise_factor=0.1,\n",
    "                    alpha=0.0001,\n",
    "                    verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize DAE model parameters\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        hidden_layer_sizes : tuple\n",
    "            Size of hidden layers\n",
    "        activation : str\n",
    "            Activation function ('relu', 'tanh', 'logistic')\n",
    "        max_iter : int\n",
    "            Maximum number of iterations\n",
    "        learning_rate : str\n",
    "            Learning rate schedule\n",
    "        learning_rate_init : float\n",
    "            Initial learning rate\n",
    "        noise_factor : float\n",
    "            Amount of noise to add for denoising effect\n",
    "        alpha : float\n",
    "            L2 regularization parameter\n",
    "        verbose : bool\n",
    "            Whether to print progress\n",
    "        \"\"\"\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.noise_factor = noise_factor\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "        self.scaler_X = StandardScaler()\n",
    "        \n",
    "    def fit(self, X, missing_mask=None):\n",
    "        \"\"\"\n",
    "        Fit the DAE model to input data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray or pandas.DataFrame\n",
    "            Input data with missing values (NaN)\n",
    "        missing_mask : numpy.ndarray\n",
    "            Binary mask where 1 indicates observed value, 0 for missing\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        # Convert to numpy array if DataFrame\n",
    "        is_df = isinstance(X, pd.DataFrame)\n",
    "        if is_df:\n",
    "            if self.verbose:\n",
    "                print(\"Converting DataFrame to numpy array\")\n",
    "            X_values = X.values\n",
    "        else:\n",
    "            X_values = X\n",
    "        \n",
    "        # Create missing mask if not provided\n",
    "        if missing_mask is None:\n",
    "            missing_mask = ~np.isnan(X_values)\n",
    "        \n",
    "        # Initial imputation (replace missing values with column means)\n",
    "        X_imputed = np.copy(X_values)\n",
    "        col_means = np.nanmean(X_values, axis=0)\n",
    "        for col in range(X_values.shape[1]):\n",
    "            mask = np.isnan(X_values[:, col])\n",
    "            X_imputed[mask, col] = col_means[col]\n",
    "        \n",
    "        # Scale the data\n",
    "        X_scaled = self.scaler_X.fit_transform(X_imputed)\n",
    "        \n",
    "        # Add noise to create denoising effect\n",
    "        X_noisy = X_scaled + np.random.normal(0, self.noise_factor, X_scaled.shape)\n",
    "        \n",
    "        # Apply mask to noise (only add noise to observed values)\n",
    "        X_noisy = X_noisy * missing_mask + X_scaled * (~missing_mask)\n",
    "        \n",
    "        # Create the model\n",
    "        self.model = MLPRegressor(\n",
    "            hidden_layer_sizes=self.hidden_layer_sizes,\n",
    "            activation=self.activation,\n",
    "            solver='adam',\n",
    "            alpha=self.alpha,\n",
    "            batch_size='auto',\n",
    "            learning_rate=self.learning_rate,\n",
    "            learning_rate_init=self.learning_rate_init,\n",
    "            max_iter=self.max_iter,\n",
    "            shuffle=True,\n",
    "            random_state=42,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Fitting DAE model...\")\n",
    "        \n",
    "        # Train the model to reconstruct the original data\n",
    "        self.model.fit(X_noisy, X_scaled)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, missing_mask=None):\n",
    "        \"\"\"\n",
    "        Impute missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray or pandas.DataFrame\n",
    "            Data with missing values (NaN)\n",
    "        missing_mask : numpy.ndarray\n",
    "            Binary mask where 1 indicates observed value, 0 for missing\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        X_imputed : numpy.ndarray or pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        # Handle DataFrame input\n",
    "        is_df = isinstance(X, pd.DataFrame)\n",
    "        if is_df:\n",
    "            columns = X.columns\n",
    "            index = X.index\n",
    "            X_values = X.values\n",
    "        else:\n",
    "            X_values = X\n",
    "        \n",
    "        # Create missing mask if not provided\n",
    "        if missing_mask is None:\n",
    "            missing_mask = ~np.isnan(X_values)\n",
    "        \n",
    "        # Initial imputation for model input\n",
    "        X_imputed = np.copy(X_values)\n",
    "        col_means = np.nanmean(X_values, axis=0)\n",
    "        for col in range(X_values.shape[1]):\n",
    "            mask = np.isnan(X_values[:, col])\n",
    "            X_imputed[mask, col] = col_means[col]\n",
    "        \n",
    "        # Scale data\n",
    "        X_scaled = self.scaler_X.transform(X_imputed)\n",
    "        \n",
    "        # Reconstruct data\n",
    "        X_reconstructed_scaled = self.model.predict(X_scaled)\n",
    "        \n",
    "        # Unscale data\n",
    "        X_reconstructed = self.scaler_X.inverse_transform(X_reconstructed_scaled)\n",
    "        \n",
    "        # Only replace missing values with reconstructed values\n",
    "        X_final = np.copy(X_values)\n",
    "        mask_missing = ~missing_mask\n",
    "        X_final[mask_missing] = X_reconstructed[mask_missing]\n",
    "        \n",
    "        # Return DataFrame if input was DataFrame\n",
    "        if is_df:\n",
    "            return pd.DataFrame(X_final, index=index, columns=columns)\n",
    "        \n",
    "        return X_final\n",
    "    \n",
    "    def fit_transform(self, X, missing_mask=None):\n",
    "        \"\"\"\n",
    "        Fit the model and impute missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray or pandas.DataFrame\n",
    "            Data with missing values (NaN)\n",
    "        missing_mask : numpy.ndarray\n",
    "            Binary mask where 1 indicates observed value, 0 for missing\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        X_imputed : numpy.ndarray or pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        return self.fit(X, missing_mask).transform(X, missing_mask)\n",
    "\n",
    "\n",
    "def apply_sklearn_dae_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None):\n",
    "    \"\"\"\n",
    "    Apply scikit-learn based DAE imputation to data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation data with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract data to impute\n",
    "        X = df[columns_to_impute].copy()\n",
    "        \n",
    "        # Ensure all columns are numeric\n",
    "        for col in columns_to_impute:\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "        \n",
    "        # Create missing mask\n",
    "        missing_mask = ~X.isna()\n",
    "        \n",
    "        print(f\"Data shape: {X.shape}\")\n",
    "        print(f\"Missing values: {X.isna().sum().sum()}\")\n",
    "        \n",
    "        # Initialize DAE model with adaptive parameters based on data size\n",
    "        neurons_per_layer = min(128, max(16, X.shape[0] // 100))\n",
    "        print(f\"Using {neurons_per_layer} neurons per layer\")\n",
    "        \n",
    "        dae = SklearnDAE(\n",
    "            hidden_layer_sizes=(neurons_per_layer, neurons_per_layer//2, neurons_per_layer//4, neurons_per_layer//2, neurons_per_layer),\n",
    "            activation='relu',\n",
    "            max_iter=200,\n",
    "            learning_rate='adaptive',\n",
    "            learning_rate_init=0.001,\n",
    "            noise_factor=0.1,\n",
    "            alpha=0.0001,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Fit and transform\n",
    "        X_imputed = dae.fit_transform(X, missing_mask.values)\n",
    "        \n",
    "        # Create imputed dataframe\n",
    "        imputed_df = df.copy()\n",
    "        imputed_df[columns_to_impute] = X_imputed\n",
    "        \n",
    "        # Validate if required\n",
    "        validation_results = None\n",
    "        if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "            validation_results = {}\n",
    "            \n",
    "            # Extract validation data\n",
    "            X_val = validation_df[columns_to_impute].copy()\n",
    "            \n",
    "            # Ensure numeric\n",
    "            for col in columns_to_impute:\n",
    "                X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "                \n",
    "            # Create validation mask\n",
    "            val_mask = ~X_val.isna()\n",
    "            \n",
    "            # Impute validation data\n",
    "            X_val_imputed = dae.transform(X_val, val_mask.values)\n",
    "            \n",
    "            # Compare imputed values to real values\n",
    "            for col in columns_to_impute:\n",
    "                mask = validation_masks[col] & validation_df[col].isna()\n",
    "                \n",
    "                if mask.sum() == 0:\n",
    "                    validation_results[col] = {'error': \"No artificially missing values\"}\n",
    "                    continue\n",
    "                    \n",
    "                real_vals = original_values[col][mask]\n",
    "                imputed_vals = X_val_imputed[col][mask]\n",
    "                \n",
    "                mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "                rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "                \n",
    "                validation_results[col] = {\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'real_distribution': real_vals.describe(),\n",
    "                    'imputed_distribution': imputed_vals.describe()\n",
    "                }\n",
    "        \n",
    "        return imputed_df, validation_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in scikit-learn DAE imputation: {e}\")\n",
    "        # Fallback to simple imputation\n",
    "        result_df = df.copy()\n",
    "        for col in columns_to_impute:\n",
    "            result_df[col] = pd.to_numeric(result_df[col], errors='coerce')\n",
    "            result_df[col] = result_df[col].fillna(result_df[col].mean())\n",
    "        return result_df, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (18187, 6)\n",
      "Missing values: 92917\n",
      "Using 128 neurons per layer\n",
      "Converting DataFrame to numpy array\n",
      "Fitting DAE model...\n",
      "Iteration 1, loss = 0.16864496\n",
      "Iteration 2, loss = 0.00562373\n",
      "Iteration 3, loss = 0.00292884\n",
      "Iteration 4, loss = 0.00234576\n",
      "Iteration 5, loss = 0.00220938\n",
      "Iteration 6, loss = 0.00178736\n",
      "Iteration 7, loss = 0.00187679\n",
      "Iteration 8, loss = 0.00188498\n",
      "Iteration 9, loss = 0.00174109\n",
      "Iteration 10, loss = 0.00159538\n",
      "Iteration 11, loss = 0.00152841\n",
      "Iteration 12, loss = 0.00157138\n",
      "Iteration 13, loss = 0.00186228\n",
      "Iteration 14, loss = 0.00165298\n",
      "Iteration 15, loss = 0.00148256\n",
      "Iteration 16, loss = 0.00149563\n",
      "Iteration 17, loss = 0.00141770\n",
      "Iteration 18, loss = 0.00145362\n",
      "Iteration 19, loss = 0.00123678\n",
      "Iteration 20, loss = 0.00132795\n",
      "Iteration 21, loss = 0.00138053\n",
      "Iteration 22, loss = 0.00141435\n",
      "Iteration 23, loss = 0.00142925\n",
      "Iteration 24, loss = 0.00143241\n",
      "Iteration 25, loss = 0.00130185\n",
      "Iteration 26, loss = 0.00119649\n",
      "Iteration 27, loss = 0.00129339\n",
      "Iteration 28, loss = 0.00131087\n",
      "Iteration 29, loss = 0.00127413\n",
      "Iteration 30, loss = 0.00131043\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# Define columns to impute\n",
    "columns_to_impute = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "\n",
    "# Apply DAE imputation\n",
    "imputed_df_dae, validation_results_dae = apply_sklearn_dae_imputation(df, columns_to_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.086284\n",
       "1        1.086284\n",
       "2        1.086284\n",
       "3        1.086284\n",
       "4        1.086284\n",
       "           ...   \n",
       "18182    1.086284\n",
       "18183    1.086284\n",
       "18184    2.000000\n",
       "18185    1.086284\n",
       "18186    1.086284\n",
       "Name: ge1, Length: 18187, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_df_dae['ge1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Transformer based DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler  # For mixed precision training\n",
    "import time\n",
    "\n",
    "class TransformerImputer:\n",
    "    \"\"\"\n",
    "    A scikit-learn compatible transformer-based imputation model\n",
    "    with CUDA optimizations for faster training and inference\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 d_model=64,\n",
    "                 nhead=4,\n",
    "                 num_encoder_layers=2,\n",
    "                 dim_feedforward=128,\n",
    "                 dropout=0.1,\n",
    "                 batch_size=128,  # Increased batch size for GPU\n",
    "                 epochs=100,\n",
    "                 lr=0.001,\n",
    "                 device=None,\n",
    "                 verbose=True,\n",
    "                 num_workers=4,  # Parallel data loading\n",
    "                 use_amp=True,    # Use mixed precision training\n",
    "                 pin_memory=True): # Pin memory for faster data transfer\n",
    "        \"\"\"\n",
    "        Initialize transformer imputer with CUDA optimizations\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        d_model : int\n",
    "            Size of transformer embedding dimension\n",
    "        nhead : int\n",
    "            Number of attention heads\n",
    "        num_encoder_layers : int\n",
    "            Number of transformer encoder layers\n",
    "        dim_feedforward : int\n",
    "            Dimension of feedforward network\n",
    "        dropout : float\n",
    "            Dropout rate\n",
    "        batch_size : int\n",
    "            Batch size for training (larger for GPU)\n",
    "        epochs : int\n",
    "            Number of training epochs\n",
    "        lr : float\n",
    "            Learning rate\n",
    "        device : str\n",
    "            Device to train on ('cuda' or 'cpu')\n",
    "        verbose : bool\n",
    "            Whether to print progress\n",
    "        num_workers : int\n",
    "            Number of worker processes for data loading\n",
    "        use_amp : bool\n",
    "            Whether to use automatic mixed precision\n",
    "        pin_memory : bool\n",
    "            Whether to pin memory for faster data transfer to GPU\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.num_workers = num_workers\n",
    "        self.use_amp = use_amp\n",
    "        self.pin_memory = pin_memory\n",
    "        \n",
    "        # Set device (use GPU if available)\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            \n",
    "        # Check for GPU and print info if available\n",
    "        if torch.cuda.is_available() and self.verbose:\n",
    "            cuda_device_count = torch.cuda.device_count()\n",
    "            current_device = torch.cuda.current_device()\n",
    "            device_name = torch.cuda.get_device_name(current_device)\n",
    "            print(f\"Using CUDA: {device_name} (Device {current_device} of {cuda_device_count})\")\n",
    "            # Optimize memory allocation for GPU\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        \n",
    "    class TransformerModel(nn.Module):\n",
    "        \"\"\"\n",
    "        Transformer model for imputation with CUDA optimizations\n",
    "        \"\"\"\n",
    "        def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout):\n",
    "            super().__init__()\n",
    "            \n",
    "            # Input projection to d_model dimensions\n",
    "            self.input_projection = nn.Linear(input_dim, d_model)\n",
    "            \n",
    "            # Positional encoding (simple learnable embeddings)\n",
    "            self.pos_embedding = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "            \n",
    "            # Transformer encoder\n",
    "            encoder_layers = nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.transformer_encoder = nn.TransformerEncoder(\n",
    "                encoder_layer=encoder_layers,\n",
    "                num_layers=num_encoder_layers\n",
    "            )\n",
    "            \n",
    "            # Output layer\n",
    "            self.output_layer = nn.Linear(d_model, 1)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # Get shape information for debugging\n",
    "            batch_size, seq_len = x.shape\n",
    "            \n",
    "            # Project input to d_model dimensions\n",
    "            x = self.input_projection(x)\n",
    "            \n",
    "            # Reshape for transformer - add sequence dimension if needed\n",
    "            x = x.unsqueeze(1) if x.dim() == 2 else x\n",
    "            \n",
    "            # Add positional encoding - properly broadcast\n",
    "            pos_emb = self.pos_embedding.expand(x.size(0), -1, -1)\n",
    "            x = x + pos_emb\n",
    "            \n",
    "            # Pass through transformer encoder\n",
    "            x = self.transformer_encoder(x)\n",
    "            \n",
    "            # Global mean pooling\n",
    "            x = torch.mean(x, dim=1)\n",
    "            \n",
    "            # Output projection\n",
    "            x = self.output_layer(x)\n",
    "            \n",
    "            # Ensure output is the right shape\n",
    "            return x.squeeze(-1)\n",
    "    \n",
    "    def fit(self, X, columns_to_impute=None):\n",
    "        \"\"\"\n",
    "        Fit transformer models for each column to be imputed\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "        columns_to_impute : list, optional\n",
    "            List of column names to impute. If None, all columns with missing values.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        # Identify columns to impute if not specified\n",
    "        if columns_to_impute is None:\n",
    "            columns_to_impute = [col for col in X.columns if X[col].isna().any()]\n",
    "        \n",
    "        # Store list of columns to impute\n",
    "        self.columns_to_impute = columns_to_impute\n",
    "        \n",
    "        # Initial simple imputation for training\n",
    "        simple_imputer = SimpleImputer(strategy='mean')\n",
    "        X_simple_imputed = pd.DataFrame(\n",
    "            simple_imputer.fit_transform(X),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Create a progress bar for columns\n",
    "        pbar = tqdm(columns_to_impute, desc=\"Training transformer models\", position=0)\n",
    "        \n",
    "        # Create a separate model for each column to impute\n",
    "        for col in pbar:\n",
    "            pbar.set_description(f\"Training model for {col}\")\n",
    "            \n",
    "            # Get data excluding the target column\n",
    "            X_train = X_simple_imputed.drop(columns=[col])\n",
    "            y_train = X_simple_imputed[col]\n",
    "            \n",
    "            # Scale features\n",
    "            scaler_X = StandardScaler()\n",
    "            scaler_y = StandardScaler()\n",
    "            \n",
    "            X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "            y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Convert to PyTorch tensors\n",
    "            X_tensor = torch.FloatTensor(X_train_scaled)\n",
    "            y_tensor = torch.FloatTensor(y_train_scaled)\n",
    "            \n",
    "            # Create dataset and dataloader with CUDA optimizations\n",
    "            dataset = TensorDataset(X_tensor, y_tensor)\n",
    "            dataloader = DataLoader(\n",
    "                dataset, \n",
    "                batch_size=self.batch_size, \n",
    "                shuffle=True,\n",
    "                num_workers=self.num_workers,\n",
    "                pin_memory=self.pin_memory\n",
    "            )\n",
    "            \n",
    "            # Initialize model\n",
    "            input_dim = X_train.shape[1]\n",
    "            model = self.TransformerModel(\n",
    "                input_dim=input_dim,\n",
    "                d_model=self.d_model,\n",
    "                nhead=self.nhead,\n",
    "                num_encoder_layers=self.num_encoder_layers,\n",
    "                dim_feedforward=self.dim_feedforward,\n",
    "                dropout=self.dropout\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Initialize optimizer with weight decay for regularization\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=self.lr, weight_decay=1e-4)\n",
    "            \n",
    "            # Learning rate scheduler for better convergence\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=0.5, patience=5, verbose=self.verbose\n",
    "            )\n",
    "            \n",
    "            # Initialize gradient scaler for mixed precision training\n",
    "            scaler = GradScaler() if self.use_amp else None\n",
    "            \n",
    "            # Loss function\n",
    "            criterion = nn.MSELoss()\n",
    "            \n",
    "            # Training loop\n",
    "            model.train()\n",
    "            epoch_pbar = tqdm(range(self.epochs), desc=f\"Training epochs for {col}\", \n",
    "                              leave=False, position=1, disable=not self.verbose)\n",
    "            \n",
    "            # Pre-allocate tensors on device to avoid repeated allocations\n",
    "            batch_X_device = None\n",
    "            batch_y_device = None\n",
    "            \n",
    "            for epoch in epoch_pbar:\n",
    "                running_loss = 0.0\n",
    "                \n",
    "                for batch_X, batch_y in dataloader:\n",
    "                    # Move data to device (with pre-allocation optimization)\n",
    "                    if batch_X_device is None or batch_X_device.size(0) != batch_X.size(0):\n",
    "                        batch_X_device = batch_X.to(self.device, non_blocking=True)\n",
    "                        batch_y_device = batch_y.to(self.device, non_blocking=True)\n",
    "                    else:\n",
    "                        batch_X_device.copy_(batch_X, non_blocking=True)\n",
    "                        batch_y_device.copy_(batch_y, non_blocking=True)\n",
    "                    \n",
    "                    # Zero gradients\n",
    "                    optimizer.zero_grad(set_to_none=True)  # More efficient than zeroing\n",
    "                    \n",
    "                    # Mixed precision training\n",
    "                    if self.use_amp:\n",
    "                        with autocast():\n",
    "                            outputs = model(batch_X_device)\n",
    "                            loss = criterion(outputs, batch_y_device)\n",
    "                        \n",
    "                        # Scale loss and perform backward pass\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        # Forward pass\n",
    "                        outputs = model(batch_X_device)\n",
    "                        \n",
    "                        # Calculate loss\n",
    "                        loss = criterion(outputs, batch_y_device)\n",
    "                        \n",
    "                        # Backward pass and optimize\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    running_loss += loss.item() * batch_X.size(0)\n",
    "                \n",
    "                # Calculate epoch loss\n",
    "                epoch_loss = running_loss / len(dataset)\n",
    "                epoch_pbar.set_postfix({\"loss\": f\"{epoch_loss:.6f}\"})\n",
    "                \n",
    "                # Update learning rate based on performance\n",
    "                scheduler.step(epoch_loss)\n",
    "            \n",
    "            # Store model and scalers\n",
    "            self.models[col] = model\n",
    "            self.scalers[col] = {\n",
    "                'X': scaler_X,\n",
    "                'y': scaler_y\n",
    "            }\n",
    "            \n",
    "            # Free up memory after training each model\n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Impute missing values using trained models\n",
    "        with fixed batching for GPU acceleration\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_df : pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        # Create copy to avoid modifying original\n",
    "        imputed_df = X.copy()\n",
    "        \n",
    "        # Simple imputation for features (not targets)\n",
    "        simple_imputer = SimpleImputer(strategy='mean')\n",
    "        X_simple_imputed = pd.DataFrame(\n",
    "            simple_imputer.fit_transform(X),\n",
    "            columns=X.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Create progress bar for transformation\n",
    "        pbar = tqdm(self.columns_to_impute, desc=\"Imputing columns\", position=0)\n",
    "        \n",
    "        # Define inference batch size\n",
    "        inference_batch_size = 1000  # Process in smaller chunks\n",
    "        \n",
    "        # Impute each column\n",
    "        for col in pbar:\n",
    "            pbar.set_description(f\"Imputing {col}\")\n",
    "            \n",
    "            # Get missing values mask\n",
    "            missing_mask = X[col].isna()\n",
    "            \n",
    "            # If no missing values in this column, continue\n",
    "            if not missing_mask.any():\n",
    "                continue\n",
    "            \n",
    "            # Get data excluding the target column\n",
    "            X_pred = X_simple_imputed.drop(columns=[col])\n",
    "            \n",
    "            # Get row indices with missing values\n",
    "            missing_indices = missing_mask[missing_mask].index\n",
    "            \n",
    "            # Handle the case of no missing values to impute\n",
    "            if len(missing_indices) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Debug info\n",
    "            total_missing = len(missing_indices)\n",
    "            print(f\"Column {col}: Need to impute {total_missing} values\")\n",
    "            \n",
    "            # Prepare data for prediction\n",
    "            X_pred_missing = X_pred.loc[missing_indices]\n",
    "            \n",
    "            # Scale features\n",
    "            scaler_X = self.scalers[col]['X']\n",
    "            scaler_y = self.scalers[col]['y']\n",
    "            X_pred_missing_scaled = scaler_X.transform(X_pred_missing)\n",
    "            \n",
    "            # Get model\n",
    "            model = self.models[col]\n",
    "            model.eval()\n",
    "            \n",
    "            # Process in batches to prevent GPU memory issues\n",
    "            all_predictions = []\n",
    "            \n",
    "            # Manual batching for prediction\n",
    "            for i in range(0, total_missing, inference_batch_size):\n",
    "                end_idx = min(i + inference_batch_size, total_missing)\n",
    "                batch_size = end_idx - i\n",
    "                \n",
    "                # Extract batch data\n",
    "                batch_data = X_pred_missing_scaled[i:end_idx]\n",
    "                \n",
    "                # Convert to tensor\n",
    "                batch_tensor = torch.FloatTensor(batch_data).to(self.device)\n",
    "                \n",
    "                # Make sure batch tensor has right shape\n",
    "                print(f\"Batch {i//inference_batch_size + 1}: Tensor shape {batch_tensor.shape}\")\n",
    "                \n",
    "                # Process batch\n",
    "                with torch.no_grad():\n",
    "                    if self.use_amp:\n",
    "                        with autocast():\n",
    "                            batch_predictions = model(batch_tensor).cpu().numpy()\n",
    "                    else:\n",
    "                        batch_predictions = model(batch_tensor).cpu().numpy()\n",
    "                \n",
    "                print(f\"Batch {i//inference_batch_size + 1}: Generated {len(batch_predictions)} predictions\")\n",
    "                \n",
    "                # Add to collected predictions\n",
    "                all_predictions.append(batch_predictions)\n",
    "            \n",
    "            # Combine all batch predictions\n",
    "            if all_predictions:\n",
    "                y_pred_scaled = np.concatenate(all_predictions)\n",
    "                print(f\"Total: Generated {len(y_pred_scaled)} predictions for {total_missing} missing values\")\n",
    "                \n",
    "                # Inverse transform to original scale\n",
    "                y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "                \n",
    "                # Make sure lengths match\n",
    "                if len(y_pred) != total_missing:\n",
    "                    raise ValueError(f\"Prediction length mismatch: {len(y_pred)} vs {total_missing}\")\n",
    "                \n",
    "                # Fill missing values\n",
    "                imputed_df.loc[missing_indices, col] = y_pred\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.set_postfix({\"imputed\": total_missing})\n",
    "            \n",
    "            # Free up GPU memory\n",
    "            if self.device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return imputed_df\n",
    "    \n",
    "    def fit_transform(self, X, columns_to_impute=None):\n",
    "        \"\"\"\n",
    "        Fit models and impute missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "        columns_to_impute : list, optional\n",
    "            List of column names to impute. If None, all columns with missing values.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_df : pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        self.fit(X, columns_to_impute)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "def apply_transformer_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None):\n",
    "    \"\"\"\n",
    "    Apply transformer imputation to patient data\n",
    "    with CUDA optimizations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Patient data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation dataset with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation data\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extract relevant columns including potential predictors\n",
    "    # Use all columns except those with excessive missing values\n",
    "    threshold = 0.5  # Columns with more than 50% missing values are excluded\n",
    "    columns_to_use = [col for col in df.columns \n",
    "                      if df[col].isna().mean() < threshold]\n",
    "    \n",
    "    # Ensure all columns_to_impute are included\n",
    "    for col in columns_to_impute:\n",
    "        if col not in columns_to_use:\n",
    "            columns_to_use.append(col)\n",
    "    \n",
    "    # Extract subset of data\n",
    "    X = df[columns_to_use].copy()\n",
    "    \n",
    "    # Ensure all columns are numeric\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    if gpu_available:\n",
    "        print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        # Set GPU memory settings for better performance\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        print(\"CUDA not available. Using CPU.\")\n",
    "    \n",
    "    # Initialize transformer imputer with GPU optimizations\n",
    "    imputer = TransformerImputer(\n",
    "        d_model=64,\n",
    "        nhead=4,\n",
    "        num_encoder_layers=2,\n",
    "        dim_feedforward=128,\n",
    "        dropout=0.1,\n",
    "        batch_size=128 if gpu_available else 32,  # Larger batches for GPU\n",
    "        epochs=100,\n",
    "        lr=0.001,\n",
    "        device='cuda' if gpu_available else 'cpu',\n",
    "        verbose=True,\n",
    "        num_workers=4 if gpu_available else 0,  # Use workers for GPU\n",
    "        use_amp=gpu_available,  # Use mixed precision on GPU\n",
    "        pin_memory=gpu_available  # Pin memory for GPU\n",
    "    )\n",
    "    \n",
    "    # Fit and transform\n",
    "    print(\"Training transformer imputation models with CUDA acceleration...\")\n",
    "    X_imputed = imputer.fit_transform(X, columns_to_impute)\n",
    "    \n",
    "    # Create imputed dataframe\n",
    "    imputed_df = df.copy()\n",
    "    imputed_df[columns_to_impute] = X_imputed[columns_to_impute]\n",
    "    \n",
    "    # Calculate total time\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Total imputation time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Validate if validation data provided\n",
    "    validation_results = None\n",
    "    if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "        validation_results = {}\n",
    "        \n",
    "        # Extract validation data\n",
    "        X_val = validation_df[columns_to_use].copy()\n",
    "        \n",
    "        # Ensure all validation columns are numeric\n",
    "        for col in X_val.columns:\n",
    "            X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "            \n",
    "        # Impute validation data\n",
    "        X_val_imputed = imputer.transform(X_val)\n",
    "        \n",
    "        print(\"\\n===== VALIDATION RESULTS =====\")\n",
    "        # Compare imputed values to real values\n",
    "        all_mae = []\n",
    "        all_rmse = []\n",
    "        \n",
    "        with tqdm(columns_to_impute, desc=\"Validating results\") as pbar:\n",
    "            for col in pbar:\n",
    "                pbar.set_description(f\"Validating {col}\")\n",
    "                # Get indices where values were artificially set to NaN\n",
    "                mask = validation_masks[col] & validation_df[col].isna()\n",
    "                \n",
    "                if mask.sum() == 0:\n",
    "                    validation_results[col] = {\n",
    "                        'error': \"No artificially missing values\"\n",
    "                    }\n",
    "                    print(f\"{col}: No artificially missing values to validate\")\n",
    "                    continue\n",
    "                    \n",
    "                real_vals = original_values[col][mask]\n",
    "                imputed_vals = X_val_imputed[col][mask]\n",
    "                \n",
    "                # Calculate MAE and RMSE\n",
    "                mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "                rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "                all_mae.append(mae)\n",
    "                all_rmse.append(rmse)\n",
    "                \n",
    "                validation_results[col] = {\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'real_distribution': real_vals.describe(),\n",
    "                    'imputed_distribution': imputed_vals.describe()\n",
    "                }\n",
    "                \n",
    "                # Print metrics\n",
    "                print(f\"{col}: MAE = {mae:.4f}, RMSE = {rmse:.4f}\")\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.set_postfix({\"MAE\": f\"{mae:.4f}\", \"RMSE\": f\"{rmse:.4f}\"})\n",
    "        \n",
    "        # Print average metrics\n",
    "        if all_mae and all_rmse:\n",
    "            print(f\"\\nAverage MAE: {np.mean(all_mae):.4f}\")\n",
    "            print(f\"Average RMSE: {np.mean(all_rmse):.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo validation data provided - skipping validation\")\n",
    "    \n",
    "    return imputed_df, validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Using CPU.\n",
      "Training transformer imputation models with CUDA acceleration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model for ge1:   0%|          | 0/6 [00:03<?, ?it/s]     \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load your data - replace this with your actual data loading code\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Apply transformer imputation\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     imputed_df_transformer, validation_results_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mapply_cudatransformer_imputation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Save results if needed\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# imputed_df_transformer.to_csv('imputed_data.csv', index=False)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImputation complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 525\u001b[0m, in \u001b[0;36mapply_cudatransformer_imputation\u001b[0;34m(df, columns_to_impute, validation_df, validation_masks, original_values)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# Fit and transform\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining transformer imputation models with CUDA acceleration...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 525\u001b[0m X_imputed \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# Create imputed dataframe\u001b[39;00m\n\u001b[1;32m    528\u001b[0m imputed_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[0;32mIn[13], line 447\u001b[0m, in \u001b[0;36mTransformerImputer.fit_transform\u001b[0;34m(self, X, columns_to_impute)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, columns_to_impute\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    Fit models and impute missing values\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m        Data with imputed values\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Cell \u001b[0;32mIn[13], line 277\u001b[0m, in \u001b[0;36mTransformerImputer.fit\u001b[0;34m(self, X, columns_to_impute)\u001b[0m\n\u001b[1;32m    274\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y_device)\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 142\u001b[0m, in \u001b[0;36mTransformerImputer.TransformerModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    139\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m pos_emb\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Pass through transformer encoder\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Global mean pooling\u001b[39;00m\n\u001b[1;32m    145\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:517\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    514\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 517\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    525\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:918\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    916\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 918\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    922\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/nn/functional.py:2910\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2902\u001b[0m         layer_norm,\n\u001b[1;32m   2903\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2908\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2909\u001b[0m     )\n\u001b[0;32m-> 2910\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2911\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2912\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # This code runs if the script is executed directly\n",
    "    # Define columns to impute - you'll need to adjust this for your specific dataset\n",
    "    columns_to_impute = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "    \n",
    "    # Load your data - replace this with your actual data loading code\n",
    "    try:\n",
    "        # Apply transformer imputation\n",
    "        imputed_df_transformer, validation_results_transformer = apply_transformer_imputation(df, columns_to_impute)\n",
    "        \n",
    "        # Save results if needed\n",
    "        # imputed_df_transformer.to_csv('imputed_data.csv', index=False)\n",
    "        print(\"Imputation complete!\")\n",
    "        \n",
    "    except NameError:\n",
    "        print(\"Please define your dataframe 'df' before running this script\")\n",
    "        print(\"Example: df = pd.read_csv('your_data.csv')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Bayesian PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "\n",
    "class BayesianPCATorch(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of Bayesian PCA model\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_components):\n",
    "        super(BayesianPCATorch, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_components = n_components\n",
    "        \n",
    "        # Priors for loadings (W)\n",
    "        self.w_mu = nn.Parameter(torch.zeros(n_features, n_components), requires_grad=True)\n",
    "        self.w_log_sigma = nn.Parameter(torch.zeros(n_features, n_components), requires_grad=True)\n",
    "        \n",
    "        # Noise precision (inverse variance)\n",
    "        self.log_tau = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # Sample from approximate posterior for W\n",
    "        w_sigma = torch.exp(self.w_log_sigma)\n",
    "        epsilon_w = torch.randn_like(self.w_mu)\n",
    "        w = self.w_mu + w_sigma * epsilon_w\n",
    "        \n",
    "        # Compute reconstruction\n",
    "        tau = torch.exp(self.log_tau)\n",
    "        reconstruction = torch.matmul(z, w.t())\n",
    "        \n",
    "        return reconstruction, w, tau\n",
    "    \n",
    "    def sample_loadings(self, n_samples=1):\n",
    "        \"\"\"Sample loadings (W) from the approximate posterior\"\"\"\n",
    "        w_sigma = torch.exp(self.w_log_sigma)\n",
    "        epsilon_w = torch.randn(n_samples, self.n_features, self.n_components, device=self.w_mu.device)\n",
    "        w_samples = self.w_mu.unsqueeze(0) + w_sigma.unsqueeze(0) * epsilon_w\n",
    "        return w_samples\n",
    "\n",
    "\n",
    "class BayesianPCAImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A scikit-learn compatible Bayesian PCA imputation model\n",
    "    using PyTorch for GPU acceleration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_components=5,\n",
    "                 n_samples=1000,\n",
    "                 batch_size=64,\n",
    "                 n_epochs=100,\n",
    "                 learning_rate=0.01,\n",
    "                 device=None,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize PyTorch-based Bayesian PCA imputer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_components : int\n",
    "            Number of principal components\n",
    "        n_samples : int\n",
    "            Number of posterior samples to draw\n",
    "        batch_size : int\n",
    "            Batch size for training\n",
    "        n_epochs : int\n",
    "            Number of training epochs\n",
    "        learning_rate : float\n",
    "            Learning rate for optimizer\n",
    "        device : str or torch.device\n",
    "            Device to use ('cuda' or 'cpu'), defaults to CUDA if available\n",
    "        verbose : bool\n",
    "            Whether to print progress\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.n_samples = n_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Set device\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            \n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Initialize model components\n",
    "        self.pca_model = None  # For standard PCA initialization\n",
    "        self.scaler = None     # For data scaling\n",
    "        self.model = None      # The PyTorch model\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit Bayesian PCA model using PyTorch\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        # Store column names and index\n",
    "        self.columns = X.columns\n",
    "        self.index = X.index\n",
    "        \n",
    "        # Create a copy of data\n",
    "        X_data = X.copy()\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        X_array = X_data.values\n",
    "        \n",
    "        # Initialize scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Simple imputation for initial values (for fitting the scaler)\n",
    "        simple_imputer = SimpleImputer(strategy='mean')\n",
    "        X_imputed_for_scaling = simple_imputer.fit_transform(X_array)\n",
    "        \n",
    "        # Scale data\n",
    "        X_scaled = self.scaler.fit_transform(X_imputed_for_scaling)\n",
    "        \n",
    "        # Create a mask for missing values\n",
    "        self.missing_mask = np.isnan(X_array)\n",
    "        \n",
    "        # Perform standard PCA for initialization\n",
    "        pca = PCA(n_components=self.n_components)\n",
    "        pca.fit(X_scaled)\n",
    "        self.pca_model = pca\n",
    "        \n",
    "        # Initialize PyTorch model\n",
    "        n_samples, n_features = X_scaled.shape\n",
    "        self.model = BayesianPCATorch(n_features, self.n_components).to(self.device)\n",
    "        \n",
    "        # Initialize model parameters using standard PCA\n",
    "        with torch.no_grad():\n",
    "            # Initialize W to PCA loadings\n",
    "            self.model.w_mu.data = torch.tensor(pca.components_.T, dtype=torch.float32).to(self.device)\n",
    "            \n",
    "            # Initialize noise precision (tau) based on explained variance\n",
    "            explained_var = pca.explained_variance_\n",
    "            noise_var = np.mean(np.var(X_scaled, axis=0) - np.sum(explained_var))\n",
    "            noise_var = max(noise_var, 1e-6)  # Ensure positive variance\n",
    "            self.model.log_tau.data = torch.tensor([np.log(1.0 / noise_var)], dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Prepare data for PyTorch training\n",
    "        # Fill missing values with zeros (will be handled by the mask)\n",
    "        X_for_torch = X_scaled.copy()\n",
    "        X_for_torch[self.missing_mask] = 0.0\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_tensor = torch.tensor(X_for_torch, dtype=torch.float32).to(self.device)\n",
    "        mask_tensor = torch.tensor(~self.missing_mask, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Initialize latent variables (z) using PCA scores\n",
    "        z_init = pca.transform(X_scaled)\n",
    "        \n",
    "        # Store the latent variables as a model parameter\n",
    "        self.latent_z = nn.Parameter(torch.tensor(z_init, dtype=torch.float32, device=self.device))\n",
    "        \n",
    "        # Train the model\n",
    "        self._train_model(X_tensor, mask_tensor)\n",
    "        \n",
    "        # Store final latent variables for later use\n",
    "        self.z = self.latent_z.detach().cpu().numpy()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _train_model(self, X, mask):\n",
    "        \"\"\"\n",
    "        Train the Bayesian PCA model using stochastic variational inference\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : torch.Tensor\n",
    "            Data tensor\n",
    "        mask : torch.Tensor\n",
    "            Mask tensor (1 for observed, 0 for missing)\n",
    "        \"\"\"\n",
    "        # Setup optimizer - include latent_z as a parameter of self\n",
    "        parameters = list(self.model.parameters()) + [self.latent_z]\n",
    "        optimizer = optim.Adam(parameters, lr=self.learning_rate)\n",
    "        \n",
    "        # Setup scheduler for learning rate decay\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        if self.verbose:\n",
    "            print(f\"Training Bayesian PCA model on {self.device}...\")\n",
    "            print(f\"Data shape: {X.shape}, Components: {self.n_components}\")\n",
    "            print(f\"Missing values: {torch.sum(mask == 0).item()} out of {X.numel()}\")\n",
    "            \n",
    "        epoch_pbar = range(self.n_epochs)\n",
    "        if self.verbose:\n",
    "            epoch_pbar = tqdm(epoch_pbar, desc=\"Training\")\n",
    "            \n",
    "        # For early stopping\n",
    "        best_loss = float('inf')\n",
    "        patience = 40  # Increased patience to 40\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Store z for later use\n",
    "        for epoch in epoch_pbar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            x_recon, w_samples, tau = self.model(self.latent_z)\n",
    "            \n",
    "            # Compute loss - only for observed values\n",
    "            # Likelihood term (reconstruction error)\n",
    "            mse_loss = torch.sum(mask * (X - x_recon) ** 2)\n",
    "            \n",
    "            # Prior on z (standard normal)\n",
    "            z_prior_loss = 0.5 * torch.sum(self.latent_z ** 2)\n",
    "            \n",
    "            # Prior on W (standard normal)\n",
    "            w_mu = self.model.w_mu\n",
    "            w_sigma = torch.exp(self.model.w_log_sigma)\n",
    "            w_prior_loss = 0.5 * torch.sum(w_mu ** 2 + w_sigma ** 2 - torch.log(w_sigma ** 2) - 1)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = mse_loss * tau + z_prior_loss + w_prior_loss\n",
    "            \n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Get current loss\n",
    "            current_loss = loss.item()\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step(current_loss)\n",
    "            \n",
    "            # Update progress bar with current loss\n",
    "            if self.verbose:\n",
    "                epoch_pbar.set_postfix({\"Loss\": f\"{current_loss:.4f}\"})\n",
    "            \n",
    "            # Early stopping\n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                if self.verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "                \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Impute missing values using Bayesian PCA\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_df : pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        # Create copy to avoid modifying original\n",
    "        imputed_df = X.copy()\n",
    "        \n",
    "        # Check if this is the same data used for fitting\n",
    "        new_data = not np.array_equal(X.index, self.index) or not np.array_equal(X.columns, self.columns)\n",
    "        \n",
    "        if new_data:\n",
    "            # For new data, we need to project onto the learned components\n",
    "            # This is a simplified approach and could be improved\n",
    "            \n",
    "            # Create a copy of data\n",
    "            X_data = X.copy()\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            X_array = X_data.values\n",
    "            \n",
    "            # Simple imputation for initial values\n",
    "            simple_imputer = SimpleImputer(strategy='mean')\n",
    "            X_imputed = simple_imputer.fit_transform(X_array)\n",
    "            \n",
    "            # Scale data\n",
    "            X_scaled = self.scaler.transform(X_imputed)\n",
    "            \n",
    "            # Create a mask for missing values\n",
    "            missing_mask = np.isnan(X_array)\n",
    "            \n",
    "            # Project data onto principal components\n",
    "            z = self.pca_model.transform(X_scaled)\n",
    "            \n",
    "            # Get mean of W from model\n",
    "            w_samples = self.model.sample_loadings(n_samples=self.n_samples)\n",
    "            w_mean = w_samples.mean(dim=0).cpu().detach().numpy()\n",
    "            \n",
    "            # Reconstruct data\n",
    "            X_reconstructed = np.dot(z, w_mean.T)\n",
    "            \n",
    "            # Inverse transform to original scale\n",
    "            X_reconstructed = self.scaler.inverse_transform(X_reconstructed)\n",
    "            \n",
    "            # Only replace missing values\n",
    "            X_array_imputed = X_array.copy()\n",
    "            X_array_imputed[missing_mask] = X_reconstructed[missing_mask]\n",
    "            \n",
    "            # Convert back to DataFrame\n",
    "            imputed_df = pd.DataFrame(X_array_imputed, \n",
    "                                      columns=X.columns, \n",
    "                                      index=X.index)\n",
    "        else:\n",
    "            # For the same data used in fitting, use posterior samples\n",
    "            \n",
    "            # Sample from the model multiple times to get uncertainty estimates\n",
    "            w_samples = self.model.sample_loadings(n_samples=self.n_samples)  # [n_samples, n_features, n_components]\n",
    "            w_mean = w_samples.mean(dim=0).cpu().detach().numpy()  # [n_features, n_components]\n",
    "            \n",
    "            # Reconstruct data from latent variables (z)\n",
    "            X_reconstructed = np.dot(self.z, w_mean.T)  # [n_samples, n_features]\n",
    "            \n",
    "            # Inverse transform to original scale\n",
    "            X_reconstructed = self.scaler.inverse_transform(X_reconstructed)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            X_reconstructed_df = pd.DataFrame(X_reconstructed, \n",
    "                                             columns=self.columns, \n",
    "                                             index=self.index)\n",
    "            \n",
    "            # Only replace missing values\n",
    "            for col in imputed_df.columns:\n",
    "                missing_idx = imputed_df[col].isna()\n",
    "                if missing_idx.any():\n",
    "                    imputed_df.loc[missing_idx, col] = X_reconstructed_df.loc[missing_idx, col].values\n",
    "        \n",
    "        return imputed_df\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit and transform\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_df : pandas.DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "def apply_bpca_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None, display_progress=True, use_gpu=True):\n",
    "    \"\"\"\n",
    "    Apply Bayesian PCA imputation to patient data using PyTorch\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Patient data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation dataset with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation data\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "    display_progress : bool\n",
    "        Whether to display progress\n",
    "    use_gpu : bool\n",
    "        Whether to use GPU acceleration\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "    # Extract relevant columns including potential predictors\n",
    "    # Use all columns except those with excessive missing values\n",
    "    threshold = 0.5  # Columns with more than 50% missing values are excluded\n",
    "    columns_to_use = [col for col in df.columns \n",
    "                      if df[col].isna().mean() < threshold]\n",
    "    \n",
    "    # Ensure all columns_to_impute are included\n",
    "    for col in columns_to_impute:\n",
    "        if col not in columns_to_use:\n",
    "            columns_to_use.append(col)\n",
    "    \n",
    "    # Extract subset of data\n",
    "    X = df[columns_to_use].copy()\n",
    "    \n",
    "    # Ensure all columns are numeric\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    \n",
    "    # Initialize Bayesian PCA imputer\n",
    "    n_components = min(5, len(columns_to_use) - 1)  # Ensure n_components is valid\n",
    "    \n",
    "    # Set device based on user preference and availability\n",
    "    device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Determine batch size based on data size\n",
    "    batch_size = min(64, len(X))  # Default 64, but smaller if dataset is tiny\n",
    "    \n",
    "    # Configure epochs based on data size\n",
    "    n_epochs = max(100, min(300, 10000 // len(X) + 30))  # Updated minimum to 100 epochs\n",
    "    \n",
    "    if display_progress:\n",
    "        print(f\"Starting PyTorch Bayesian PCA imputation with {n_components} components\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        print(f\"Training for up to {n_epochs} epochs with batch size {batch_size}\")\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            print(f\"GPU Info: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        \n",
    "    # Determine reasonable number of posterior samples based on data size\n",
    "    n_samples = 1000  # Default\n",
    "    \n",
    "    imputer = BayesianPCAImputer(\n",
    "        n_components=n_components,\n",
    "        n_samples=n_samples,\n",
    "        batch_size=batch_size,\n",
    "        n_epochs=n_epochs,\n",
    "        learning_rate=0.01,\n",
    "        device=device,\n",
    "        verbose=display_progress\n",
    "    )\n",
    "    \n",
    "    # Fit and transform with progress tracking\n",
    "    start_time = None\n",
    "    if display_progress:\n",
    "        start_time = time.time()\n",
    "        print(\"Training PyTorch Bayesian PCA imputation model...\")\n",
    "    \n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    \n",
    "    if display_progress and start_time:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"BPCA imputation completed in {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")\n",
    "    \n",
    "    # Create imputed dataframe\n",
    "    imputed_df = df.copy()\n",
    "    imputed_df[columns_to_impute] = X_imputed[columns_to_impute]\n",
    "    \n",
    "    # Validate if validation data provided\n",
    "    validation_results = None\n",
    "    if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "        validation_results = {}\n",
    "        validation_start_time = None\n",
    "        \n",
    "        if display_progress:\n",
    "            validation_start_time = time.time()\n",
    "            print(\"\\nStarting validation on test data...\")\n",
    "        \n",
    "        # Extract validation data\n",
    "        X_val = validation_df[columns_to_use].copy()\n",
    "        \n",
    "        # Ensure all validation columns are numeric\n",
    "        for col in X_val.columns:\n",
    "            X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "            \n",
    "        # Impute validation data\n",
    "        X_val_imputed = imputer.transform(X_val)\n",
    "        \n",
    "        # For storing overall metrics\n",
    "        all_real_vals = []\n",
    "        all_imputed_vals = []\n",
    "        column_metrics = []\n",
    "        \n",
    "        # Compare imputed values to real values\n",
    "        with tqdm(columns_to_impute, desc=\"Validating results\") as pbar:\n",
    "            for col in pbar:\n",
    "                pbar.set_description(f\"Validating {col}\")\n",
    "                # Get indices where values were artificially set to NaN\n",
    "                mask = validation_masks[col] & validation_df[col].isna()\n",
    "                \n",
    "                if mask.sum() == 0:\n",
    "                    validation_results[col] = {\n",
    "                        'error': \"No artificially missing values\"\n",
    "                    }\n",
    "                    continue\n",
    "                    \n",
    "                real_vals = original_values[col][mask]\n",
    "                imputed_vals = X_val_imputed[col][mask]\n",
    "                \n",
    "                # Collect all values for overall metrics\n",
    "                all_real_vals.extend(real_vals.values)\n",
    "                all_imputed_vals.extend(imputed_vals.values)\n",
    "                \n",
    "                # Calculate MAE and RMSE\n",
    "                mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "                rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "                \n",
    "                # Store metrics for summary\n",
    "                column_metrics.append({\n",
    "                    'column': col,\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'count': len(real_vals)\n",
    "                })\n",
    "                \n",
    "                validation_results[col] = {\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'real_distribution': real_vals.describe(),\n",
    "                    'imputed_distribution': imputed_vals.describe()\n",
    "                }\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.set_postfix({\"MAE\": f\"{mae:.4f}\", \"RMSE\": f\"{rmse:.4f}\"})\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        if all_real_vals:\n",
    "            overall_mae = mean_absolute_error(all_real_vals, all_imputed_vals)\n",
    "            overall_rmse = np.sqrt(mean_squared_error(all_real_vals, all_imputed_vals))\n",
    "            \n",
    "            validation_results['overall'] = {\n",
    "                'mae': overall_mae,\n",
    "                'rmse': overall_rmse,\n",
    "                'total_values': len(all_real_vals)\n",
    "            }\n",
    "        \n",
    "        # Print detailed summary\n",
    "        if display_progress:\n",
    "            validation_time = time.time() - validation_start_time if validation_start_time else 0\n",
    "            total_time = validation_time + (time.time() - start_time if start_time else 0)\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"PYTORCH BAYESIAN PCA IMPUTATION SUMMARY (Device: {device})\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # Time information\n",
    "            print(f\"\\nTIMING INFORMATION:\")\n",
    "            print(f\"  Training Time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")\n",
    "            print(f\"  Validation Time: {validation_time:.2f} seconds ({validation_time/60:.2f} minutes)\")\n",
    "            print(f\"  Total Time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "            \n",
    "            # Overall metrics\n",
    "            if 'overall' in validation_results:\n",
    "                print(f\"\\nOVERALL METRICS (across {validation_results['overall']['total_values']} values):\")\n",
    "                print(f\"  MAE: {validation_results['overall']['mae']:.4f}\")\n",
    "                print(f\"  RMSE: {validation_results['overall']['rmse']:.4f}\")\n",
    "            \n",
    "            # Per-column metrics\n",
    "            print(\"\\nPER-COLUMN METRICS:\")\n",
    "            print(\"-\"*80)\n",
    "            print(f\"{'Column':<20} {'MAE':<10} {'RMSE':<10} {'Count':<10}\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            for metric in sorted(column_metrics, key=lambda x: x['mae']):\n",
    "                print(f\"{metric['column']:<20} {metric['mae']:<10.4f} {metric['rmse']:<10.4f} {metric['count']:<10}\")\n",
    "            \n",
    "            print(\"=\"*80)\n",
    "    \n",
    "    return imputed_df, validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PyTorch Bayesian PCA imputation with 5 components\n",
      "Using device: cpu\n",
      "Training for up to 100 epochs with batch size 64\n",
      "Training PyTorch Bayesian PCA imputation model...\n",
      "Training Bayesian PCA model on cpu...\n",
      "Data shape: torch.Size([18187, 14]), Components: 5\n",
      "Missing values: 194691 out of 254618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 67/100 [02:43<01:20,  2.44s/it, Loss=552953118720.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 68\n",
      "BPCA imputation completed in 164.47 seconds (2.74 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Define columns to impute\n",
    "columns_to_impute = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "\n",
    "# Apply Transformer imputation\n",
    "imputed_df_bpca, validation_results_bpca = apply_bpca_imputation(df, columns_to_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.104062\n",
       "1        1.101906\n",
       "2        1.101906\n",
       "3        1.101906\n",
       "4        1.101906\n",
       "           ...   \n",
       "18182    1.072600\n",
       "18183    1.072600\n",
       "18184    2.000000\n",
       "18185    1.075125\n",
       "18186    1.071828\n",
       "Name: ge1, Length: 18187, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_df_bpca['ge1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - MICE with Bayesian priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import arviz as az\n",
    "\n",
    "\n",
    "class BayesianMICEImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Multiple Imputation by Chained Equations with Bayesian Priors\n",
    "    Using PyMC instead of PyMC3/Theano for better compatibility\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_burn=500,\n",
    "                 n_iter=10,\n",
    "                 n_samples=5,\n",
    "                 max_value=None,\n",
    "                 min_value=None,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize Bayesian MICE imputer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_burn : int\n",
    "            Number of burn-in iterations\n",
    "        n_iter : int\n",
    "            Number of imputation iterations\n",
    "        n_samples : int\n",
    "            Number of multiple imputations\n",
    "        max_value : dict, optional\n",
    "            Dictionary with column names as keys and maximum allowed values\n",
    "        min_value : dict, optional\n",
    "            Dictionary with column names as keys and minimum allowed values\n",
    "        verbose : bool\n",
    "            Whether to print progress\n",
    "        \"\"\"\n",
    "        self.n_burn = n_burn\n",
    "        self.n_iter = n_iter\n",
    "        self.n_samples = n_samples\n",
    "        self.max_value = max_value if max_value is not None else {}\n",
    "        self.min_value = min_value if min_value is not None else {}\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.posterior_samples = {}\n",
    "        \n",
    "    def _fit_bayesian_model(self, X, y, col):\n",
    "        \"\"\"\n",
    "        Fit Bayesian linear regression model for a column\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Predictor variables\n",
    "        y : pandas.Series\n",
    "            Target variable\n",
    "        col : str\n",
    "            Column name\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        trace : arviz InferenceData\n",
    "            MCMC samples\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        with pm.Model() as model:\n",
    "            # Priors for weights (can be customized based on domain knowledge)\n",
    "            # Using weakly informative priors\n",
    "            weights = pm.Normal('weights', mu=0, sigma=1, \n",
    "                               shape=n_features)\n",
    "            \n",
    "            # Prior for intercept\n",
    "            intercept = pm.Normal('intercept', mu=y.mean(), sigma=y.std() * 2)\n",
    "            \n",
    "            # Model error - half normal to ensure positive\n",
    "            sigma = pm.HalfNormal('sigma', sigma=y.std() * 2)\n",
    "            \n",
    "            # Expected value of outcome\n",
    "            mu = intercept + pm.math.dot(X, weights)\n",
    "            \n",
    "            # Likelihood\n",
    "            likelihood = pm.Normal('y', mu=mu, sigma=sigma, observed=y)\n",
    "            \n",
    "            # Sample from posterior\n",
    "            idata = pm.sample(500, tune=200, chains=2, cores=1, \n",
    "                             progressbar=self.verbose)\n",
    "            \n",
    "        return idata\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit Bayesian MICE imputer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        # Store data info\n",
    "        self.columns = X.columns\n",
    "        self.dtypes = X.dtypes\n",
    "        \n",
    "        # Create a copy of data\n",
    "        self.X_filled = X.copy()\n",
    "        \n",
    "        # Create a mask for missing values\n",
    "        self.missing_mask = X.isna()\n",
    "        \n",
    "        # Initial imputation using simple strategy\n",
    "        simple_imputer = SimpleImputer(strategy='mean')\n",
    "        self.X_filled[:] = simple_imputer.fit_transform(X)\n",
    "        \n",
    "        # Create scalers for each column\n",
    "        for col in self.columns:\n",
    "            self.scalers[col] = StandardScaler()\n",
    "            self.X_filled[col] = self.scalers[col].fit_transform(\n",
    "                self.X_filled[col].values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Identify columns with missing values\n",
    "        cols_with_missing = [col for col in self.columns \n",
    "                            if self.missing_mask[col].any()]\n",
    "        \n",
    "        # Initialize models\n",
    "        for col in tqdm(cols_with_missing, desc=\"Fitting Bayesian models\", disable=not self.verbose):\n",
    "            # Create predictors and target\n",
    "            y = self.X_filled[col]\n",
    "            X = self.X_filled.drop(columns=[col])\n",
    "            \n",
    "            # Select rows with non-missing values for training\n",
    "            train_idx = ~self.missing_mask[col]\n",
    "            X_train = X.loc[train_idx]\n",
    "            y_train = y[train_idx]\n",
    "            \n",
    "            # Fit Bayesian model\n",
    "            idata = self._fit_bayesian_model(X_train.values, y_train.values, col)\n",
    "            \n",
    "            # Extract posterior samples - FIX HERE\n",
    "            # Use proper extraction from InferenceData object\n",
    "            posterior = idata.posterior\n",
    "            \n",
    "            self.posterior_samples[col] = {\n",
    "                'intercept': posterior['intercept'].values.flatten(),\n",
    "                'weights': posterior['weights'].values.reshape(-1, X_train.shape[1]),\n",
    "                'sigma': posterior['sigma'].values.flatten()\n",
    "            }\n",
    "        \n",
    "        # Perform burn-in iterations\n",
    "        self._perform_imputation(self.n_burn, burn_in=True)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _perform_imputation(self, iterations, burn_in=False):\n",
    "        \"\"\"\n",
    "        Perform multiple iterations of imputation\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        iterations : int\n",
    "            Number of iterations\n",
    "        burn_in : bool\n",
    "            Whether this is a burn-in phase\n",
    "        \"\"\"\n",
    "        # Identify columns with missing values\n",
    "        cols_with_missing = [col for col in self.columns \n",
    "                            if self.missing_mask[col].any()]\n",
    "        \n",
    "        # Create progress bar\n",
    "        iter_pbar = tqdm(range(iterations), \n",
    "                        desc=\"Burn-in\" if burn_in else \"Imputation iterations\",\n",
    "                        disable=not self.verbose)\n",
    "        \n",
    "        # Multiple iterations of imputation\n",
    "        for iteration in iter_pbar:\n",
    "            # Shuffle column order for imputation\n",
    "            np.random.shuffle(cols_with_missing)\n",
    "            \n",
    "            # Impute each column\n",
    "            for col in cols_with_missing:\n",
    "                # Get missing indices for this column\n",
    "                missing_idx = self.missing_mask[col]\n",
    "                \n",
    "                if not missing_idx.any():\n",
    "                    continue\n",
    "                \n",
    "                # Create predictors\n",
    "                X_miss = self.X_filled.drop(columns=[col]).loc[missing_idx]\n",
    "                \n",
    "                # Get posterior samples\n",
    "                intercept_samples = self.posterior_samples[col]['intercept']\n",
    "                weight_samples = self.posterior_samples[col]['weights']\n",
    "                sigma_samples = self.posterior_samples[col]['sigma']\n",
    "                \n",
    "                # Randomly select sample indices\n",
    "                if burn_in:\n",
    "                    # For burn-in, use last sample\n",
    "                    sample_idx = len(intercept_samples) - 1\n",
    "                else:\n",
    "                    # For regular imputation, randomly select\n",
    "                    sample_idx = np.random.randint(len(intercept_samples))\n",
    "                \n",
    "                # Get model parameters\n",
    "                intercept = intercept_samples[sample_idx]\n",
    "                weights = weight_samples[sample_idx]\n",
    "                sigma = sigma_samples[sample_idx]\n",
    "                \n",
    "                # Predict\n",
    "                y_pred = intercept + np.dot(X_miss.values, weights)\n",
    "                \n",
    "                # Add noise for probabilistic imputation\n",
    "                noise = np.random.normal(0, sigma, size=len(y_pred))\n",
    "                y_pred += noise\n",
    "                \n",
    "                # Apply min/max constraints if specified\n",
    "                if col in self.min_value:\n",
    "                    min_val = self.min_value[col]\n",
    "                    min_val_scaled = self.scalers[col].transform([[min_val]])[0][0]\n",
    "                    y_pred = np.maximum(y_pred, min_val_scaled)\n",
    "                \n",
    "                if col in self.max_value:\n",
    "                    max_val = self.max_value[col]\n",
    "                    max_val_scaled = self.scalers[col].transform([[max_val]])[0][0]\n",
    "                    y_pred = np.minimum(y_pred, max_val_scaled)\n",
    "                \n",
    "                # Update imputed values\n",
    "                self.X_filled.loc[missing_idx, col] = y_pred\n",
    "        \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Impute missing values using Bayesian MICE\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_df : pandas.DataFrame or list\n",
    "            Data with imputed values or list of imputed datasets\n",
    "        \"\"\"\n",
    "        # Check if same data used for fitting\n",
    "        if not np.array_equal(X.columns, self.columns):\n",
    "            raise ValueError(\"Transform data must have the same columns as fit data\")\n",
    "        \n",
    "        # Create a copy of the input data\n",
    "        imputed_df = X.copy()\n",
    "        \n",
    "        # Store multiple imputations if requested\n",
    "        all_imputations = []\n",
    "        \n",
    "        # Scale the data\n",
    "        X_scaled = X.copy()\n",
    "        for col in self.columns:\n",
    "            # For non-missing values, scale them\n",
    "            non_missing_mask = ~X[col].isna()\n",
    "            if non_missing_mask.any():\n",
    "                X_scaled.loc[non_missing_mask, col] = self.scalers[col].transform(\n",
    "                    X.loc[non_missing_mask, col].values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Perform initial simple imputation\n",
    "        simple_imputer = SimpleImputer(strategy='mean')\n",
    "        X_filled = pd.DataFrame(simple_imputer.fit_transform(X_scaled),\n",
    "                               columns=X_scaled.columns,\n",
    "                               index=X_scaled.index)\n",
    "        \n",
    "        # Create missing mask for this dataset\n",
    "        missing_mask = X.isna()\n",
    "        \n",
    "        # For each imputation sample\n",
    "        for m in tqdm(range(self.n_samples), desc=\"Creating multiple imputations\",\n",
    "                     disable=not self.verbose or self.n_samples <= 1):\n",
    "            # Copy the scaled data with initial imputation\n",
    "            X_current = X_filled.copy()\n",
    "            \n",
    "            # Multiple iterations of imputation\n",
    "            for iteration in tqdm(range(self.n_iter), \n",
    "                                desc=f\"Imputation {m+1}/{self.n_samples}\", \n",
    "                                disable=not self.verbose):\n",
    "                # Identify columns with missing values\n",
    "                cols_with_missing = [col for col in self.columns \n",
    "                                   if missing_mask[col].any()]\n",
    "                \n",
    "                # Shuffle column order for imputation\n",
    "                np.random.shuffle(cols_with_missing)\n",
    "                \n",
    "                # Impute each column\n",
    "                for col in cols_with_missing:\n",
    "                    # Get missing indices for this column\n",
    "                    col_missing_idx = missing_mask[col]\n",
    "                    \n",
    "                    if not col_missing_idx.any():\n",
    "                        continue\n",
    "                    \n",
    "                    # Create predictors\n",
    "                    X_miss = X_current.drop(columns=[col]).loc[col_missing_idx]\n",
    "                    \n",
    "                    # Get posterior samples\n",
    "                    intercept_samples = self.posterior_samples[col]['intercept']\n",
    "                    weight_samples = self.posterior_samples[col]['weights']\n",
    "                    sigma_samples = self.posterior_samples[col]['sigma']\n",
    "                    \n",
    "                    # Randomly select a posterior sample\n",
    "                    sample_idx = np.random.randint(len(intercept_samples))\n",
    "                    \n",
    "                    # Get model parameters\n",
    "                    intercept = intercept_samples[sample_idx]\n",
    "                    weights = weight_samples[sample_idx]\n",
    "                    sigma = sigma_samples[sample_idx]\n",
    "                    \n",
    "                    # Predict\n",
    "                    y_pred = intercept + np.dot(X_miss.values, weights)\n",
    "                    \n",
    "                    # Add noise for probabilistic imputation\n",
    "                    noise = np.random.normal(0, sigma, size=len(y_pred))\n",
    "                    y_pred += noise\n",
    "                    \n",
    "                    # Apply min/max constraints if specified\n",
    "                    if col in self.min_value:\n",
    "                        min_val = self.min_value[col]\n",
    "                        min_val_scaled = self.scalers[col].transform([[min_val]])[0][0]\n",
    "                        y_pred = np.maximum(y_pred, min_val_scaled)\n",
    "                    \n",
    "                    if col in self.max_value:\n",
    "                        max_val = self.max_value[col]\n",
    "                        max_val_scaled = self.scalers[col].transform([[max_val]])[0][0]\n",
    "                        y_pred = np.minimum(y_pred, max_val_scaled)\n",
    "                    \n",
    "                    # Update imputed values\n",
    "                    X_current.loc[col_missing_idx, col] = y_pred\n",
    "            \n",
    "            # Unscale the data\n",
    "            X_imputed = X_current.copy()\n",
    "            for col in self.columns:\n",
    "                X_imputed[col] = self.scalers[col].inverse_transform(\n",
    "                    X_current[col].values.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Store this imputation\n",
    "            all_imputations.append(X_imputed)\n",
    "            \n",
    "            # Update the final result (either the mean or the first imputation)\n",
    "            if m == 0:\n",
    "                imputed_df = X_imputed.copy()\n",
    "            else:\n",
    "                # For multiple imputations, take the mean\n",
    "                for col in self.columns:\n",
    "                    missing_idx = missing_mask[col]\n",
    "                    if missing_idx.any():\n",
    "                        imputed_values = []\n",
    "                        for imp in all_imputations:\n",
    "                            imputed_values.append(imp.loc[missing_idx, col].values)\n",
    "                        \n",
    "                        # Calculate mean across imputations\n",
    "                        mean_values = np.mean(np.array(imputed_values), axis=0)\n",
    "                        imputed_df.loc[missing_idx, col] = mean_values\n",
    "        \n",
    "        # If multiple imputations requested, return all of them\n",
    "        if self.n_samples > 1:\n",
    "            return all_imputations\n",
    "        else:\n",
    "            return imputed_df\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit and transform\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        imputed_df : pandas.DataFrame or list\n",
    "            Data with imputed values or list of imputed datasets\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        \n",
    "        # For fit_transform, we already have the data in self.X_filled\n",
    "        # Perform the final iterations\n",
    "        self._perform_imputation(self.n_iter)\n",
    "        \n",
    "        # Unscale the data\n",
    "        imputed_df = X.copy()\n",
    "        for col in self.columns:\n",
    "            missing_idx = self.missing_mask[col]\n",
    "            if missing_idx.any():\n",
    "                imputed_values = self.scalers[col].inverse_transform(\n",
    "                    self.X_filled.loc[missing_idx, col].values.reshape(-1, 1)).flatten()\n",
    "                imputed_df.loc[missing_idx, col] = imputed_values\n",
    "        \n",
    "        # If multiple imputations requested, generate them\n",
    "        if self.n_samples > 1:\n",
    "            all_imputations = [imputed_df.copy()]\n",
    "            \n",
    "            # Generate additional imputations\n",
    "            for m in tqdm(range(1, self.n_samples), \n",
    "                         desc=\"Creating additional imputations\",\n",
    "                         disable=not self.verbose):\n",
    "                # Start from initial state and impute again\n",
    "                self._perform_imputation(self.n_iter)\n",
    "                \n",
    "                # Unscale the data\n",
    "                m_imputed_df = X.copy()\n",
    "                for col in self.columns:\n",
    "                    missing_idx = self.missing_mask[col]\n",
    "                    if missing_idx.any():\n",
    "                        imputed_values = self.scalers[col].inverse_transform(\n",
    "                            self.X_filled.loc[missing_idx, col].values.reshape(-1, 1)).flatten()\n",
    "                        m_imputed_df.loc[missing_idx, col] = imputed_values\n",
    "                \n",
    "                all_imputations.append(m_imputed_df)\n",
    "            \n",
    "            return all_imputations\n",
    "        else:\n",
    "            return imputed_df\n",
    "\n",
    "\n",
    "def apply_bayesian_mice_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None):\n",
    "    \"\"\"\n",
    "    Apply Bayesian MICE imputation to patient data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Patient data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation dataset with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation data\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "    # Extract relevant columns including potential predictors\n",
    "    # Use all columns except those with excessive missing values\n",
    "    threshold = 0.5  # Columns with more than 50% missing values are excluded\n",
    "    columns_to_use = [col for col in df.columns \n",
    "                      if df[col].isna().mean() < threshold]\n",
    "    \n",
    "    # Ensure all columns_to_impute are included\n",
    "    for col in columns_to_impute:\n",
    "        if col not in columns_to_use:\n",
    "            columns_to_use.append(col)\n",
    "    \n",
    "    # Extract subset of data\n",
    "    X = df[columns_to_use].copy()\n",
    "    \n",
    "    # Ensure all columns are numeric\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    \n",
    "    # Define min/max constraints for domain knowledge if available\n",
    "    # This is optional and can be customized based on your data\n",
    "    min_values = {}\n",
    "    max_values = {}\n",
    "    \n",
    "    # For demonstration, we'll set some reasonable bounds for FACT-E scores\n",
    "    # Assuming scores are on a 0-4 scale\n",
    "    for col in columns_to_impute:\n",
    "        if col.startswith('GE') or col.startswith('ge'):\n",
    "            min_values[col] = 0\n",
    "            max_values[col] = 4\n",
    "    \n",
    "    # Initialize Bayesian MICE imputer with reduced computational load for testing\n",
    "    imputer = BayesianMICEImputer(\n",
    "        n_burn=500,    # Reduced from 500\n",
    "        n_iter=10,      # Reduced from 10\n",
    "        n_samples=5,   # Reduced from 5\n",
    "        min_value=min_values,\n",
    "        max_value=max_values,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Fit and transform\n",
    "    print(\"Training Bayesian MICE imputation model...\")\n",
    "    try:\n",
    "        X_imputed = imputer.fit_transform(X)\n",
    "        \n",
    "        # Handle multiple imputations if returned\n",
    "        if isinstance(X_imputed, list):\n",
    "            # If multiple imputations, take the mean\n",
    "            combined_imputation = df.copy()\n",
    "            for col in columns_to_impute:\n",
    "                missing_mask = df[col].isna()\n",
    "                if missing_mask.any():\n",
    "                    # Average across imputations\n",
    "                    imputed_values = np.mean([imp.loc[missing_mask, col].values \n",
    "                                              for imp in X_imputed], axis=0)\n",
    "                    combined_imputation.loc[missing_mask, col] = imputed_values\n",
    "            \n",
    "            imputed_df = combined_imputation\n",
    "        else:\n",
    "            # Single imputation\n",
    "            imputed_df = df.copy()\n",
    "            imputed_df[columns_to_impute] = X_imputed[columns_to_impute]\n",
    "        \n",
    "        # Validate if validation data provided\n",
    "        validation_results = None\n",
    "        if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "            validation_results = {}\n",
    "            \n",
    "            # Extract validation data\n",
    "            X_val = validation_df[columns_to_use].copy()\n",
    "            \n",
    "            # Ensure all validation columns are numeric\n",
    "            for col in X_val.columns:\n",
    "                X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "                \n",
    "            # Impute validation data\n",
    "            X_val_imputed = imputer.transform(X_val)\n",
    "            \n",
    "            # Handle multiple imputations if returned\n",
    "            if isinstance(X_val_imputed, list):\n",
    "                # If multiple imputations, take the mean for validation\n",
    "                X_val_combined = validation_df.copy()\n",
    "                for col in columns_to_impute:\n",
    "                    missing_mask = validation_df[col].isna()\n",
    "                    if missing_mask.any():\n",
    "                        # Average across imputations\n",
    "                        imputed_values = np.mean([imp.loc[missing_mask, col].values \n",
    "                                                 for imp in X_val_imputed], axis=0)\n",
    "                        X_val_combined.loc[missing_mask, col] = imputed_values\n",
    "                \n",
    "                X_val_imputed = X_val_combined\n",
    "            \n",
    "            # Compare imputed values to real values\n",
    "            with tqdm(columns_to_impute, desc=\"Validating results\") as pbar:\n",
    "                for col in pbar:\n",
    "                    pbar.set_description(f\"Validating {col}\")\n",
    "                    # Get indices where values were artificially set to NaN\n",
    "                    mask = validation_masks[col] & validation_df[col].isna()\n",
    "                    \n",
    "                    if mask.sum() == 0:\n",
    "                        validation_results[col] = {\n",
    "                            'error': \"No artificially missing values\"\n",
    "                        }\n",
    "                        continue\n",
    "                        \n",
    "                    real_vals = original_values[col][mask]\n",
    "                    imputed_vals = X_val_imputed[col][mask]\n",
    "                    \n",
    "                    # Calculate MAE and RMSE\n",
    "                    mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "                    rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "                    \n",
    "                    validation_results[col] = {\n",
    "                        'mae': mae,\n",
    "                        'rmse': rmse,\n",
    "                        'real_distribution': real_vals.describe(),\n",
    "                        'imputed_distribution': imputed_vals.describe()\n",
    "                    }\n",
    "                    \n",
    "                    # Update progress\n",
    "                    pbar.set_postfix({\"MAE\": f\"{mae:.4f}\", \"RMSE\": f\"{rmse:.4f}\"})\n",
    "        \n",
    "        return imputed_df, validation_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during Bayesian MICE imputation: {e}\")\n",
    "        print(\"Falling back to simpler imputation method...\")\n",
    "        \n",
    "        # Fall back to simple imputation\n",
    "        imputed_df = df.copy()\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        \n",
    "        for col in columns_to_impute:\n",
    "            missing_mask = df[col].isna()\n",
    "            if missing_mask.any():\n",
    "                X_col = df[columns_to_use].copy()\n",
    "                X_col = pd.DataFrame(imputer.fit_transform(X_col), columns=X_col.columns)\n",
    "                imputed_df.loc[missing_mask, col] = X_col.loc[missing_mask, col]\n",
    "        \n",
    "        return imputed_df, None\n",
    "    \n",
    "# Define columns to impute\n",
    "columns_to_impute = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "\n",
    "# Apply Transformer imputation\n",
    "imputed_df_bayseianmice, validation_results_bayseianmice = apply_bayesian_mice_imputation(df, columns_to_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bayesian MICE imputation model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Bayesian models:   0%|          | 0/12 [00:00<?, ?it/s]Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (2 chains in 1 job)\n",
      "NUTS: [weights, intercept, sigma]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 1 chain for 200 tune and 500 draw iterations (200 + 500 draws total) took 4 seconds.\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Fitting Bayesian models:   8%|▊         | 1/12 [00:08<01:33,  8.50s/it]Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (2 chains in 1 job)\n",
      "NUTS: [weights, intercept, sigma]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 200 tune and 500 draw iterations (400 + 1_000 draws total) took 5 seconds.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "Fitting Bayesian models:  17%|█▋        | 2/12 [00:16<01:24,  8.49s/it]Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (2 chains in 1 job)\n",
      "NUTS: [weights, intercept, sigma]\n"
     ]
    }
   ],
   "source": [
    "# Define columns to impute\n",
    "columns_to_impute = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "\n",
    "# Apply Transformer imputation\n",
    "imputed_df_bayseianmice, validation_results_bayseianmice = apply_bayesian_mice_imputation(df, columns_to_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Da Xu (interview paper) DL method"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "https://www.sciencedirect.com/science/article/pii/S1532046420302045\n",
    "https://ieeexplore.ieee.org/document/9238392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Deep Autoencoder Imputer:   0%|          | 0/100 [42:55<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DeepAutoencoderModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Autoencoder model for missing value imputation with patient embedding\n",
    "    \n",
    "    Based on the papers:\n",
    "    - \"A Deep Learning–Based Unsupervised Method to Impute Missing Values in Patient Records\"\n",
    "    - \"A deep learning–based, unsupervised method to impute missing values in electronic health records\"\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, patient_vocab_size, embedding_dim=16, hidden_dims=(64, 32, 16, 32, 64), \n",
    "                 dropout_rate=0.2, activation='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Dimension of input data\n",
    "        patient_vocab_size : int\n",
    "            Number of unique patients for embedding\n",
    "        embedding_dim : int\n",
    "            Size of patient embedding vectors\n",
    "        hidden_dims : tuple\n",
    "            Sizes of hidden layers\n",
    "        dropout_rate : float\n",
    "            Dropout rate for regularization\n",
    "        activation : str\n",
    "            Activation function ('relu' or 'elu')\n",
    "        \"\"\"\n",
    "        super(DeepAutoencoderModel, self).__init__()\n",
    "        \n",
    "        # Patient embedding layer\n",
    "        self.patient_embedding = nn.Embedding(patient_vocab_size + 1, embedding_dim)\n",
    "        \n",
    "        # Build encoder layers\n",
    "        self.encoder_layers = nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Get the number of layers for encoder (half of hidden_dims rounded up)\n",
    "        encoder_size = (len(hidden_dims) + 1) // 2\n",
    "        \n",
    "        for i in range(encoder_size):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Linear(prev_dim, hidden_dims[i]),\n",
    "                nn.BatchNorm1d(hidden_dims[i]),\n",
    "                nn.ReLU() if activation == 'relu' else nn.ELU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "            self.encoder_layers.append(layer)\n",
    "            prev_dim = hidden_dims[i]\n",
    "        \n",
    "        # Code layer dimension (bottleneck)\n",
    "        self.code_dim = hidden_dims[encoder_size - 1]\n",
    "        \n",
    "        # Build decoder layers\n",
    "        self.decoder_layers = nn.ModuleList()\n",
    "        \n",
    "        # First decoder layer takes concatenated code and patient embedding\n",
    "        prev_dim = self.code_dim + embedding_dim\n",
    "        \n",
    "        for i in range(encoder_size, len(hidden_dims)):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Linear(prev_dim, hidden_dims[i]),\n",
    "                nn.BatchNorm1d(hidden_dims[i]),\n",
    "                nn.ReLU() if activation == 'relu' else nn.ELU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "            self.decoder_layers.append(layer)\n",
    "            prev_dim = hidden_dims[i]\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(prev_dim, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode input data to latent representation\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Encoded representation\n",
    "        \"\"\"\n",
    "        h = x\n",
    "        for layer in self.encoder_layers:\n",
    "            h = layer(h)\n",
    "        return h\n",
    "    \n",
    "    def decode(self, code, patient_emb):\n",
    "        \"\"\"\n",
    "        Decode latent representation to reconstructed input\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        code : torch.Tensor\n",
    "            Encoded representation\n",
    "        patient_emb : torch.Tensor\n",
    "            Patient embedding vectors\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Reconstructed input\n",
    "        \"\"\"\n",
    "        # Concatenate code and patient embedding\n",
    "        h = torch.cat([code, patient_emb], dim=1)\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            h = layer(h)\n",
    "            \n",
    "        return self.output_layer(h)\n",
    "    \n",
    "    def forward(self, x, patient_ids):\n",
    "        \"\"\"\n",
    "        Forward pass through the model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input data\n",
    "        patient_ids : torch.Tensor\n",
    "            Patient IDs\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Reconstructed input\n",
    "        \"\"\"\n",
    "        # Get patient embeddings\n",
    "        patient_emb = self.patient_embedding(patient_ids).squeeze(1)\n",
    "        \n",
    "        # Encode input\n",
    "        code = self.encode(x)\n",
    "        \n",
    "        # Decode latent representation\n",
    "        output = self.decode(code, patient_emb)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    MSE loss that ignores missing values\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "        \n",
    "    def forward(self, pred, target, mask):\n",
    "        \"\"\"\n",
    "        Calculate MSE loss only on observed values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pred : torch.Tensor\n",
    "            Predicted values\n",
    "        target : torch.Tensor\n",
    "            Target values\n",
    "        mask : torch.Tensor\n",
    "            Binary mask (1 for observed, 0 for missing)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Masked MSE loss\n",
    "        \"\"\"\n",
    "        # Apply mask to predictions and targets\n",
    "        masked_pred = pred * mask\n",
    "        masked_target = target * mask\n",
    "        \n",
    "        # Calculate squared error\n",
    "        squared_error = (masked_pred - masked_target) ** 2\n",
    "        \n",
    "        # Sum squared error and count observed values\n",
    "        sum_squared_error = torch.sum(squared_error)\n",
    "        count = torch.sum(mask) + 1e-8  # Add small epsilon to avoid division by zero\n",
    "        \n",
    "        # Return MSE\n",
    "        return sum_squared_error / count\n",
    "\n",
    "\n",
    "class TemporalSimilarityLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss component for temporal similarity regularization\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TemporalSimilarityLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, pred, patient_batch_indices):\n",
    "        \"\"\"\n",
    "        Calculate temporal similarity loss based on batch proximity\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        pred : torch.Tensor\n",
    "            Predicted values\n",
    "        patient_batch_indices : torch.Tensor\n",
    "            Indices grouping patients in the batch\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Temporal similarity loss\n",
    "        \"\"\"\n",
    "        batch_size = pred.size(0)\n",
    "        \n",
    "        # If batch size is too small, return zero loss\n",
    "        if batch_size <= 1:\n",
    "            return torch.tensor(0.0, device=pred.device)\n",
    "        \n",
    "        # Create batch position indices\n",
    "        indices = torch.arange(batch_size, device=pred.device).float()\n",
    "        \n",
    "        # Expand dimensions for pairwise operations\n",
    "        pred_expanded_1 = pred.unsqueeze(1)  # [batch, 1, features]\n",
    "        pred_expanded_2 = pred.unsqueeze(0)  # [1, batch, features]\n",
    "        \n",
    "        indices_1 = indices.unsqueeze(1)  # [batch, 1]\n",
    "        indices_2 = indices.unsqueeze(0)  # [1, batch]\n",
    "        \n",
    "        # Calculate pairwise differences between predictions\n",
    "        pred_diff = torch.sum((pred_expanded_1 - pred_expanded_2) ** 2, dim=2)\n",
    "        \n",
    "        # Create patient similarity mask (1 where same patient, 0 otherwise)\n",
    "        patient_expanded_1 = patient_batch_indices.unsqueeze(1)\n",
    "        patient_expanded_2 = patient_batch_indices.unsqueeze(0)\n",
    "        patient_mask = (patient_expanded_1 == patient_expanded_2).float()\n",
    "        \n",
    "        # Create diagonal mask to exclude self-comparisons\n",
    "        diag_mask = 1.0 - torch.eye(batch_size, device=pred.device)\n",
    "        \n",
    "        # Combine masks\n",
    "        combined_mask = patient_mask * diag_mask\n",
    "        \n",
    "        # If no patient pairs exist, return zero loss\n",
    "        if torch.sum(combined_mask) == 0:\n",
    "            return torch.tensor(0.0, device=pred.device)\n",
    "        \n",
    "        # Calculate temporal similarity weights based on proximity in batch\n",
    "        # (Simple approach: closer indices = more similar)\n",
    "        temporal_diff = torch.abs(indices_1 - indices_2)\n",
    "        similarity_weights = 1.0 / (temporal_diff + 1.0)\n",
    "        \n",
    "        # Apply masks and weights\n",
    "        weighted_diff = pred_diff * similarity_weights * combined_mask\n",
    "        \n",
    "        # Normalize and return loss\n",
    "        return torch.sum(weighted_diff) / (torch.sum(combined_mask) + 1e-8)\n",
    "\n",
    "\n",
    "class DeepAutoencoderImputer:\n",
    "    \"\"\"\n",
    "    Deep Learning-Based Unsupervised Method for Missing Value Imputation\n",
    "    \n",
    "    Based on the papers:\n",
    "    - \"A Deep Learning–Based Unsupervised Method to Impute Missing Values in Patient Records\"\n",
    "    - \"A deep learning–based, unsupervised method to impute missing values in electronic health records\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedding_dim=16,\n",
    "                 hidden_layers=(64, 32, 16, 32, 64),\n",
    "                 activation='relu',\n",
    "                 dropout_rate=0.2,\n",
    "                 learning_rate=0.001,\n",
    "                 weight_decay=0.00001,\n",
    "                 temporal_weight=0.3,\n",
    "                 batch_size=16,\n",
    "                 epochs=100,\n",
    "                 patience=10,\n",
    "                 device=None,\n",
    "                 verbose=1):\n",
    "        \"\"\"\n",
    "        Initialize imputer\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        embedding_dim : int\n",
    "            Size of patient embedding vectors\n",
    "        hidden_layers : tuple\n",
    "            Sizes of hidden layers\n",
    "        activation : str\n",
    "            Activation function ('relu' or 'elu')\n",
    "        dropout_rate : float\n",
    "            Dropout rate for regularization\n",
    "        learning_rate : float\n",
    "            Learning rate for optimizer\n",
    "        weight_decay : float\n",
    "            L2 regularization strength\n",
    "        temporal_weight : float\n",
    "            Weight for temporal similarity regularization\n",
    "        batch_size : int\n",
    "            Batch size for training\n",
    "        epochs : int\n",
    "            Maximum number of training epochs\n",
    "        patience : int\n",
    "            Patience for early stopping\n",
    "        device : str or torch.device\n",
    "            Device to use ('cpu', 'cuda', or None for auto-detection)\n",
    "        verbose : int\n",
    "            Verbosity level (0 or 1)\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.temporal_weight = temporal_weight\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Set device\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            \n",
    "        if self.verbose:\n",
    "            print(f\"Using device: {self.device}\")\n",
    "            \n",
    "            # Print CUDA details if using GPU\n",
    "            if self.device.type == 'cuda':\n",
    "                print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "                print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "                print(f\"CUDA capability: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}\")\n",
    "        \n",
    "        # Will be initialized during fitting\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.columns = None\n",
    "        self.patient_id_col = None\n",
    "        self.time_col = None\n",
    "        self.numerical_cols = []\n",
    "        self.categorical_cols = []\n",
    "        self.ordinal_cols = []\n",
    "        self.cat_encoders = {}\n",
    "        self.columns_to_impute = None\n",
    "        self.history = {'train_loss': [], 'val_loss': []}\n",
    "        \n",
    "    def fit(self, X, patient_id_col, time_col=None, numerical_cols=None, \n",
    "            categorical_cols=None, ordinal_cols=None):\n",
    "        \"\"\"\n",
    "        Fit the imputer model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Input data with missing values\n",
    "        patient_id_col : str\n",
    "            Name of the column containing patient IDs\n",
    "        time_col : str, optional\n",
    "            Name of the column containing time information\n",
    "        numerical_cols : list, optional\n",
    "            Names of numerical columns. If None, autodetect.\n",
    "        categorical_cols : list, optional\n",
    "            Names of categorical columns. If None, autodetect.\n",
    "        ordinal_cols : list, optional\n",
    "            Names of ordinal columns. If None, autodetect.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self\n",
    "        \"\"\"\n",
    "        self.original_data = X.copy()\n",
    "        self.original_columns = X.columns.tolist()\n",
    "        self.patient_id_col = patient_id_col\n",
    "        self.time_col = time_col\n",
    "        \n",
    "        # Autodetect column types if not provided\n",
    "        if numerical_cols is None:\n",
    "            numerical_cols = X.select_dtypes(include=['float', 'int']).columns.tolist()\n",
    "            # Exclude patient_id_col\n",
    "            if patient_id_col in numerical_cols:\n",
    "                numerical_cols.remove(patient_id_col)\n",
    "        self.numerical_cols = numerical_cols\n",
    "        \n",
    "        if categorical_cols is None:\n",
    "            categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "            # Exclude patient_id_col and time_col\n",
    "            if patient_id_col in categorical_cols:\n",
    "                categorical_cols.remove(patient_id_col)\n",
    "            if time_col and time_col in categorical_cols:\n",
    "                categorical_cols.remove(time_col)\n",
    "        self.categorical_cols = categorical_cols\n",
    "        \n",
    "        if ordinal_cols is None:\n",
    "            ordinal_cols = []\n",
    "        self.ordinal_cols = ordinal_cols\n",
    "        \n",
    "        # Preprocess data\n",
    "        X_processed, missing_mask, cat_indices = self._preprocess_data(X, fit=True)\n",
    "        \n",
    "        # Extract patient IDs\n",
    "        patient_ids = X[patient_id_col].values\n",
    "        \n",
    "        # Train model\n",
    "        self._train_model(X_processed, patient_ids, missing_mask)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _preprocess_data(self, X, fit=True):\n",
    "        \"\"\"\n",
    "        Preprocess data for the imputation model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Input data\n",
    "        fit : bool\n",
    "            Whether to fit or transform\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (processed data, missing mask, categorical indices)\n",
    "        \"\"\"\n",
    "        # Make a copy of the input data\n",
    "        X_processed = X.copy()\n",
    "        \n",
    "        # Handle datetime column if present\n",
    "        if self.time_col and self.time_col in X.columns and pd.api.types.is_datetime64_any_dtype(X[self.time_col]):\n",
    "            # Extract features from datetime\n",
    "            X_processed[f'{self.time_col}_year'] = X[self.time_col].dt.year\n",
    "            X_processed[f'{self.time_col}_month'] = X[self.time_col].dt.month\n",
    "            X_processed[f'{self.time_col}_day'] = X[self.time_col].dt.day\n",
    "            \n",
    "            # Add these new columns to numerical cols\n",
    "            if fit:\n",
    "                self.numerical_cols.extend([f'{self.time_col}_year', f'{self.time_col}_month', f'{self.time_col}_day'])\n",
    "                \n",
    "            # Drop the original datetime column\n",
    "            X_processed = X_processed.drop(columns=[self.time_col])\n",
    "        \n",
    "        # Create missing value mask (1 for observed, 0 for missing)\n",
    "        missing_mask = ~X_processed.isna()\n",
    "        \n",
    "        # Scale numerical columns\n",
    "        if self.numerical_cols:\n",
    "            # Use only available numeric columns\n",
    "            numeric_cols_present = [col for col in self.numerical_cols if col in X_processed.columns]\n",
    "            \n",
    "            if fit:\n",
    "                self.num_scaler = StandardScaler()\n",
    "                if numeric_cols_present:\n",
    "                    # Temporarily fill missing values for scaling\n",
    "                    temp_imputer = SimpleImputer(strategy='mean')\n",
    "                    X_temp = pd.DataFrame(\n",
    "                        temp_imputer.fit_transform(X_processed[numeric_cols_present]),\n",
    "                        columns=numeric_cols_present,\n",
    "                        index=X_processed.index\n",
    "                    )\n",
    "                    \n",
    "                    # Fit scaler on non-missing data\n",
    "                    X_numerical_scaled = self.num_scaler.fit_transform(X_temp)\n",
    "                    \n",
    "                    # Update processed data with scaled values\n",
    "                    for i, col in enumerate(numeric_cols_present):\n",
    "                        X_processed[col] = X_numerical_scaled[:, i]\n",
    "            else:\n",
    "                if numeric_cols_present:\n",
    "                    # Temporarily fill missing values for scaling\n",
    "                    temp_imputer = SimpleImputer(strategy='mean')\n",
    "                    X_temp = pd.DataFrame(\n",
    "                        temp_imputer.fit_transform(X_processed[numeric_cols_present]),\n",
    "                        columns=numeric_cols_present,\n",
    "                        index=X_processed.index\n",
    "                    )\n",
    "                    \n",
    "                    # Transform with scaler\n",
    "                    X_numerical_scaled = self.num_scaler.transform(X_temp)\n",
    "                    \n",
    "                    # Update processed data with scaled values\n",
    "                    for i, col in enumerate(numeric_cols_present):\n",
    "                        X_processed[col] = X_numerical_scaled[:, i]\n",
    "        \n",
    "        # Encode categorical columns\n",
    "        cat_indices = {}\n",
    "        current_idx = len(self.numerical_cols)\n",
    "        \n",
    "        for col in self.categorical_cols:\n",
    "            if col in X_processed.columns:\n",
    "                # Handle categorical data safely\n",
    "                if pd.api.types.is_categorical_dtype(X_processed[col]):\n",
    "                    # Convert to string first to avoid category errors\n",
    "                    filled_col = X_processed[col].astype(str).fillna('MISSING')\n",
    "                else:\n",
    "                    # Fill NaN values with a placeholder\n",
    "                    filled_col = X_processed[col].fillna('MISSING')\n",
    "                \n",
    "                if fit:\n",
    "                    # Handle both older and newer scikit-learn versions\n",
    "                    try:\n",
    "                        # For newer scikit-learn versions\n",
    "                        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "                    except TypeError:\n",
    "                        # For older scikit-learn versions\n",
    "                        encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "                        \n",
    "                    encoded = encoder.fit_transform(filled_col.values.reshape(-1, 1))\n",
    "                    self.cat_encoders[col] = encoder\n",
    "                else:\n",
    "                    encoder = self.cat_encoders[col]\n",
    "                    encoded = encoder.transform(filled_col.values.reshape(-1, 1))\n",
    "                \n",
    "                # Add encoded columns\n",
    "                encoded_cols = [f\"{col}_{cat}\" for cat in encoder.categories_[0]]\n",
    "                cat_indices[col] = (current_idx, current_idx + len(encoded_cols))\n",
    "                current_idx += len(encoded_cols)\n",
    "                \n",
    "                for i, encoded_col in enumerate(encoded_cols):\n",
    "                    X_processed[encoded_col] = encoded[:, i]\n",
    "                \n",
    "                # Drop original categorical column\n",
    "                X_processed = X_processed.drop(columns=[col])\n",
    "                \n",
    "                # Update missing mask for one-hot encoded columns\n",
    "                for encoded_col in encoded_cols:\n",
    "                    missing_mask[encoded_col] = missing_mask[col]\n",
    "        \n",
    "        # Drop non-numeric columns and ensure all data is numeric\n",
    "        X_processed = X_processed.select_dtypes(include=[np.number])\n",
    "        missing_mask = missing_mask[X_processed.columns]\n",
    "        \n",
    "        if fit:\n",
    "            self.processed_columns = X_processed.columns.tolist()\n",
    "            \n",
    "        return X_processed, missing_mask, cat_indices\n",
    "    \n",
    "    def _train_model(self, X_processed, patient_ids, missing_mask):\n",
    "        \"\"\"\n",
    "        Train the imputation model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_processed : DataFrame\n",
    "            Preprocessed data\n",
    "        patient_ids : ndarray\n",
    "            Patient IDs for each record\n",
    "        missing_mask : DataFrame\n",
    "            Binary mask for observed values\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Convert to numpy arrays\n",
    "        X_np = X_processed.values\n",
    "        mask_np = missing_mask.values\n",
    "        \n",
    "        # Create patient ID mapping\n",
    "        unique_patient_ids = np.unique(patient_ids)\n",
    "        patient_id_to_idx = {id_: i for i, id_ in enumerate(unique_patient_ids)}\n",
    "        \n",
    "        # Convert patient IDs to numeric indices\n",
    "        patient_indices = np.array([patient_id_to_idx.get(id_, len(patient_id_to_idx)) for id_ in patient_ids])\n",
    "        \n",
    "        # Also create a mapping from record index to patient index for temporal loss\n",
    "        record_to_patient_idx = patient_indices.copy()\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_tensor = torch.FloatTensor(X_np)\n",
    "        mask_tensor = torch.FloatTensor(mask_np)\n",
    "        patient_tensor = torch.LongTensor(patient_indices)\n",
    "        patient_batch_tensor = torch.LongTensor(record_to_patient_idx)\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = TensorDataset(X_tensor, mask_tensor, patient_tensor, patient_batch_tensor)\n",
    "        \n",
    "        # Split into train and validation sets\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "            dataset, [train_size, val_size]\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=0 if self.device.type == 'cuda' else 2  # Use multiple workers on CPU\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=0 if self.device.type == 'cuda' else 2\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        input_dim = X_np.shape[1]\n",
    "        patient_vocab_size = len(unique_patient_ids)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Building model with input dimension {input_dim} and {patient_vocab_size} unique patients\")\n",
    "            \n",
    "        self.model = DeepAutoencoderModel(\n",
    "            input_dim=input_dim,\n",
    "            patient_vocab_size=patient_vocab_size,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            hidden_dims=self.hidden_layers,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            activation=self.activation\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Initialize loss functions\n",
    "        masked_loss_fn = MaskedMSELoss().to(self.device)\n",
    "        temporal_loss_fn = TemporalSimilarityLoss().to(self.device)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        optimizer = optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=self.learning_rate, \n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Initialize learning rate scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min', \n",
    "            factor=0.5, \n",
    "            patience=self.patience // 2, \n",
    "            verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        # Initialize early stopping variables\n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Training loop\n",
    "        if self.verbose:\n",
    "            print(f\"Starting training for {self.epochs} epochs\")\n",
    "            \n",
    "        # Use tqdm for progress tracking\n",
    "        pbar = tqdm(range(self.epochs), desc=\"Training\", disable=not self.verbose)\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_recon_loss = 0.0\n",
    "            train_temporal_loss = 0.0\n",
    "            batch_count = 0\n",
    "            \n",
    "            for X_batch, mask_batch, patient_batch, patient_idx_batch in train_loader:\n",
    "                # Move tensors to device\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                mask_batch = mask_batch.to(self.device)\n",
    "                patient_batch = patient_batch.to(self.device)\n",
    "                patient_idx_batch = patient_idx_batch.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                output = self.model(X_batch, patient_batch)\n",
    "                \n",
    "                # Calculate loss\n",
    "                recon_loss = masked_loss_fn(output, X_batch, mask_batch)\n",
    "                \n",
    "                if self.temporal_weight > 0:\n",
    "                    temporal_loss = temporal_loss_fn(output, patient_idx_batch)\n",
    "                    loss = recon_loss + self.temporal_weight * temporal_loss\n",
    "                else:\n",
    "                    temporal_loss = torch.tensor(0.0, device=self.device)\n",
    "                    loss = recon_loss\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Accumulate losses\n",
    "                train_loss += loss.item()\n",
    "                train_recon_loss += recon_loss.item()\n",
    "                train_temporal_loss += temporal_loss.item()\n",
    "                batch_count += 1\n",
    "            \n",
    "            # Calculate average training losses\n",
    "            avg_train_loss = train_loss / batch_count\n",
    "            avg_train_recon_loss = train_recon_loss / batch_count\n",
    "            avg_train_temporal_loss = train_temporal_loss / batch_count\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_batch_count = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_batch, mask_batch, patient_batch, patient_idx_batch in val_loader:\n",
    "                    # Move tensors to device\n",
    "                    X_batch = X_batch.to(self.device)\n",
    "                    mask_batch = mask_batch.to(self.device)\n",
    "                    patient_batch = patient_batch.to(self.device)\n",
    "                    patient_idx_batch = patient_idx_batch.to(self.device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    output = self.model(X_batch, patient_batch)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    recon_loss = masked_loss_fn(output, X_batch, mask_batch)\n",
    "                    \n",
    "                    if self.temporal_weight > 0:\n",
    "                        temporal_loss = temporal_loss_fn(output, patient_idx_batch)\n",
    "                        loss = recon_loss + self.temporal_weight * temporal_loss\n",
    "                    else:\n",
    "                        loss = recon_loss\n",
    "                    \n",
    "                    # Accumulate loss\n",
    "                    val_loss += loss.item()\n",
    "                    val_batch_count += 1\n",
    "            \n",
    "            # Calculate average validation loss\n",
    "            avg_val_loss = val_loss / val_batch_count\n",
    "            \n",
    "            # Update learning rate scheduler\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(avg_train_loss)\n",
    "            self.history['val_loss'].append(avg_val_loss)\n",
    "            \n",
    "            # Update progress bar\n",
    "            if self.verbose:\n",
    "                pbar.set_postfix({\n",
    "                    \"train_loss\": f\"{avg_train_loss:.4f}\",\n",
    "                    \"val_loss\": f\"{avg_val_loss:.4f}\",\n",
    "                    \"recon\": f\"{avg_train_recon_loss:.4f}\",\n",
    "                    \"temporal\": f\"{avg_train_temporal_loss:.4f}\"\n",
    "                })\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_state = self.model.state_dict()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= self.patience:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        # Load best model\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "            \n",
    "        # Return model to evaluation mode\n",
    "        self.model.eval()\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Impute missing values using trained model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Input data with missing values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        # Check if model is trained\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call fit() first.\")\n",
    "        \n",
    "        # Make a copy of the input data\n",
    "        X_imputed = X.copy()\n",
    "        \n",
    "        # Preprocess data\n",
    "        X_processed, missing_mask, cat_indices = self._preprocess_data(X, fit=False)\n",
    "        \n",
    "        # Get patient IDs\n",
    "        patient_ids = X[self.patient_id_col].values\n",
    "        \n",
    "        # Create patient ID mapping\n",
    "        unique_patient_ids = np.unique(self.original_data[self.patient_id_col])\n",
    "        patient_id_to_idx = {id_: i for i, id_ in enumerate(unique_patient_ids)}\n",
    "        \n",
    "        # Convert patient IDs to numeric indices (with fallback for unseen patients)\n",
    "        patient_indices = np.array([patient_id_to_idx.get(id_, len(patient_id_to_idx)) for id_ in patient_ids])\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_tensor = torch.FloatTensor(X_processed.values).to(self.device)\n",
    "        patient_tensor = torch.LongTensor(patient_indices).to(self.device)\n",
    "        \n",
    "        # Perform imputation in batches to avoid memory issues\n",
    "        batch_size = self.batch_size * 2  # Use larger batch size for inference\n",
    "        n_samples = X_tensor.shape[0]\n",
    "        imputed_data = np.zeros_like(X_tensor.cpu().numpy())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                end_idx = min(i + batch_size, n_samples)\n",
    "                batch_X = X_tensor[i:end_idx]\n",
    "                batch_patient = patient_tensor[i:end_idx]\n",
    "                \n",
    "                # Get model predictions\n",
    "                batch_output = self.model(batch_X, batch_patient)\n",
    "                \n",
    "                # Store predictions\n",
    "                imputed_data[i:end_idx] = batch_output.cpu().numpy()\n",
    "        \n",
    "        # Convert back to DataFrame\n",
    "        imputed_df = pd.DataFrame(imputed_data, columns=X_processed.columns, index=X_processed.index)\n",
    "        \n",
    "        # Handle numerical columns\n",
    "        for col in self.numerical_cols:\n",
    "            if col in X.columns and not col.startswith(f'{self.time_col}_'):\n",
    "                # Get mask for missing values in original data\n",
    "                missing_indices = X[col].isna()\n",
    "                \n",
    "                if missing_indices.any():\n",
    "                    # Only update missing values\n",
    "                    # If column is in columns_to_impute, round to nearest integer\n",
    "                    if self.columns_to_impute and col in self.columns_to_impute:\n",
    "                        X_imputed.loc[missing_indices, col] = np.round(\n",
    "                            imputed_df.loc[missing_indices, col].values\n",
    "                        )\n",
    "                    else:\n",
    "                        X_imputed.loc[missing_indices, col] = imputed_df.loc[missing_indices, col].values\n",
    "        \n",
    "        # Handle categorical columns (need to convert one-hot back to original categories)\n",
    "        for col in self.categorical_cols:\n",
    "            if col in X.columns:\n",
    "                missing_indices = X[col].isna()\n",
    "                \n",
    "                if missing_indices.any() and col in cat_indices:\n",
    "                    # Get the one-hot encoded columns for this categorical variable\n",
    "                    start_idx, end_idx = cat_indices[col]\n",
    "                    encoded_cols = self.processed_columns[start_idx:end_idx]\n",
    "                    \n",
    "                    # Get indices where values are missing\n",
    "                    missing_idx = missing_indices[missing_indices].index\n",
    "                    \n",
    "                    # Extract one-hot encoded values for the missing entries\n",
    "                    one_hot_values = imputed_df.loc[missing_idx, encoded_cols].values\n",
    "                    \n",
    "                    # Get the index of the max value along axis 1\n",
    "                    max_indices = np.argmax(one_hot_values, axis=1)\n",
    "                    \n",
    "                    # Map back to original categories\n",
    "                    categories = self.cat_encoders[col].categories_[0]\n",
    "                    imputed_categories = [categories[idx] for idx in max_indices]\n",
    "                    \n",
    "                    # Handle categorical data types safely\n",
    "                    if pd.api.types.is_categorical_dtype(X[col]):\n",
    "                        # Get the existing categories\n",
    "                        existing_cats = X[col].cat.categories\n",
    "                        \n",
    "                        # Filter out categories that aren't in the existing ones\n",
    "                        valid_categories = []\n",
    "                        for cat in imputed_categories:\n",
    "                            if cat in existing_cats:\n",
    "                                valid_categories.append(cat)\n",
    "                            else:\n",
    "                                # Use the most common category as fallback\n",
    "                                most_common = X[col].value_counts().index[0]\n",
    "                                valid_categories.append(most_common)\n",
    "                        \n",
    "                        # Update missing values\n",
    "                        for idx, cat in zip(missing_idx, valid_categories):\n",
    "                            X_imputed.loc[idx, col] = cat\n",
    "                    else:\n",
    "                        # Update missing values in the original DataFrame\n",
    "                        X_imputed.loc[missing_idx, col] = imputed_categories\n",
    "        \n",
    "        return X_imputed\n",
    "    \n",
    "    def fit_transform(self, X, patient_id_col, time_col=None, numerical_cols=None, \n",
    "                     categorical_cols=None, ordinal_cols=None):\n",
    "        \"\"\"\n",
    "        Fit the model and impute missing values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : DataFrame\n",
    "            Input data with missing values\n",
    "        patient_id_col : str\n",
    "            Name of the column containing patient IDs\n",
    "        time_col : str, optional\n",
    "            Name of the column containing time information\n",
    "        numerical_cols : list, optional\n",
    "            Names of numerical columns. If None, autodetect.\n",
    "        categorical_cols : list, optional\n",
    "            Names of categorical columns. If None, autodetect.\n",
    "        ordinal_cols : list, optional\n",
    "            Names of ordinal columns. If None, autodetect.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame\n",
    "            Data with imputed values\n",
    "        \"\"\"\n",
    "        self.fit(X, patient_id_col, time_col, numerical_cols, categorical_cols, ordinal_cols)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def plot_loss(self):\n",
    "        \"\"\"\n",
    "        Plot the training and validation loss curves\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib figure\n",
    "            Loss curve plot\n",
    "        \"\"\"\n",
    "        if not self.history['train_loss']:\n",
    "            raise ValueError(\"Model has not been trained yet. Call fit() first.\")\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.history['train_loss'], label='Training Loss')\n",
    "        plt.plot(self.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Deep Autoencoder Imputation Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        return plt\n",
    "\n",
    "\n",
    "def apply_deep_autoencoder_imputation(df, columns_to_impute, validation_df=None, validation_masks=None, original_values=None, \n",
    "                                     hidden_layers=(64, 32, 16, 32, 64), latent_dim=8, batch_size=16, epochs=100, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Apply deep autoencoder imputation to data with an interface matching your VAE implementation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Data with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    validation_df : pandas.DataFrame, optional\n",
    "        Validation dataset with artificially missing values\n",
    "    validation_masks : dict, optional\n",
    "        Dictionary of masks for validation data\n",
    "    original_values : dict, optional\n",
    "        Dictionary of original values for validation\n",
    "    hidden_layers : tuple\n",
    "        Sizes of hidden layers\n",
    "    latent_dim : int\n",
    "        Dimension of the patient embedding (equivalent to latent_dim in VAE)\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "    epochs : int\n",
    "        Number of training epochs\n",
    "    learning_rate : float\n",
    "        Learning rate for optimizer\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_df : pandas.DataFrame\n",
    "        Data with imputed values\n",
    "    validation_results : dict, optional\n",
    "        Validation results if validation data provided\n",
    "    \"\"\"\n",
    "    # Print column information\n",
    "    print(f\"Data has {len(df)} rows and {len(df.columns)} columns\")\n",
    "    print(f\"Columns to impute: {columns_to_impute}\")\n",
    "    \n",
    "    # Print missingness information\n",
    "    print(\"\\nMissingness rates in columns to impute:\")\n",
    "    for col in columns_to_impute:\n",
    "        if col in df.columns:\n",
    "            miss_rate = df[col].isna().mean() * 100\n",
    "            print(f\"{col}: {miss_rate:.2f}%\")\n",
    "        else:\n",
    "            print(f\"{col}: column not found in data\")\n",
    "    \n",
    "    # Extract relevant columns including potential predictors\n",
    "    # Use all columns except those with excessive missing values\n",
    "    threshold = 0.5  # Columns with more than 50% missing values are excluded\n",
    "    columns_to_use = [col for col in df.columns \n",
    "                     if df[col].isna().mean() < threshold]\n",
    "    \n",
    "    # Ensure all columns_to_impute are included\n",
    "    for col in columns_to_impute:\n",
    "        if col not in columns_to_use and col in df.columns:\n",
    "            columns_to_use.append(col)\n",
    "            \n",
    "    # Always include id and operation_date if available\n",
    "    id_col = 'id'\n",
    "    if id_col not in columns_to_use and id_col in df.columns:\n",
    "        columns_to_use.append(id_col)\n",
    "        \n",
    "    time_col = 'operation_date'\n",
    "    if time_col not in columns_to_use and time_col in df.columns:\n",
    "        columns_to_use.append(time_col)\n",
    "    \n",
    "    print(f\"\\nUsing {len(columns_to_use)} columns for imputation model\")\n",
    "    \n",
    "    # Extract subset of data\n",
    "    X = df[columns_to_use].copy()\n",
    "    \n",
    "    # Ensure all columns to impute are numeric\n",
    "    print(\"\\nConverting columns to numeric...\")\n",
    "    with tqdm(columns_to_impute) as pbar:\n",
    "        for col in pbar:\n",
    "            pbar.set_description(f\"Processing {col}\")\n",
    "            if col in X.columns:\n",
    "                # Convert to numeric if possible\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize deep autoencoder imputer\n",
    "    print(\"\\nInitializing deep autoencoder imputer...\")\n",
    "    imputer = DeepAutoencoderImputer(\n",
    "        embedding_dim=latent_dim,\n",
    "        hidden_layers=hidden_layers,\n",
    "        activation='elu',\n",
    "        dropout_rate=0.3,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=0.0001,\n",
    "        temporal_weight=0.4,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        patience=20,\n",
    "        device=device,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Store the columns to impute in the imputer object\n",
    "    imputer.columns_to_impute = columns_to_impute\n",
    "    \n",
    "    # Fit and transform\n",
    "    print(\"\\nTraining deep autoencoder imputation model...\")\n",
    "    X_imputed = imputer.fit_transform(\n",
    "        X, \n",
    "        patient_id_col=id_col,  # Use 'id' as the patient ID column\n",
    "        time_col=None,  # Disable temporal information to avoid shape errors\n",
    "        numerical_cols=columns_to_impute  # Only specify columns to impute as numerical\n",
    "    )\n",
    "    \n",
    "    # Create imputed dataframe\n",
    "    print(\"\\nCreating final imputed dataframe...\")\n",
    "    imputed_df = df.copy()\n",
    "    \n",
    "    # Copy imputed values to the output dataframe\n",
    "    for col in columns_to_impute:\n",
    "        if col in X_imputed.columns and col in imputed_df.columns:\n",
    "            # Get mask for missing values in the original data\n",
    "            missing_mask = df[col].isna()\n",
    "            # Only update missing values\n",
    "            imputed_df.loc[missing_mask, col] = X_imputed.loc[missing_mask, col]\n",
    "    \n",
    "    # Print post-imputation statistics\n",
    "    print(\"\\nPost-imputation statistics:\")\n",
    "    for col in columns_to_impute:\n",
    "        if col in imputed_df.columns:\n",
    "            # Count originally missing values that have been imputed\n",
    "            missing_before = df[col].isna().sum()\n",
    "            missing_after = imputed_df[col].isna().sum()\n",
    "            num_imputed = missing_before - missing_after\n",
    "            print(f\"{col}: {num_imputed} values imputed\")\n",
    "    \n",
    "    # Validate if validation data provided\n",
    "    validation_results = None\n",
    "    if validation_df is not None and validation_masks is not None and original_values is not None:\n",
    "        validation_results = {}\n",
    "        \n",
    "        print(\"\\nPerforming validation on held-out data...\")\n",
    "        \n",
    "        # Extract validation data\n",
    "        X_val = validation_df[columns_to_use].copy()\n",
    "        \n",
    "        # Ensure all validation columns are numeric\n",
    "        for col in X_val.columns:\n",
    "            if col in columns_to_impute:\n",
    "                X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
    "            \n",
    "        # Impute validation data\n",
    "        print(\"Imputing validation data...\")\n",
    "        X_val_imputed = imputer.transform(X_val)\n",
    "        \n",
    "        # Compare imputed values to real values\n",
    "        print(\"Calculating validation metrics...\")\n",
    "        with tqdm(columns_to_impute, desc=\"Validating results\") as pbar:\n",
    "            for col in pbar:\n",
    "                pbar.set_description(f\"Validating {col}\")\n",
    "                # Get indices where values were artificially set to NaN\n",
    "                mask = validation_masks[col] & validation_df[col].isna()\n",
    "                \n",
    "                if mask.sum() == 0:\n",
    "                    validation_results[col] = {\n",
    "                        'error': \"No artificially missing values\"\n",
    "                    }\n",
    "                    continue\n",
    "                    \n",
    "                real_vals = original_values[col][mask]\n",
    "                imputed_vals = X_val_imputed.loc[mask, col]\n",
    "                \n",
    "                # Calculate MAE and RMSE\n",
    "                mae = mean_absolute_error(real_vals, imputed_vals)\n",
    "                rmse = np.sqrt(mean_squared_error(real_vals, imputed_vals))\n",
    "                \n",
    "                validation_results[col] = {\n",
    "                    'mae': mae,\n",
    "                    'rmse': rmse,\n",
    "                    'real_distribution': real_vals.describe(),\n",
    "                    'imputed_distribution': pd.Series(imputed_vals).describe()\n",
    "                }\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.set_postfix({\"MAE\": f\"{mae:.4f}\", \"RMSE\": f\"{rmse:.4f}\"})\n",
    "    \n",
    "    # Plot the training loss curve\n",
    "    print(\"\\nGenerating training loss plot...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(imputer.history['train_loss'], label='Training Loss')\n",
    "    plt.plot(imputer.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Deep Autoencoder Imputation Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('deep_autoencoder_loss.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\nTraining loss plot saved to deep_autoencoder_loss.png\")\n",
    "    print(\"\\nImputation complete!\")\n",
    "    \n",
    "    return imputed_df, validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 18187 rows and 66 columns\n",
      "Columns to impute: ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
      "\n",
      "Missingness rates in columns to impute:\n",
      "ge1: 84.97%\n",
      "ge2: 85.21%\n",
      "ge3: 85.46%\n",
      "ge4: 85.07%\n",
      "ge5: 85.13%\n",
      "ge6: 85.06%\n",
      "\n",
      "Using 15 columns for imputation model\n",
      "\n",
      "Converting columns to numeric...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ge6:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ge6: 100%|██████████| 6/6 [00:00<00:00, 1464.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing deep autoencoder imputer...\n",
      "Using device: cpu\n",
      "\n",
      "Training deep autoencoder imputation model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with input dimension 69 and 1754 unique patients\n",
      "Starting training for 100 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/100 [08:57<?, ?it/s]\n",
      "Training Deep Autoencoder Imputer:   0%|          | 0/100 [46:59<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m columns_to_impute \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mge6\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Apply deep autoencoder imputation with explicit column specifications\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m imputed_df, validation_results \u001b[38;5;241m=\u001b[39m \u001b[43mapply_deep_autoencoder_imputation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns_to_impute\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 1049\u001b[0m, in \u001b[0;36mapply_deep_autoencoder_imputation\u001b[0;34m(df, columns_to_impute, validation_df, validation_masks, original_values, hidden_layers, latent_dim, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# Fit and transform\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining deep autoencoder imputation model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1049\u001b[0m X_imputed \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatient_id_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use 'id' as the patient ID column\u001b[39;49;00m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Disable temporal information to avoid shape errors\u001b[39;49;00m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumerical_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns_to_impute\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Only specify columns to impute as numerical\u001b[39;49;00m\n\u001b[1;32m   1054\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# Create imputed dataframe\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCreating final imputed dataframe...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 916\u001b[0m, in \u001b[0;36mDeepAutoencoderImputer.fit_transform\u001b[0;34m(self, X, patient_id_col, time_col, numerical_cols, categorical_cols, ordinal_cols)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, patient_id_col, time_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, numerical_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m    892\u001b[0m                  categorical_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ordinal_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    893\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;124;03m    Fit the model and impute missing values\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m        Data with imputed values\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 916\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_id_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumerical_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mordinal_cols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Cell \u001b[0;32mIn[15], line 418\u001b[0m, in \u001b[0;36mDeepAutoencoderImputer.fit\u001b[0;34m(self, X, patient_id_col, time_col, numerical_cols, categorical_cols, ordinal_cols)\u001b[0m\n\u001b[1;32m    415\u001b[0m patient_ids \u001b[38;5;241m=\u001b[39m X[patient_id_col]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "Cell \u001b[0;32mIn[15], line 689\u001b[0m, in \u001b[0;36mDeepAutoencoderImputer._train_model\u001b[0;34m(self, X_processed, patient_ids, missing_mask)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m    688\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 689\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    692\u001b[0m \u001b[38;5;66;03m# Accumulate losses\u001b[39;00m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jinenv/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "columns_to_impute = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "# Apply deep autoencoder imputation with explicit column specifications\n",
    "imputed_df, validation_results = apply_deep_autoencoder_imputation(\n",
    "    df=df, \n",
    "    columns_to_impute=columns_to_impute\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Analyzing Dataset =====\n",
      "Dataset shape: (18187, 65)\n",
      "\n",
      "Missing value counts:\n",
      "ge1: 15454 missing values (84.97%)\n",
      "ge2: 15497 missing values (85.21%)\n",
      "ge3: 15543 missing values (85.46%)\n",
      "ge4: 15472 missing values (85.07%)\n",
      "ge5: 15482 missing values (85.13%)\n",
      "ge6: 15469 missing values (85.06%)\n",
      "\n",
      "Observed values per column:\n",
      "ge1: 2733 observed values (15.03%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 1.0893, Std: 1.1332\n",
      "  Unique values: 5 (0.2% unique)\n",
      "ge2: 2690 observed values (14.79%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 2.4647, Std: 1.1927\n",
      "  Unique values: 5 (0.2% unique)\n",
      "ge3: 2644 observed values (14.54%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 0.5843, Std: 1.0138\n",
      "  Unique values: 5 (0.2% unique)\n",
      "ge4: 2715 observed values (14.93%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 1.2891, Std: 1.2363\n",
      "  Unique values: 5 (0.2% unique)\n",
      "ge5: 2705 observed values (14.87%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 0.9567, Std: 1.1935\n",
      "  Unique values: 5 (0.2% unique)\n",
      "ge6: 2718 observed values (14.94%)\n",
      "  Range: 0.0 to 4.0\n",
      "  Mean: 1.5839, Std: 1.2910\n",
      "  Unique values: 5 (0.2% unique)\n",
      "\n",
      "===== Running imputation on real data =====\n",
      "Missingness statistics before imputation:\n",
      "ge1: 15454 missing values (84.97%)\n",
      "ge2: 15497 missing values (85.21%)\n",
      "ge3: 15543 missing values (85.46%)\n",
      "ge4: 15472 missing values (85.07%)\n",
      "ge5: 15482 missing values (85.13%)\n",
      "ge6: 15469 missing values (85.06%)\n",
      "\n",
      "Running MICE imputation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MICE Imputation: 100%|██████████| 5/5 [00:42<00:00,  8.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MICE completed in 42.63 seconds\n",
      "MICE - ge1 distribution similarity: KS=0.4182, p=0.0000\n",
      "MICE - ge2 distribution similarity: KS=0.5261, p=0.0000\n",
      "MICE - ge3 distribution similarity: KS=0.6244, p=0.0000\n",
      "MICE - ge4 distribution similarity: KS=0.5207, p=0.0000\n",
      "MICE - ge5 distribution similarity: KS=0.6050, p=0.0000\n",
      "MICE - ge6 distribution similarity: KS=0.4982, p=0.0000\n",
      "\n",
      "Running Autoencoder imputation...\n",
      "Training autoencoder imputation models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model for ge5:  67%|██████▋   | 4/6 [02:25<01:14, 37.03s/it]"
     ]
    }
   ],
   "source": [
    "# Add these imports at the top of your file\n",
    "# from TransformerImputer import apply_transformer_imputation\n",
    "# from CUDATransformerImputer import apply_cudatransformer_imputation\n",
    "\n",
    "def evaluate_real_data_imputation(df, columns_to_impute):\n",
    "    \"\"\"\n",
    "    Evaluate imputation methods on real data with existing missing values\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Your real dataset with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute (ge1-ge6)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    all_imputed_dfs : dict\n",
    "        Dictionary with imputed DataFrames from each method\n",
    "    original_df_with_missing : pandas.DataFrame\n",
    "        Original dataframe with missing values\n",
    "    execution_times : dict\n",
    "        Dictionary with execution times for each method\n",
    "    \"\"\"\n",
    "    # Define the methods to compare\n",
    "    methods = {\n",
    "        'MICE': apply_mice_imputation,\n",
    "        'VAE': apply_vae_imputation,\n",
    "        'DAE': apply_sklearn_dae_imputation,\n",
    "        'Transformer': apply_transformer_imputation,  # Add the CUDA-optimized transformer method\n",
    "        'Bayesian PCA': apply_bpca_imputation,\n",
    "        'Bayesian MICE': apply_bayesian_mice_imputation,\n",
    "        'Da XU DL': apply_deep_autoencoder_imputation\n",
    "    }\n",
    "    \n",
    "    # Initialize dictionaries to store results\n",
    "    all_imputed_dfs = {}\n",
    "    execution_times = {}\n",
    "    distribution_similarity = {method: {} for method in methods}\n",
    "    \n",
    "    # Calculate missingness statistics before imputation\n",
    "    print(\"Missingness statistics before imputation:\")\n",
    "    for col in columns_to_impute:\n",
    "        missing_count = df[col].isna().sum()\n",
    "        missing_percent = (missing_count / len(df)) * 100\n",
    "        print(f\"{col}: {missing_count} missing values ({missing_percent:.2f}%)\")\n",
    "    \n",
    "    # Save a copy of the original data with missing values\n",
    "    original_df_with_missing = df.copy()\n",
    "    \n",
    "    # Run each imputation method\n",
    "    for method_name, method_func in methods.items():\n",
    "        print(f\"\\nRunning {method_name} imputation...\")\n",
    "        \n",
    "        try:\n",
    "            # Time the imputation\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Apply the imputation method\n",
    "            imputed_df, _ = method_func(\n",
    "                df.copy(), \n",
    "                columns_to_impute\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "            execution_times[method_name] = execution_time\n",
    "            \n",
    "            # Store the imputed dataframe\n",
    "            all_imputed_dfs[method_name] = imputed_df\n",
    "            \n",
    "            # Print execution time\n",
    "            print(f\"{method_name} completed in {execution_time:.2f} seconds\")\n",
    "            \n",
    "            # Check if imputation was successful (no missing values in imputed columns)\n",
    "            for col in columns_to_impute:\n",
    "                remaining_missing = imputed_df[col].isna().sum()\n",
    "                if remaining_missing > 0:\n",
    "                    print(f\"Warning: {method_name} left {remaining_missing} missing values in {col}\")\n",
    "                \n",
    "                # Measure distribution similarity between observed and imputed values\n",
    "                observed_values = original_df_with_missing.loc[original_df_with_missing[col].notna(), col]\n",
    "                imputed_values = imputed_df.loc[original_df_with_missing[col].isna(), col].dropna()\n",
    "                \n",
    "                if len(observed_values) > 0 and len(imputed_values) > 0:\n",
    "                    # Kolmogorov-Smirnov test for distribution similarity\n",
    "                    ks_stat, ks_pval = ks_2samp(observed_values, imputed_values)\n",
    "                    distribution_similarity[method_name][col] = {\n",
    "                        'ks_stat': ks_stat,\n",
    "                        'ks_pval': ks_pval\n",
    "                    }\n",
    "                    print(f\"{method_name} - {col} distribution similarity: KS={ks_stat:.4f}, p={ks_pval:.4f}\")\n",
    "                    #Lower KS statistic and higher p-values indicate better distribution preservation\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error running {method_name}: {e}\")\n",
    "    \n",
    "    return all_imputed_dfs, original_df_with_missing, execution_times, distribution_similarity\n",
    "\n",
    "\n",
    "def evaluate_with_sparse_validation(df, columns_to_impute, n_folds=3):\n",
    "    \"\"\"\n",
    "    Evaluate imputation methods using cross-validation optimized for sparse data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Your real dataset with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    n_folds : int\n",
    "        Number of validation folds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary with validation results for each method\n",
    "    \"\"\"\n",
    "    # Define the methods to compare\n",
    "    methods = {\n",
    "        'MICE': apply_mice_imputation,\n",
    "        'VAE': apply_vae_imputation,\n",
    "        'DAE': apply_sklearn_dae_imputation,\n",
    "        'Transformer': apply_transformer_imputation,  # Add the CUDA-optimized transformer method\n",
    "        'Bayesian PCA': apply_bpca_imputation,\n",
    "        'Bayesian MICE': apply_bayesian_mice_imputation,\n",
    "        'Da XU DL': apply_deep_autoencoder_imputation\n",
    "    }\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {method: {'mae': [], 'rmse': [], 'time': []} for method in methods}\n",
    "    \n",
    "    # For each column, perform validation on non-missing values\n",
    "    for col in columns_to_impute:\n",
    "        print(f\"\\n===== Sparse validation for column: {col} =====\")\n",
    "        \n",
    "        # Get indices of non-missing values for this column\n",
    "        observed_indices = df.index[df[col].notna()].tolist()\n",
    "        n_observed = len(observed_indices)\n",
    "        \n",
    "        print(f\"Column {col}: {n_observed} observed values ({n_observed/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        # Skip if too few observations\n",
    "        if n_observed < max(5, n_folds * 2):\n",
    "            print(f\"Too few observed values for validation. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Instead of standard KFold, use a more conservative approach for sparse data\n",
    "        fold_size = max(5, n_observed // n_folds)  # Ensure at least 5 samples per fold\n",
    "        \n",
    "        # Shuffle observed indices\n",
    "        np.random.seed(random.randint(0, 10000))  # For reproducibility\n",
    "        shuffled_indices = np.random.permutation(observed_indices)\n",
    "        \n",
    "        for fold in range(min(n_folds, n_observed // fold_size)):\n",
    "            # Select test indices for this fold\n",
    "            start_idx = fold * fold_size\n",
    "            end_idx = min(start_idx + fold_size, n_observed)\n",
    "            test_indices = shuffled_indices[start_idx:end_idx]\n",
    "            \n",
    "            print(f\"\\nFold {fold+1}/{min(n_folds, n_observed // fold_size)} - Testing on {len(test_indices)} samples\")\n",
    "            \n",
    "            # Create a copy of the original dataframe\n",
    "            df_fold = df.copy()\n",
    "            \n",
    "            # Store original values from test set\n",
    "            original_values = df_fold.loc[test_indices, col].copy()\n",
    "            \n",
    "            # Set test values to NaN (simulating missingness)\n",
    "            df_fold.loc[test_indices, col] = np.nan\n",
    "            \n",
    "            # For each imputation method\n",
    "            for method_name, method_func in methods.items():\n",
    "                print(f\"Running {method_name}...\")\n",
    "                \n",
    "                try:\n",
    "                    # Time the imputation\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    # Apply imputation\n",
    "                    imputed_df, _ = method_func(df_fold.copy(), columns_to_impute)\n",
    "                    \n",
    "                    end_time = time.time()\n",
    "                    execution_time = end_time - start_time\n",
    "                    \n",
    "                    # Get imputed values for the test indices\n",
    "                    imputed_values = imputed_df.loc[test_indices, col]\n",
    "                    \n",
    "                    # Check for any still-missing values\n",
    "                    still_missing = imputed_values.isna().sum()\n",
    "                    if still_missing > 0:\n",
    "                        print(f\"Warning: {method_name} failed to impute {still_missing}/{len(test_indices)} values\")\n",
    "                        # Use only successfully imputed values for evaluation\n",
    "                        valid_indices = test_indices.copy()\n",
    "                        for idx in test_indices:\n",
    "                            if pd.isna(imputed_df.loc[idx, col]):\n",
    "                                valid_indices.remove(idx)\n",
    "                        \n",
    "                        if not valid_indices:\n",
    "                            print(f\"No valid imputations to evaluate for {method_name}\")\n",
    "                            continue\n",
    "                            \n",
    "                        original_values = df.loc[valid_indices, col]\n",
    "                        imputed_values = imputed_df.loc[valid_indices, col]\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    mae = np.mean(np.abs(original_values - imputed_values))\n",
    "                    rmse = np.sqrt(np.mean((original_values - imputed_values) ** 2))\n",
    "                    \n",
    "                    # Store results\n",
    "                    results[method_name]['mae'].append(mae)\n",
    "                    results[method_name]['rmse'].append(rmse)\n",
    "                    results[method_name]['time'].append(execution_time)\n",
    "                    \n",
    "                    print(f\"{method_name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}, Time: {execution_time:.2f}s\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {method_name}: {e}\")\n",
    "    \n",
    "    # Calculate average results across all columns and folds\n",
    "    for method in methods:\n",
    "        if results[method]['mae']:\n",
    "            results[method]['avg_mae'] = np.mean(results[method]['mae'])\n",
    "            results[method]['std_mae'] = np.std(results[method]['mae'])\n",
    "            results[method]['avg_rmse'] = np.mean(results[method]['rmse'])\n",
    "            results[method]['std_rmse'] = np.std(results[method]['rmse'])\n",
    "            results[method]['avg_time'] = np.mean(results[method]['time'])\n",
    "            results[method]['std_time'] = np.std(results[method]['time'])\n",
    "        else:\n",
    "            print(f\"No valid results for {method}\")\n",
    "            results[method]['avg_mae'] = np.nan\n",
    "            results[method]['std_mae'] = np.nan\n",
    "            results[method]['avg_rmse'] = np.nan\n",
    "            results[method]['std_rmse'] = np.nan\n",
    "            results[method]['avg_time'] = np.nan\n",
    "            results[method]['std_time'] = np.nan\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_execution_times(execution_times):\n",
    "    \"\"\"\n",
    "    Plot execution times for each imputation method\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    execution_times : dict\n",
    "        Dictionary with execution times for each method\n",
    "    \"\"\"\n",
    "    methods = list(execution_times.keys())\n",
    "    times = list(execution_times.values())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(methods, times, alpha=0.7, color='green')\n",
    "    \n",
    "    plt.title('Execution Time by Imputation Method', fontsize=14)\n",
    "    plt.ylabel('Time (seconds)', fontsize=12)\n",
    "    plt.xlabel('Method', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.02 * max(times),\n",
    "                f'{height:.2f}s', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('imputation_execution_times.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution_similarity(distribution_similarity):\n",
    "    \"\"\"\n",
    "    Plot KS statistics for distribution similarity\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    distribution_similarity : dict\n",
    "        Dictionary with KS test results\n",
    "    \"\"\"\n",
    "    # Prepare data for plotting\n",
    "    methods = list(distribution_similarity.keys())\n",
    "    columns = list(distribution_similarity[methods[0]].keys()) if methods else []\n",
    "    \n",
    "    if not methods or not columns:\n",
    "        print(\"No distribution similarity data to plot\")\n",
    "        return\n",
    "    \n",
    "    # Create a DataFrame for easier plotting\n",
    "    data = []\n",
    "    for method in methods:\n",
    "        for col in columns:\n",
    "            if col in distribution_similarity[method]:\n",
    "                data.append({\n",
    "                    'Method': method,\n",
    "                    'Column': col,\n",
    "                    'KS Statistic': distribution_similarity[method][col]['ks_stat'],\n",
    "                    'p-value': distribution_similarity[method][col]['ks_pval']\n",
    "                })\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No valid distribution similarity data to plot\")\n",
    "        return\n",
    "    \n",
    "    df_plot = pd.DataFrame(data)\n",
    "    \n",
    "    # Plot KS statistics (lower is better - more similar distributions)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    chart = sns.barplot(x='Method', y='KS Statistic', hue='Column', data=df_plot)\n",
    "    \n",
    "    plt.title('Distribution Similarity by Method (Lower KS = More Similar)', fontsize=14)\n",
    "    plt.ylabel('Kolmogorov-Smirnov Statistic', fontsize=12)\n",
    "    plt.xlabel('Method', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Column', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('distribution_similarity.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot p-values (higher is better - can't reject null hypothesis of same distribution)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    chart = sns.barplot(x='Method', y='p-value', hue='Column', data=df_plot)\n",
    "    \n",
    "    plt.title('Distribution Similarity p-values by Method (Higher = More Similar)', fontsize=14)\n",
    "    plt.ylabel('p-value', fontsize=12)\n",
    "    plt.xlabel('Method', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Column', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('distribution_pvalues.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_imputation_histograms(original_df, imputed_dfs, columns_to_impute):\n",
    "    \"\"\"\n",
    "    Plot histograms of imputed values compared to original observed values\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_df : pandas.DataFrame\n",
    "        Original data with missing values\n",
    "    imputed_dfs : dict\n",
    "        Dictionary of imputed DataFrames from different methods\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    \"\"\"\n",
    "    # Number of columns and methods\n",
    "    n_cols = len(columns_to_impute)\n",
    "    n_methods = len(imputed_dfs)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(n_cols, 1, figsize=(12, 4 * n_cols))\n",
    "    if n_cols == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot for each column\n",
    "    for i, col in enumerate(columns_to_impute):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot original observed distribution\n",
    "        observed_values = original_df[col].dropna()\n",
    "        n_observed = len(observed_values)\n",
    "        \n",
    "        sns.histplot(observed_values, ax=ax, \n",
    "                    label=f'Original (observed, n={n_observed})', \n",
    "                    alpha=0.5, color='black', kde=True)\n",
    "        \n",
    "        # Plot imputed distributions (only for previously missing values)\n",
    "        colors = plt.cm.tab10.colors\n",
    "        for j, (method_name, imputed_df) in enumerate(imputed_dfs.items()):\n",
    "            # Get just the imputed values (where original was missing)\n",
    "            missing_mask = original_df[col].isna()\n",
    "            imputed_values = imputed_df.loc[missing_mask, col].dropna()\n",
    "            n_imputed = len(imputed_values)\n",
    "            \n",
    "            if not imputed_values.empty:\n",
    "                color = colors[j % len(colors)]\n",
    "                sns.histplot(imputed_values, ax=ax, \n",
    "                            label=f'{method_name} (imputed, n={n_imputed})', \n",
    "                            alpha=0.5, color=color, kde=True)\n",
    "        \n",
    "        # Add distribution statistics\n",
    "        if not observed_values.empty:\n",
    "            obs_mean = observed_values.mean()\n",
    "            obs_std = observed_values.std()\n",
    "            ax.axvline(obs_mean, color='black', linestyle='--', alpha=0.7)\n",
    "            textstr = f'Observed: μ={obs_mean:.2f}, σ={obs_std:.2f}'\n",
    "            props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "            ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "                    verticalalignment='top', bbox=props)\n",
    "        \n",
    "        ax.set_title(f'Distribution for {col}', fontsize=14)\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('imputation_histograms.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_correlation_preservation(original_df, imputed_dfs, columns_to_impute):\n",
    "    \"\"\"\n",
    "    Plot heatmaps showing how well each method preserves correlations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_df : pandas.DataFrame\n",
    "        Original data with missing values\n",
    "    imputed_dfs : dict\n",
    "        Dictionary of imputed DataFrames from different methods\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute\n",
    "    \"\"\"\n",
    "    # Calculate original correlations (using only complete cases)\n",
    "    # This is important for sparse data - we need a baseline for comparison\n",
    "    complete_cases = original_df[columns_to_impute].dropna()\n",
    "    \n",
    "    if len(complete_cases) < 2:\n",
    "        print(\"Not enough complete cases to calculate original correlations\")\n",
    "        # Use pairwise correlations instead\n",
    "        original_corr = original_df[columns_to_impute].corr(method='pearson')\n",
    "    else:\n",
    "        original_corr = complete_cases.corr()\n",
    "    \n",
    "    # Set up the figure\n",
    "    n_methods = len(imputed_dfs)\n",
    "    fig, axes = plt.subplots(1, n_methods + 1, figsize=(5 * (n_methods + 1), 4))\n",
    "    \n",
    "    if n_methods == 0:\n",
    "        print(\"No imputed dataframes to plot correlations\")\n",
    "        return\n",
    "    \n",
    "    # Plot original correlation\n",
    "    sns.heatmap(original_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, ax=axes[0])\n",
    "    axes[0].set_title('Original Correlations', fontsize=14)\n",
    "    \n",
    "    # Plot correlations for each imputation method\n",
    "    for i, (method_name, imputed_df) in enumerate(imputed_dfs.items()):\n",
    "        imputed_corr = imputed_df[columns_to_impute].corr()\n",
    "        sns.heatmap(imputed_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1, ax=axes[i+1])\n",
    "        axes[i+1].set_title(f'{method_name} Correlations', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_preservation.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and plot correlation differences\n",
    "    print(\"Calculating correlation differences...\")\n",
    "    correlation_diffs = {}\n",
    "    \n",
    "    for method_name, imputed_df in imputed_dfs.items():\n",
    "        imputed_corr = imputed_df[columns_to_impute].corr()\n",
    "        # Calculate absolute differences between original and imputed correlations\n",
    "        diff_matrix = np.abs(original_corr - imputed_corr)\n",
    "        correlation_diffs[method_name] = diff_matrix\n",
    "    \n",
    "    # Plot correlation differences (lower is better - less difference from original)\n",
    "    fig, axes = plt.subplots(1, n_methods, figsize=(5 * n_methods, 4))\n",
    "    \n",
    "    if n_methods == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (method_name, diff_matrix) in enumerate(correlation_diffs.items()):\n",
    "        sns.heatmap(diff_matrix, annot=True, cmap='YlOrRd', vmin=0, vmax=1, ax=axes[i])\n",
    "        axes[i].set_title(f'{method_name} Correlation Differences', fontsize=14)\n",
    "        \n",
    "        # Calculate and display average difference\n",
    "        avg_diff = np.mean(diff_matrix.values)\n",
    "        axes[i].text(0.5, -0.1, f'Avg Diff: {avg_diff:.4f}', \n",
    "                    horizontalalignment='center', transform=axes[i].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_differences.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_validation_results(validation_results):\n",
    "    \"\"\"\n",
    "    Plot validation results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    validation_results : dict\n",
    "        Dictionary with validation results\n",
    "    \"\"\"\n",
    "    # Prepare data for plotting\n",
    "    methods = [m for m in validation_results.keys() \n",
    "              if 'avg_mae' in validation_results[m] and not np.isnan(validation_results[m]['avg_mae'])]\n",
    "    \n",
    "    if not methods:\n",
    "        print(\"No valid validation results to plot\")\n",
    "        return\n",
    "    \n",
    "    mae_means = [validation_results[m]['avg_mae'] for m in methods]\n",
    "    mae_stds = [validation_results[m]['std_mae'] for m in methods]\n",
    "    rmse_means = [validation_results[m]['avg_rmse'] for m in methods]\n",
    "    rmse_stds = [validation_results[m]['std_rmse'] for m in methods]\n",
    "    time_means = [validation_results[m]['avg_time'] for m in methods]\n",
    "    time_stds = [validation_results[m]['std_time'] for m in methods]\n",
    "    \n",
    "    # Create figure with three subplots\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
    "    \n",
    "    # Plot MAE\n",
    "    bars1 = axes[0].bar(methods, mae_means, yerr=mae_stds, capsize=5, alpha=0.7)\n",
    "    axes[0].set_title('Mean Absolute Error (MAE) by Method', fontsize=14)\n",
    "    axes[0].set_ylabel('MAE', fontsize=12)\n",
    "    axes[0].set_xlabel('Imputation Method', fontsize=12)\n",
    "    axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.02 * max(mae_means),\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Plot RMSE\n",
    "    bars2 = axes[1].bar(methods, rmse_means, yerr=rmse_stds, capsize=5, alpha=0.7, color='orange')\n",
    "    axes[1].set_title('Root Mean Squared Error (RMSE) by Method', fontsize=14)\n",
    "    axes[1].set_ylabel('RMSE', fontsize=12)\n",
    "    axes[1].set_xlabel('Imputation Method', fontsize=12)\n",
    "    axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.02 * max(rmse_means),\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Plot execution time\n",
    "    bars3 = axes[2].bar(methods, time_means, yerr=time_stds, capsize=5, alpha=0.7, color='green')\n",
    "    axes[2].set_title('Execution Time by Method', fontsize=14)\n",
    "    axes[2].set_ylabel('Time (seconds)', fontsize=12)\n",
    "    axes[2].set_xlabel('Imputation Method', fontsize=12)\n",
    "    axes[2].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for bar in bars3:\n",
    "        height = bar.get_height()\n",
    "        axes[2].text(bar.get_x() + bar.get_width()/2., height + 0.02 * max(time_means),\n",
    "                f'{height:.2f}s', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('validation_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def create_summary_dataframe(imputed_dfs, validation_results, distribution_similarity, execution_times):\n",
    "    \"\"\"\n",
    "    Create a summary dataframe of all results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    imputed_dfs : dict\n",
    "        Dictionary of imputed DataFrames\n",
    "    validation_results : dict\n",
    "        Dictionary with validation results\n",
    "    distribution_similarity : dict\n",
    "        Dictionary with distribution similarity results\n",
    "    execution_times : dict\n",
    "        Dictionary with execution times\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    summary_df : pandas.DataFrame\n",
    "        DataFrame with summary of all results\n",
    "    \"\"\"\n",
    "    # Get list of methods\n",
    "    methods = list(imputed_dfs.keys())\n",
    "    \n",
    "    # Initialize summary data\n",
    "    summary_data = []\n",
    "    \n",
    "    for method in methods:\n",
    "        method_summary = {'Method': method}\n",
    "        \n",
    "        # Add validation metrics if available\n",
    "        if method in validation_results and 'avg_mae' in validation_results[method]:\n",
    "            method_summary['MAE'] = validation_results[method]['avg_mae']\n",
    "            method_summary['RMSE'] = validation_results[method]['avg_rmse']\n",
    "        else:\n",
    "            method_summary['MAE'] = np.nan\n",
    "            method_summary['RMSE'] = np.nan\n",
    "        \n",
    "        # Add execution time\n",
    "        if method in execution_times:\n",
    "            method_summary['Time (s)'] = execution_times[method]\n",
    "        else:\n",
    "            method_summary['Time (s)'] = np.nan\n",
    "        \n",
    "        # Add average distribution similarity metrics if available\n",
    "        if method in distribution_similarity:\n",
    "            ks_stats = []\n",
    "            ks_pvals = []\n",
    "            \n",
    "            for col, results in distribution_similarity[method].items():\n",
    "                if 'ks_stat' in results:\n",
    "                    ks_stats.append(results['ks_stat'])\n",
    "                if 'ks_pval' in results:\n",
    "                    ks_pvals.append(results['ks_pval'])\n",
    "            \n",
    "            if ks_stats:\n",
    "                method_summary['Avg KS Stat'] = np.mean(ks_stats)\n",
    "            else:\n",
    "                method_summary['Avg KS Stat'] = np.nan\n",
    "                \n",
    "            if ks_pvals:\n",
    "                method_summary['Avg KS p-value'] = np.mean(ks_pvals)\n",
    "            else:\n",
    "                method_summary['Avg KS p-value'] = np.nan\n",
    "        else:\n",
    "            method_summary['Avg KS Stat'] = np.nan\n",
    "            method_summary['Avg KS p-value'] = np.nan\n",
    "        \n",
    "        summary_data.append(method_summary)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Add ranks for each metric (1 is best)\n",
    "    for metric in ['MAE', 'RMSE', 'Time (s)', 'Avg KS Stat']:\n",
    "        if metric == 'Avg KS Stat' or metric == 'Time (s)':\n",
    "            # Lower is better for these metrics\n",
    "            summary_df[f'{metric} Rank'] = summary_df[metric].rank()\n",
    "        else:\n",
    "            # Higher is better for p-value\n",
    "            summary_df[f'{metric} Rank'] = summary_df[metric].rank(ascending=False)\n",
    "    \n",
    "    # Add average rank\n",
    "    rank_columns = [col for col in summary_df.columns if 'Rank' in col]\n",
    "    if rank_columns:\n",
    "        summary_df['Average Rank'] = summary_df[rank_columns].mean(axis=1)\n",
    "        summary_df = summary_df.sort_values('Average Rank')\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "def main_real_data(df, columns_to_impute=['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']):\n",
    "    \"\"\"\n",
    "    Main function to run imputation comparison on real data with high missingness\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Your real dataset with missing values\n",
    "    columns_to_impute : list\n",
    "        List of column names to impute (default: ge1-ge6)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    imputed_dfs : dict\n",
    "        Dictionary of imputed DataFrames from each method\n",
    "    summary_df : pandas.DataFrame\n",
    "        DataFrame with summary of all results\n",
    "    \"\"\"\n",
    "    print(\"===== Analyzing Dataset =====\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Show basic statistics of the dataset\n",
    "    print(\"\\nMissing value counts:\")\n",
    "    missing_counts = df[columns_to_impute].isna().sum()\n",
    "    missing_percents = (missing_counts / len(df)) * 100\n",
    "    for col, count, percent in zip(columns_to_impute, missing_counts, missing_percents):\n",
    "        print(f\"{col}: {count} missing values ({percent:.2f}%)\")\n",
    "    \n",
    "    # Print observed values statistics\n",
    "    print(\"\\nObserved values per column:\")\n",
    "    for i, col in enumerate(columns_to_impute):\n",
    "        observed = df[col].dropna()\n",
    "        n_observed = len(observed)\n",
    "        if n_observed > 0:\n",
    "            print(f\"{col}: {n_observed} observed values ({100-missing_percents[i]:.2f}%)\")\n",
    "            print(f\"  Range: {observed.min()} to {observed.max()}\")\n",
    "            print(f\"  Mean: {observed.mean():.4f}, Std: {observed.std():.4f}\")\n",
    "            print(f\"  Unique values: {observed.nunique()} ({observed.nunique()/n_observed*100:.1f}% unique)\")\n",
    "        else:\n",
    "            print(f\"{col}: No observed values\")\n",
    "    \n",
    "    print(\"\\n===== Running imputation on real data =====\")\n",
    "    imputed_dfs, original_df, execution_times, distribution_similarity = evaluate_real_data_imputation(\n",
    "        df, columns_to_impute\n",
    "    )\n",
    "    \n",
    "    # Plot execution times\n",
    "    print(\"\\nPlotting execution times...\")\n",
    "    plot_execution_times(execution_times)\n",
    "    \n",
    "    # Plot distribution similarity\n",
    "    print(\"\\nPlotting distribution similarity...\")\n",
    "    plot_distribution_similarity(distribution_similarity)\n",
    "    \n",
    "    # Plot histograms of imputed values\n",
    "    print(\"\\nPlotting imputation histograms...\")\n",
    "    plot_imputation_histograms(original_df, imputed_dfs, columns_to_impute)\n",
    "    \n",
    "    # Plot correlation preservation\n",
    "    print(\"\\nPlotting correlation preservation...\")\n",
    "    plot_correlation_preservation(original_df, imputed_dfs, columns_to_impute)\n",
    "    \n",
    "    print(\"\\n===== Running sparse validation =====\")\n",
    "    validation_results = evaluate_with_sparse_validation(df, columns_to_impute, n_folds=3)\n",
    "    \n",
    "    # Plot validation results\n",
    "    print(\"\\nPlotting validation results...\")\n",
    "    plot_validation_results(validation_results)\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    print(\"\\nCreating summary of results...\")\n",
    "    summary_df = create_summary_dataframe(\n",
    "        imputed_dfs, \n",
    "        validation_results, \n",
    "        distribution_similarity, \n",
    "        execution_times\n",
    "    )\n",
    "    \n",
    "    # Print summary of results\n",
    "    print(\"\\n===== Summary of Results =====\")\n",
    "    print(summary_df.to_string())\n",
    "    \n",
    "    # Save summary to CSV\n",
    "    summary_df.to_csv('imputation_summary.csv', index=False)\n",
    "    \n",
    "    # Print recommended method based on average rank\n",
    "    if 'Average Rank' in summary_df.columns:\n",
    "        best_method = summary_df.iloc[0]['Method']\n",
    "        print(f\"\\nRecommended imputation method: {best_method}\")\n",
    "    \n",
    "    return imputed_dfs, summary_df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your real dataset with missing values here\n",
    "    # df = pd.read_csv('your_dataset.csv')\n",
    "    \n",
    "    # Define columns to impute\n",
    "    columns_to_impute = ['ge1', 'ge2', 'ge3', 'ge4', 'ge5', 'ge6']\n",
    "    \n",
    "    # Run the analysis\n",
    "    imputed_dfs, summary_df = main_real_data(df, columns_to_impute)\n",
    "    \n",
    "    # Example of how to save the best imputed dataset\n",
    "    if not summary_df.empty:\n",
    "        worst_method = summary_df.iloc[0]['Method']\n",
    "        print(f\"\\nSaving imputed dataset from {worst_method}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MICE':          id   redcap_event_name  age_diagnosis gender overall_primary_tumour  \\\n",
       " 0         1      baseline_arm_1            NaN    1.0                    nan   \n",
       " 1         1                 nan            NaN    NaN                    nan   \n",
       " 2         1                 nan            NaN    NaN                    nan   \n",
       " 3         1                 nan            NaN    NaN                    nan   \n",
       " 4         1                 nan            NaN    NaN                    nan   \n",
       " ...     ...                 ...            ...    ...                    ...   \n",
       " 18182  1770  preoperative_arm_1            NaN    NaN                    nan   \n",
       " 18183  1770  preoperative_arm_1            NaN    NaN                    nan   \n",
       " 18184  1770  preoperative_arm_1            NaN    NaN                    nan   \n",
       " 18185  1770      baseline_arm_1            NaN    1.0                    nan   \n",
       " 18186  1770       surgery_arm_1            NaN    NaN                    nan   \n",
       " \n",
       "       overall_regional_ln overall_distant_metastasis neotx___notx  \\\n",
       " 0                     nan                        nan          NaN   \n",
       " 1                     nan                        nan          NaN   \n",
       " 2                     nan                        nan          NaN   \n",
       " 3                     nan                        nan          NaN   \n",
       " 4                     nan                        nan          NaN   \n",
       " ...                   ...                        ...          ...   \n",
       " 18182                 nan                        nan          0.0   \n",
       " 18183                 nan                        nan          NaN   \n",
       " 18184                 nan                        nan          NaN   \n",
       " 18185                 nan                        nan          NaN   \n",
       " 18186                 nan                        nan          NaN   \n",
       " \n",
       "       neotx___chemo neotx___rads  ... a_e4 a_e5 a_e6 a_e7 a_c6 a_c2 a_act11  \\\n",
       " 0               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 1               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 2               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 3               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 4               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " ...             ...          ...  ...  ...  ...  ...  ...  ...  ...     ...   \n",
       " 18182           0.0          0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18183           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18184           NaN          NaN  ...  0.0  0.0  0.0  0.0  1.0  3.0     3.0   \n",
       " 18185           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18186           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " \n",
       "       readmission_30d  postop_comp   los  \n",
       " 0                 NaN          0.0   NaN  \n",
       " 1                 NaN          NaN   NaN  \n",
       " 2                 NaN          NaN  16.0  \n",
       " 3                 NaN          NaN   8.0  \n",
       " 4                 NaN          NaN  13.0  \n",
       " ...               ...          ...   ...  \n",
       " 18182             NaN          NaN   NaN  \n",
       " 18183             NaN          NaN   NaN  \n",
       " 18184             NaN          NaN   NaN  \n",
       " 18185             NaN          NaN   NaN  \n",
       " 18186             NaN          NaN   NaN  \n",
       " \n",
       " [18187 rows x 65 columns],\n",
       " 'GAIN':          id redcap_event_name  age_diagnosis gender overall_primary_tumour  \\\n",
       " 0         1                 8            NaN    1.0                    nan   \n",
       " 1         1                 9            NaN    NaN                    nan   \n",
       " 2         1                 9            NaN    NaN                    nan   \n",
       " 3         1                 9            NaN    NaN                    nan   \n",
       " 4         1                 9            NaN    NaN                    nan   \n",
       " ...     ...               ...            ...    ...                    ...   \n",
       " 18182  1770                10            NaN    NaN                    nan   \n",
       " 18183  1770                10            NaN    NaN                    nan   \n",
       " 18184  1770                10            NaN    NaN                    nan   \n",
       " 18185  1770                 8            NaN    1.0                    nan   \n",
       " 18186  1770                11            NaN    NaN                    nan   \n",
       " \n",
       "       overall_regional_ln overall_distant_metastasis neotx___notx  \\\n",
       " 0                     nan                        nan          NaN   \n",
       " 1                     nan                        nan          NaN   \n",
       " 2                     nan                        nan          NaN   \n",
       " 3                     nan                        nan          NaN   \n",
       " 4                     nan                        nan          NaN   \n",
       " ...                   ...                        ...          ...   \n",
       " 18182                 nan                        nan          0.0   \n",
       " 18183                 nan                        nan          NaN   \n",
       " 18184                 nan                        nan          NaN   \n",
       " 18185                 nan                        nan          NaN   \n",
       " 18186                 nan                        nan          NaN   \n",
       " \n",
       "       neotx___chemo neotx___rads  ... a_e4 a_e5 a_e6 a_e7 a_c6 a_c2 a_act11  \\\n",
       " 0               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 1               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 2               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 3               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 4               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " ...             ...          ...  ...  ...  ...  ...  ...  ...  ...     ...   \n",
       " 18182           0.0          0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18183           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18184           NaN          NaN  ...  0.0  0.0  0.0  0.0  1.0  3.0     3.0   \n",
       " 18185           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18186           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " \n",
       "       readmission_30d  postop_comp   los  \n",
       " 0                 NaN          0.0   NaN  \n",
       " 1                 NaN          NaN   NaN  \n",
       " 2                 NaN          NaN  16.0  \n",
       " 3                 NaN          NaN   8.0  \n",
       " 4                 NaN          NaN  13.0  \n",
       " ...               ...          ...   ...  \n",
       " 18182             NaN          NaN   NaN  \n",
       " 18183             NaN          NaN   NaN  \n",
       " 18184             NaN          NaN   NaN  \n",
       " 18185             NaN          NaN   NaN  \n",
       " 18186             NaN          NaN   NaN  \n",
       " \n",
       " [18187 rows x 65 columns],\n",
       " 'Original GAIN':            id  redcap_event_name  age_diagnosis    gender  \\\n",
       " 0         1.0                8.0      94.920326  1.000000   \n",
       " 1         1.0                9.0      94.888451  1.000066   \n",
       " 2         1.0                9.0      94.937500  1.000061   \n",
       " 3         1.0                9.0      94.937004  1.000062   \n",
       " 4         1.0                9.0      94.937317  1.000061   \n",
       " ...       ...                ...            ...       ...   \n",
       " 18182  1770.0               10.0      94.978004  1.000018   \n",
       " 18183  1770.0               10.0      94.862984  1.000049   \n",
       " 18184  1770.0               10.0      94.784492  1.005744   \n",
       " 18185  1770.0                8.0      94.898445  1.000000   \n",
       " 18186  1770.0               11.0      94.924126  1.000023   \n",
       " \n",
       "       overall_primary_tumour overall_regional_ln overall_distant_metastasis  \\\n",
       " 0                        nan                 nan                        nan   \n",
       " 1                        nan                 nan                        nan   \n",
       " 2                        nan                 nan                        nan   \n",
       " 3                        nan                 nan                        nan   \n",
       " 4                        nan                 nan                        nan   \n",
       " ...                      ...                 ...                        ...   \n",
       " 18182                    nan                 nan                        nan   \n",
       " 18183                    nan                 nan                        nan   \n",
       " 18184                    nan                 nan                        nan   \n",
       " 18185                    nan                 nan                        nan   \n",
       " 18186                    nan                 nan                        nan   \n",
       " \n",
       "        neotx___notx  neotx___chemo  neotx___rads  ...      a_e4      a_e5  \\\n",
       " 0          0.999939       0.999503      0.999924  ...  0.009684  0.015438   \n",
       " 1          0.999930       0.999568      0.999907  ...  0.009407  0.016509   \n",
       " 2          0.999937       0.999585      0.999932  ...  0.010012  0.014660   \n",
       " 3          0.999938       0.999584      0.999932  ...  0.010027  0.014629   \n",
       " 4          0.999937       0.999585      0.999932  ...  0.010018  0.014648   \n",
       " ...             ...            ...           ...  ...       ...       ...   \n",
       " 18182      0.000000       0.000000      0.000000  ...  0.007331  0.013290   \n",
       " 18183      0.999931       0.999605      0.999909  ...  0.008063  0.014976   \n",
       " 18184      0.999931       0.996361      0.999889  ...  0.000000  0.000000   \n",
       " 18185      0.999938       0.999600      0.999914  ...  0.007151  0.014301   \n",
       " 18186      0.999931       0.999612      0.999923  ...  0.006821  0.011577   \n",
       " \n",
       "            a_e6      a_e7      a_c6      a_c2   a_act11 readmission_30d  \\\n",
       " 0      0.500004  0.027925  2.858042  2.068872  0.197515        1.999922   \n",
       " 1      0.538392  0.029193  2.869888  2.048409  0.236205        1.999915   \n",
       " 2      0.446607  0.027651  2.992539  2.017362  0.188988        1.999928   \n",
       " 3      0.447808  0.027851  2.994638  2.017884  0.189275        1.999928   \n",
       " 4      0.447057  0.027726  2.993326  2.017557  0.189096        1.999928   \n",
       " ...         ...       ...       ...       ...       ...             ...   \n",
       " 18182  0.977640  0.033473  3.313914  1.744292  0.267624        1.999960   \n",
       " 18183  0.619012  0.026011  2.928266  1.884847  0.238257        1.999928   \n",
       " 18184  0.000000  0.000000  1.000000  3.000000  3.000000        1.999191   \n",
       " 18185  0.514294  0.021851  2.924754  1.927312  0.235560        1.999923   \n",
       " 18186  0.529615  0.022342  2.947128  1.866192  0.201514        1.999956   \n",
       " \n",
       "        postop_comp        los  \n",
       " 0         0.000000   0.000027  \n",
       " 1         0.997535   0.000031  \n",
       " 2         0.997885  16.000000  \n",
       " 3         0.997903   8.000000  \n",
       " 4         0.997892  13.000000  \n",
       " ...            ...        ...  \n",
       " 18182     0.998914   0.000014  \n",
       " 18183     0.997739   0.000022  \n",
       " 18184     0.996054   0.008485  \n",
       " 18185     0.997756   0.000024  \n",
       " 18186     0.998019   0.000014  \n",
       " \n",
       " [18187 rows x 65 columns],\n",
       " 'Autoencoder':          id redcap_event_name  age_diagnosis gender overall_primary_tumour  \\\n",
       " 0         1                 8            NaN    1.0                    nan   \n",
       " 1         1                 9            NaN    NaN                    nan   \n",
       " 2         1                 9            NaN    NaN                    nan   \n",
       " 3         1                 9            NaN    NaN                    nan   \n",
       " 4         1                 9            NaN    NaN                    nan   \n",
       " ...     ...               ...            ...    ...                    ...   \n",
       " 18182  1770                10            NaN    NaN                    nan   \n",
       " 18183  1770                10            NaN    NaN                    nan   \n",
       " 18184  1770                10            NaN    NaN                    nan   \n",
       " 18185  1770                 8            NaN    1.0                    nan   \n",
       " 18186  1770                11            NaN    NaN                    nan   \n",
       " \n",
       "       overall_regional_ln overall_distant_metastasis neotx___notx  \\\n",
       " 0                     nan                        nan          NaN   \n",
       " 1                     nan                        nan          NaN   \n",
       " 2                     nan                        nan          NaN   \n",
       " 3                     nan                        nan          NaN   \n",
       " 4                     nan                        nan          NaN   \n",
       " ...                   ...                        ...          ...   \n",
       " 18182                 nan                        nan          0.0   \n",
       " 18183                 nan                        nan          NaN   \n",
       " 18184                 nan                        nan          NaN   \n",
       " 18185                 nan                        nan          NaN   \n",
       " 18186                 nan                        nan          NaN   \n",
       " \n",
       "       neotx___chemo neotx___rads  ... a_e4 a_e5 a_e6 a_e7 a_c6 a_c2 a_act11  \\\n",
       " 0               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 1               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 2               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 3               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 4               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " ...             ...          ...  ...  ...  ...  ...  ...  ...  ...     ...   \n",
       " 18182           0.0          0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18183           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18184           NaN          NaN  ...  0.0  0.0  0.0  0.0  1.0  3.0     3.0   \n",
       " 18185           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18186           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " \n",
       "       readmission_30d  postop_comp   los  \n",
       " 0                 NaN          0.0   NaN  \n",
       " 1                 NaN          NaN   NaN  \n",
       " 2                 NaN          NaN  16.0  \n",
       " 3                 NaN          NaN   8.0  \n",
       " 4                 NaN          NaN  13.0  \n",
       " ...               ...          ...   ...  \n",
       " 18182             NaN          NaN   NaN  \n",
       " 18183             NaN          NaN   NaN  \n",
       " 18184             NaN          NaN   NaN  \n",
       " 18185             NaN          NaN   NaN  \n",
       " 18186             NaN          NaN   NaN  \n",
       " \n",
       " [18187 rows x 65 columns],\n",
       " 'DAE':          id redcap_event_name  age_diagnosis gender overall_primary_tumour  \\\n",
       " 0         1                 8            NaN    1.0                    nan   \n",
       " 1         1                 9            NaN    NaN                    nan   \n",
       " 2         1                 9            NaN    NaN                    nan   \n",
       " 3         1                 9            NaN    NaN                    nan   \n",
       " 4         1                 9            NaN    NaN                    nan   \n",
       " ...     ...               ...            ...    ...                    ...   \n",
       " 18182  1770                10            NaN    NaN                    nan   \n",
       " 18183  1770                10            NaN    NaN                    nan   \n",
       " 18184  1770                10            NaN    NaN                    nan   \n",
       " 18185  1770                 8            NaN    1.0                    nan   \n",
       " 18186  1770                11            NaN    NaN                    nan   \n",
       " \n",
       "       overall_regional_ln overall_distant_metastasis neotx___notx  \\\n",
       " 0                     nan                        nan          NaN   \n",
       " 1                     nan                        nan          NaN   \n",
       " 2                     nan                        nan          NaN   \n",
       " 3                     nan                        nan          NaN   \n",
       " 4                     nan                        nan          NaN   \n",
       " ...                   ...                        ...          ...   \n",
       " 18182                 nan                        nan          0.0   \n",
       " 18183                 nan                        nan          NaN   \n",
       " 18184                 nan                        nan          NaN   \n",
       " 18185                 nan                        nan          NaN   \n",
       " 18186                 nan                        nan          NaN   \n",
       " \n",
       "       neotx___chemo neotx___rads  ... a_e4 a_e5 a_e6 a_e7 a_c6 a_c2 a_act11  \\\n",
       " 0               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 1               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 2               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 3               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 4               NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " ...             ...          ...  ...  ...  ...  ...  ...  ...  ...     ...   \n",
       " 18182           0.0          0.0  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18183           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18184           NaN          NaN  ...  0.0  0.0  0.0  0.0  1.0  3.0     3.0   \n",
       " 18185           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " 18186           NaN          NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN     NaN   \n",
       " \n",
       "       readmission_30d  postop_comp   los  \n",
       " 0                 NaN          0.0   NaN  \n",
       " 1                 NaN          NaN   NaN  \n",
       " 2                 NaN          NaN  16.0  \n",
       " 3                 NaN          NaN   8.0  \n",
       " 4                 NaN          NaN  13.0  \n",
       " ...               ...          ...   ...  \n",
       " 18182             NaN          NaN   NaN  \n",
       " 18183             NaN          NaN   NaN  \n",
       " 18184             NaN          NaN   NaN  \n",
       " 18185             NaN          NaN   NaN  \n",
       " 18186             NaN          NaN   NaN  \n",
       " \n",
       " [18187 rows x 65 columns]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Time (s)</th>\n",
       "      <th>Avg KS Stat</th>\n",
       "      <th>Avg KS p-value</th>\n",
       "      <th>MAE Rank</th>\n",
       "      <th>RMSE Rank</th>\n",
       "      <th>Time (s) Rank</th>\n",
       "      <th>Avg KS Stat Rank</th>\n",
       "      <th>Average Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GAIN</td>\n",
       "      <td>1.154310</td>\n",
       "      <td>1.316881</td>\n",
       "      <td>0.580499</td>\n",
       "      <td>0.789512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Original GAIN</td>\n",
       "      <td>1.041114</td>\n",
       "      <td>1.391916</td>\n",
       "      <td>9.689930</td>\n",
       "      <td>0.726424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DAE</td>\n",
       "      <td>0.966525</td>\n",
       "      <td>1.177642</td>\n",
       "      <td>3.644855</td>\n",
       "      <td>0.590214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MICE</td>\n",
       "      <td>0.658630</td>\n",
       "      <td>1.019815</td>\n",
       "      <td>27.133862</td>\n",
       "      <td>0.532098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Autoencoder</td>\n",
       "      <td>0.842492</td>\n",
       "      <td>1.082844</td>\n",
       "      <td>173.436089</td>\n",
       "      <td>0.588847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Method       MAE      RMSE    Time (s)  Avg KS Stat  Avg KS p-value  \\\n",
       "1           GAIN  1.154310  1.316881    0.580499     0.789512             0.0   \n",
       "2  Original GAIN  1.041114  1.391916    9.689930     0.726424             0.0   \n",
       "4            DAE  0.966525  1.177642    3.644855     0.590214             0.0   \n",
       "0           MICE  0.658630  1.019815   27.133862     0.532098             0.0   \n",
       "3    Autoencoder  0.842492  1.082844  173.436089     0.588847             0.0   \n",
       "\n",
       "   MAE Rank  RMSE Rank  Time (s) Rank  Avg KS Stat Rank  Average Rank  \n",
       "1       1.0        2.0            1.0               5.0          2.25  \n",
       "2       2.0        1.0            3.0               4.0          2.50  \n",
       "4       3.0        3.0            2.0               3.0          2.75  \n",
       "0       5.0        5.0            4.0               1.0          3.75  \n",
       "3       4.0        4.0            5.0               2.0          3.75  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jinenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
